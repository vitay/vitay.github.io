<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <meta name="author" content="Julien Vitay">
  <title>Deep Reinforcement Learning</title>


  <link rel="stylesheet" href="assets/syntax.css">
  <link rel="stylesheet" href="assets/menu.css">
  <link rel="stylesheet" href="assets/github.css">
  <script src="assets/menu.js" type="text/javascript"></script>
  <script src="/usr/share/mathjax2/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->

</head>

<body class="markdown-body">

<div class="navbar">
  <div class="dropdown" id="dropdown">
    <button class="dropbtn">
      <div class="dropicon-first"></div>
      <div class="dropicon"></div>
      <div class="dropicon"></div>
      <i class="fa fa-caret-down"></i>
    </button> 
    <span class="navbar-title">Deep Reinforcement Learning - Julien Vitay</span>
    <div class="dropdown-content">

<ul>
<li><a href="./Introduction.html#sec:introduction"><span class="toc-section-number">1</span> Introduction</a></li>
<li><a href="./BasicRL.html#sec:basics"><span class="toc-section-number">2</span> Basics</a><ul>
<li><a href="./BasicRL.html#sec:reinforcement-learning-and-markov-decision-process"><span class="toc-section-number">2.1</span> Reinforcement learning and Markov Decision Process</a><ul>
<li><a href="./BasicRL.html#sec:policy-and-value-functions"><span class="toc-section-number">2.1.1</span> Policy and value functions</a></li>
<li><a href="./BasicRL.html#sec:bellman-equations"><span class="toc-section-number">2.1.2</span> Bellman equations</a></li>
<li><a href="./BasicRL.html#sec:dynamic-programming"><span class="toc-section-number">2.1.3</span> Dynamic programming</a></li>
<li><a href="./BasicRL.html#sec:monte-carlo-sampling"><span class="toc-section-number">2.1.4</span> Monte-Carlo sampling</a></li>
<li><a href="./BasicRL.html#sec:temporal-difference"><span class="toc-section-number">2.1.5</span> Temporal Difference</a></li>
<li><a href="./BasicRL.html#sec:eligibility-traces"><span class="toc-section-number">2.1.6</span> Eligibility traces</a></li>
<li><a href="./BasicRL.html#sec:actor-critic-architectures"><span class="toc-section-number">2.1.7</span> Actor-critic architectures</a></li>
<li><a href="./BasicRL.html#sec:function-approximation"><span class="toc-section-number">2.1.8</span> Function approximation</a></li>
</ul></li>
<li><a href="./DeepLearning.html#sec:deep-learning"><span class="toc-section-number">2.2</span> Deep learning</a><ul>
<li><a href="./DeepLearning.html#sec:deep-neural-networks"><span class="toc-section-number">2.2.1</span> Deep neural networks</a></li>
<li><a href="./DeepLearning.html#sec:convolutional-networks"><span class="toc-section-number">2.2.2</span> Convolutional networks</a></li>
<li><a href="./DeepLearning.html#sec:recurrent-neural-networks"><span class="toc-section-number">2.2.3</span> Recurrent neural networks</a></li>
</ul></li>
</ul></li>
<li><a href="./Valuebased.html#sec:value-based-methods"><span class="toc-section-number">3</span> Value-based methods</a><ul>
<li><a href="./Valuebased.html#sec:limitations-of-deep-neural-networks-for-function-approximation"><span class="toc-section-number">3.1</span> Limitations of deep neural networks for function approximation</a></li>
<li><a href="./Valuebased.html#sec:deep-q-network-dqn"><span class="toc-section-number">3.2</span> Deep Q-Network (DQN)</a></li>
<li><a href="./Valuebased.html#sec:double-dqn"><span class="toc-section-number">3.3</span> Double DQN</a></li>
<li><a href="./Valuebased.html#sec:prioritized-experience-replay"><span class="toc-section-number">3.4</span> Prioritized experience replay</a></li>
<li><a href="./Valuebased.html#sec:duelling-network"><span class="toc-section-number">3.5</span> Duelling network</a></li>
<li><a href="./Valuebased.html#sec:distributed-dqn-gorila"><span class="toc-section-number">3.6</span> Distributed DQN (GORILA)</a></li>
<li><a href="./Valuebased.html#sec:deep-recurrent-q-learning-drqn"><span class="toc-section-number">3.7</span> Deep Recurrent Q-learning (DRQN)</a></li>
<li><a href="./Valuebased.html#sec:other-variants-of-dqn"><span class="toc-section-number">3.8</span> Other variants of DQN</a></li>
</ul></li>
<li><a href="./PolicyGradient.html#sec:policy-gradient-methods"><span class="toc-section-number">4</span> Policy Gradient methods</a><ul>
<li><a href="./PolicyGradient.html#sec:reinforce"><span class="toc-section-number">4.1</span> REINFORCE</a><ul>
<li><a href="./PolicyGradient.html#sec:estimating-the-policy-gradient"><span class="toc-section-number">4.1.1</span> Estimating the policy gradient</a></li>
<li><a href="./PolicyGradient.html#sec:reducing-the-variance"><span class="toc-section-number">4.1.2</span> Reducing the variance</a></li>
<li><a href="./PolicyGradient.html#sec:policy-gradient-theorem"><span class="toc-section-number">4.1.3</span> Policy Gradient theorem</a></li>
</ul></li>
<li><a href="./ActorCritic.html#sec:advantage-actor-critic-methods"><span class="toc-section-number">4.2</span> Advantage Actor-Critic methods</a><ul>
<li><a href="./ActorCritic.html#sec:advantage-actor-critic-a2c"><span class="toc-section-number">4.2.1</span> Advantage Actor-Critic (A2C)</a></li>
<li><a href="./ActorCritic.html#sec:asynchronous-advantage-actor-critic-a3c"><span class="toc-section-number">4.2.2</span> Asynchronous Advantage Actor-Critic (A3C)</a></li>
<li><a href="./ActorCritic.html#sec:generalized-advantage-estimation-gae"><span class="toc-section-number">4.2.3</span> Generalized Advantage Estimation (GAE)</a></li>
<li><a href="./ActorCritic.html#sec:stochastic-actor-critic-for-continuous-action-spaces"><span class="toc-section-number">4.2.4</span> Stochastic actor-critic for continuous action spaces</a></li>
</ul></li>
<li><a href="./ImportanceSampling.html#sec:off-policy-actor-critic"><span class="toc-section-number">4.3</span> Off-policy Actor-Critic</a><ul>
<li><a href="./ImportanceSampling.html#sec:importance-sampling"><span class="toc-section-number">4.3.1</span> Importance sampling</a></li>
<li><a href="./ImportanceSampling.html#sec:linear-off-policy-actor-critic-off-pac"><span class="toc-section-number">4.3.2</span> Linear Off-Policy Actor-Critic (Off-PAC)</a></li>
<li><a href="./ImportanceSampling.html#sec:retrace"><span class="toc-section-number">4.3.3</span> Retrace</a></li>
<li><a href="./ImportanceSampling.html#sec:self-imitation-learning-sil"><span class="toc-section-number">4.3.4</span> Self-Imitation Learning (SIL)</a></li>
</ul></li>
<li><a href="./DPG.html#sec:deterministic-policy-gradient-dpg"><span class="toc-section-number">4.4</span> Deterministic Policy Gradient (DPG)</a><ul>
<li><a href="./DPG.html#sec:deterministic-policy-gradient-theorem"><span class="toc-section-number">4.4.1</span> Deterministic policy gradient theorem</a></li>
<li><a href="./DPG.html#sec:deep-deterministic-policy-gradient-ddpg"><span class="toc-section-number">4.4.2</span> Deep Deterministic Policy Gradient (DDPG)</a></li>
<li><a href="./DPG.html#sec:distributed-distributional-ddpg-d4pg"><span class="toc-section-number">4.4.3</span> Distributed Distributional DDPG (D4PG)</a></li>
</ul></li>
<li><a href="./NaturalGradient.html#sec:natural-gradients"><span class="toc-section-number">4.5</span> Natural Gradients</a><ul>
<li><a href="./NaturalGradient.html#sec:principle-of-natural-gradients"><span class="toc-section-number">4.5.1</span> Principle of natural gradients</a></li>
<li><a href="./NaturalGradient.html#sec:natural-policy-gradient-and-natural-actor-critic-nac"><span class="toc-section-number">4.5.2</span> Natural policy gradient and Natural Actor Critic (NAC)</a></li>
<li><a href="./NaturalGradient.html#sec:trust-region-policy-optimization-trpo"><span class="toc-section-number">4.5.3</span> Trust Region Policy Optimization (TRPO)</a></li>
<li><a href="./NaturalGradient.html#sec:proximal-policy-optimization-ppo"><span class="toc-section-number">4.5.4</span> Proximal Policy Optimization (PPO)</a></li>
<li><a href="./NaturalGradient.html#sec:actor-critic-with-experience-replay-acer"><span class="toc-section-number">4.5.5</span> Actor-Critic with Experience Replay (ACER)</a></li>
</ul></li>
<li><a href="./EntropyRL.html#sec:maximum-entropy-rl"><span class="toc-section-number">4.6</span> Maximum Entropy RL</a><ul>
<li><a href="#sec:entropy-regularization-1"><span class="toc-section-number">4.6.1</span> Entropy regularization</a></li>
<li><a href="./EntropyRL.html#sec:soft-q-learning"><span class="toc-section-number">4.6.2</span> Soft Q-learning</a></li>
<li><a href="./EntropyRL.html#sec:soft-actor-critic-sac"><span class="toc-section-number">4.6.3</span> Soft Actor-Critic (SAC)</a></li>
</ul></li>
<li><a href="./DistributionalRL.html#sec:distributional-learning"><span class="toc-section-number">4.7</span> Distributional learning</a><ul>
<li><a href="./DistributionalRL.html#sec:categorical-dqn"><span class="toc-section-number">4.7.1</span> Categorical DQN</a></li>
<li><a href="./DistributionalRL.html#sec:the-reactor"><span class="toc-section-number">4.7.2</span> The Reactor</a></li>
</ul></li>
<li><a href="./OtherPolicyGradient.html#sec:miscellaneous-model-free-algorithm"><span class="toc-section-number">4.8</span> Miscellaneous model-free algorithm</a><ul>
<li><a href="./OtherPolicyGradient.html#sec:stochastic-value-gradient-svg"><span class="toc-section-number">4.8.1</span> Stochastic Value Gradient (SVG)</a></li>
<li><a href="./OtherPolicyGradient.html#sec:q-prop"><span class="toc-section-number">4.8.2</span> Q-Prop</a></li>
<li><a href="./OtherPolicyGradient.html#sec:normalized-advantage-function-naf"><span class="toc-section-number">4.8.3</span> Normalized Advantage Function (NAF)</a></li>
<li><a href="./OtherPolicyGradient.html#sec:fictitious-self-play-fsp"><span class="toc-section-number">4.8.4</span> Fictitious Self-Play (FSP)</a></li>
</ul></li>
<li><a href="./OtherPolicyGradient.html#sec:comparison-between-value-based-and-policy-gradient-methods"><span class="toc-section-number">4.9</span> Comparison between value-based and policy gradient methods</a></li>
<li><a href="./OtherPolicyGradient.html#sec:gradient-free-policy-search"><span class="toc-section-number">4.10</span> Gradient-free policy search</a><ul>
<li><a href="./OtherPolicyGradient.html#sec:cross-entropy-method-cem"><span class="toc-section-number">4.10.1</span> Cross-entropy Method (CEM)</a></li>
<li><a href="./OtherPolicyGradient.html#sec:evolutionary-search-es"><span class="toc-section-number">4.10.2</span> Evolutionary Search (ES)</a></li>
</ul></li>
</ul></li>
<li><a href="./ModelBased.html#sec:model-based-rl"><span class="toc-section-number">5</span> Model-based RL</a><ul>
<li><a href="./ModelBased.html#sec:dyna-q"><span class="toc-section-number">5.1</span> Dyna-Q</a></li>
<li><a href="./ModelBased.html#sec:unsorted-references"><span class="toc-section-number">5.2</span> Unsorted references</a></li>
</ul></li>
<li><a href="./Practice.html#sec:deep-rl-in-practice"><span class="toc-section-number">6</span> Deep RL in practice</a><ul>
<li><a href="./Practice.html#sec:limitations"><span class="toc-section-number">6.1</span> Limitations</a></li>
<li><a href="./Practice.html#sec:simulation-environments"><span class="toc-section-number">6.2</span> Simulation environments</a></li>
<li><a href="./Practice.html#sec:algorithm-implementations"><span class="toc-section-number">6.3</span> Algorithm implementations</a></li>
</ul></li>
<li><a href="./References.html#sec:references">References</a></li>
</ul>


    </div>
  </div>
</div>

<article>

<h2 id="sec:deep-learning"><span class="header-section-number">2.2</span> Deep learning</h2>
<p>Deep RL uses deep neural networks as function approximators, allowing complex representations of the value of state-action pairs to be learned. This section provides a very quick overview of deep learning. For additional details, refer to the excellent book of <span class="citation" data-cites="Goodfellow2016">Goodfellow et al. (<a href="References.html#ref-Goodfellow2016" role="doc-biblioref">2016</a>)</span>.</p>
<h3 id="sec:deep-neural-networks"><span class="header-section-number">2.2.1</span> Deep neural networks</h3>
<p>A deep neural network (DNN) consists of one input layer <span class="math inline">\(\mathbf{x}\)</span>, one or several hidden layers <span class="math inline">\(\mathbf{h_1}, \mathbf{h_2}, \ldots, \mathbf{h_n}\)</span> and one output layer <span class="math inline">\(\mathbf{y}\)</span> (Fig. <a href="#fig:dnn">9</a>).</p>
<figure>
<img src="img/dnn.png" id="fig:dnn" style="width:60.0%" alt="" /><figcaption>Figure 9: Architecture of a deep neural network. Figure taken from <span class="citation" data-cites="Nielsen2015">Nielsen (<a href="References.html#ref-Nielsen2015" role="doc-biblioref">2015</a>)</span>, CC-BY-NC.</figcaption>
</figure>
<p>Each layer <span class="math inline">\(k\)</span> (called <em>fully-connected</em>) transforms the activity of the previous layer (the vector <span class="math inline">\(\mathbf{h_{k-1}}\)</span>) into another vector <span class="math inline">\(\mathbf{h_{k}}\)</span> by multiplying it with a <strong>weight matrix</strong> <span class="math inline">\(W_k\)</span>, adding a <strong>bias</strong> vector <span class="math inline">\(\mathbf{b_k}\)</span> and applying a non-linear <strong>activation function</strong> <span class="math inline">\(f\)</span>.</p>
<p><span id="eq:fullyconnected"><span class="math display">\[
    \mathbf{h_{k}} = f(W_k \times \mathbf{h_{k-1}} + \mathbf{b_k})
\qquad(4)\]</span></span></p>
<p>The activation function can theoretically be of any type as long as it is non-linear (sigmoid, tanh…), but modern neural networks use preferentially the <strong>Rectified Linear Unit</strong> (ReLU) function <span class="math inline">\(f(x) = \max(0, x)\)</span> or its parameterized variants.</p>
<p>The goal of learning is to find the weights and biases <span class="math inline">\(\theta\)</span> minimizing a given <strong>loss function</strong> on a training set <span class="math inline">\(\mathcal{D}\)</span>.</p>
<ul>
<li>In <em>regression</em> problems, the <strong>mean square error</strong> (mse) is minimized:</li>
</ul>
<p><span class="math display">\[
    \mathcal{L}(\theta) = \mathbb{E}_{\mathbf{x}, \mathbf{t} \in \mathcal{D}} [||\mathbf{t} - \mathbf{y}||^2]
\]</span></p>
<p>where <span class="math inline">\(\mathbf{x}\)</span> is the input, <span class="math inline">\(\mathbf{t}\)</span> the true output (defined in the training set) and <span class="math inline">\(\mathbf{y}\)</span> the prediction of the NN for the input <span class="math inline">\(\mathbf{x}\)</span>. The closer the prediction from the true value, the smaller the mse.</p>
<ul>
<li>In <em>classification</em> problems, the <strong>cross entropy</strong> (or negative log-likelihood) is minimized:</li>
</ul>
<p><span class="math display">\[
    \mathcal{L}(\theta) = - \mathbb{E}_{\mathbf{x}, \mathbf{t} \in \mathcal{D}} [\sum_i t_i \log y_i]
\]</span></p>
<p>where the log-likelihood of the prediction <span class="math inline">\(\mathbf{y}\)</span> to match the data <span class="math inline">\(\mathbf{t}\)</span> is maximized over the training set. The mse could be used for classification problems too, but the output layer usually has a softmax activation function for classification problems, which works nicely with the cross entropy loss function. See <a href="https://rdipietro.github.io/friendly-intro-to-cross-entropy-loss" class="uri">https://rdipietro.github.io/friendly-intro-to-cross-entropy-loss</a> for the link between cross entropy and log-likelihood and <a href="https://deepnotes.io/softmax-crossentropy" class="uri">https://deepnotes.io/softmax-crossentropy</a> for the interplay between softmax and cross entropy.</p>
<p>Once the loss function is defined, it has to be minimized by searching optimal values for the free parameters <span class="math inline">\(\theta\)</span>. This optimization procedure is based on <strong>gradient descent</strong>, which is an iterative procedure modifying estimates of the free parameters in the opposite direction of the gradient of the loss function:</p>
<p><span class="math display">\[
\Delta \theta = -\eta \, \nabla_\theta \mathcal{L}(\theta) = -\eta \, \frac{\partial \mathcal{L}(\theta)}{\partial \theta}
\]</span></p>
<p>The learning rate <span class="math inline">\(\eta\)</span> is chosen very small to ensure a smooth convergence. Intuitively, the gradient (or partial derivative) represents how the loss function changes when each parameter is slightly increased. If the gradient w.r.t a single parameter (e.g. a weight <span class="math inline">\(w\)</span>) is positive, increasing the weight increases the loss function (i.e. the error), so the weight should be slightly decreased instead. If the gradient is negative, one should increase the weight.</p>
<p>The question is now to compute the gradient of the loss function w.r.t all the parameters of the DNN, i.e. each single weight and bias. The solution is given by the <strong>backpropagation</strong> algorithm, which is simply an application of the <strong>chain rule</strong> to feedforward neural networks:</p>
<p><span class="math display">\[
    \frac{\partial \mathcal{L}(\theta)}{\partial W_k} = \frac{\partial \mathcal{L}(\theta)}{\partial \mathbf{y}} \times \frac{\partial \mathbf{y}}{\partial \mathbf{h_n}} \times \frac{\partial \mathbf{h_n}}{\partial \mathbf{h_{n-1}}} \times \ldots \times \frac{\partial \mathbf{h_k}}{\partial W_k}
\]</span></p>
<p>Each layer of the network adds a contribution to the gradient when going <strong>backwards</strong> from the loss function to the parameters. Importantly, all functions used in a NN are differentiable, i.e. those partial derivatives exist (and are easy to compute). For the fully connected layer represented by Eq. <a href="#eq:fullyconnected">4</a>, the partial derivative is given by:</p>
<p><span class="math display">\[
    \frac{\partial \mathbf{h_{k}}}{\partial \mathbf{h_{k-1}}} = f&#39;(W_k \times \mathbf{h_{k-1}} + \mathbf{b_k}) \, W_k
\]</span></p>
<p>and its dependency on the parameters is:</p>
<p><span class="math display">\[
    \frac{\partial \mathbf{h_{k}}}{\partial W_k} = f&#39;(W_k \times \mathbf{h_{k-1}} + \mathbf{b_k}) \, \mathbf{h_{k-1}}
\]</span> <span class="math display">\[
    \frac{\partial \mathbf{h_{k}}}{\partial \mathbf{b_k}} = f&#39;(W_k \times \mathbf{h_{k-1}} + \mathbf{b_k})
\]</span></p>
<p>Activation functions are chosen to have an easy-to-compute derivative, such as the ReLU function:</p>
<p><span class="math display">\[
    f&#39;(x) = \begin{cases} 1 \quad \text{if} \quad x &gt; 0 \\ 0 \quad \text{otherwise.} \end{cases}
\]</span></p>
<p>Partial derivatives are automatically computed by the underlying libraries, such as tensorflow, theano, pytorch, etc. The next step is choose an <strong>optimizer</strong>, i.e. a gradient-based optimization method allow to modify the free parameters using the gradients. Optimizers do not work on the whole training set, but use <strong>minibatches</strong> (a random sample of training examples: their number is called the <em>batch size</em>) to compute iteratively the loss function. The most popular optimizers are:</p>
<ul>
<li>SGD (stochastic gradient descent): vanilla gradient descent on random minibatches.</li>
<li>SGD with momentum (Nesterov or not): additional momentum to avoid local minima of the loss function.</li>
<li>Adagrad</li>
<li>Adadelta</li>
<li>RMSprop</li>
<li>Adam</li>
<li>Many others. Check the doc of keras to see what is available: <a href="https://keras.io/optimizers" class="uri">https://keras.io/optimizers</a></li>
</ul>
<p>See this useful post for a comparison of the different optimizers: <a href="http://ruder.io/optimizing-gradient-descent" class="uri">http://ruder.io/optimizing-gradient-descent</a> <span class="citation" data-cites="Ruder2016">(Ruder, <a href="References.html#ref-Ruder2016" role="doc-biblioref">2016</a>)</span>. The common wisdom is that SGD with Nesterov momentum works best (i.e. it finds a better minimum) but its meta-parameters (learning rate, momentum) are hard to find, while Adam works out-of-the-box, at the cost of a slightly worse minimum. For deep RL, Adam is usually preferred, as the goal is to quickly find a working solution, not to optimize it to the last decimal.</p>
<!-- ![Comparison of different optimizers. Source: @Ruder2016, <http://ruder.io/optimizing-gradient-descent>.](img/optimizers.gif){#fig:optimizers width=50%} -->
<p>Additional regularization mechanisms are now typically part of DNNs in order to avoid overfitting (learning by heart the training set but failing to generalize): L1/L2 regularization, dropout, batch normalization, etc. Refer to <span class="citation" data-cites="Goodfellow2016">Goodfellow et al. (<a href="References.html#ref-Goodfellow2016" role="doc-biblioref">2016</a>)</span> for further details.</p>
<h3 id="sec:convolutional-networks"><span class="header-section-number">2.2.2</span> Convolutional networks</h3>
<p>Convolutional Neural Networks (CNN) are an adaptation of DNNs to deal with highly dimensional input spaces such as images. The idea is that neurons in the hidden layer reuse (“share”) weights over the input image, as the features learned by early layers are probably local in visual classification tasks: in computer vision, an edge can be detected by the same filter all over the input image.</p>
<p>A <strong>convolutional layer</strong> learns to extract a given number of features (typically 16, 32, 64, etc) represented by 3x3 or 5x5 matrices. These matrices are then convoluted over the whole input image (or the previous convolutional layer) to produce <strong>feature maps</strong>. If the input image has a size NxMx1 (grayscale) or NxMx3 (colored), the convolutional layer will be a tensor of size NxMxF, where F is the number of extracted features. Padding issues may reduce marginally the spatial dimensions. One important aspect is that the convolutional layer is fully differentiable, so backpropagation and the usual optimizers can be used to learn the filters.</p>
<figure>
<img src="img/convlayer.gif" id="fig:convlayer" style="width:50.0%" alt="" /><figcaption>Figure 10: Convolutional layer. Source: <a href="https://github.com/vdumoulin/conv_arithmetic" class="uri">https://github.com/vdumoulin/conv_arithmetic</a>.</figcaption>
</figure>
<p>After a convolutional layer, the spatial dimensions are preserved. In classification tasks, it does not matter where the object is in the image, the only thing that matters is what it is: classification requires <strong>spatial invariance</strong> in the learned representations. The <strong>max-pooling layer</strong> was introduced to downsample each feature map individually and increase their spatial invariance. Each feature map is divided into 2x2 blocks (generally): only the maximal feature activation in that block is preserved in the max-pooling layer. This reduces the spatial dimensions by a factor two in each direction, but keeps the number of features equal.</p>
<figure>
<img src="img/maxpooling.png" id="fig:maxpooling" alt="" /><figcaption>Figure 11: Max-pooling layer. Source: Stanford’s CS231n course <a href="http://cs231n.github.io/convolutional-networks" class="uri">http://cs231n.github.io/convolutional-networks</a></figcaption>
</figure>
<p>A convolutional neural network is simply a sequence of convolutional layers and max-pooling layers (sometime two convolutional layers are applied in a row before max-pooling, as in VGG <span class="citation" data-cites="Simonyan2015">(Simonyan and Zisserman, <a href="References.html#ref-Simonyan2015" role="doc-biblioref">2015</a>)</span>), followed by a couple of fully-connected layers and a softmax output layer. Fig. <a href="#fig:alexnet">12</a> shows the architecture of AlexNet, the winning architecture of the ImageNet challenge in 2012 <span class="citation" data-cites="Krizhevsky2012">(Krizhevsky et al., <a href="References.html#ref-Krizhevsky2012" role="doc-biblioref">2012</a>)</span>.</p>
<figure>
<img src="img/alexnet.png" id="fig:alexnet" alt="" /><figcaption>Figure 12: Architecture of the AlexNet CNN. Taken from <span class="citation" data-cites="Krizhevsky2012">Krizhevsky et al. (<a href="References.html#ref-Krizhevsky2012" role="doc-biblioref">2012</a>)</span>.</figcaption>
</figure>
<p>Many improvements have been proposed since 2012 (e.g. ResNets <span class="citation" data-cites="He2015">(He et al., <a href="References.html#ref-He2015" role="doc-biblioref">2015</a>)</span>) but the idea stays similar. Generally, convolutional and max-pooling layers are alternated until the spatial dimensions are so reduced (around 10x10) that they can be put into a single vector and fed into a fully-connected layer. This is <strong>NOT</strong> the case in deep RL! Contrary to object classification, spatial information is crucial in deep RL: position of the ball, position of the body, etc. It matters whether the ball is to the right or to the left of your paddle when you decide how to move it. Max-pooling layers are therefore omitted and the CNNs only consist of convolutional and fully-connected layers. This greatly increases the number of weights in the networks, hence the number of training examples needed to train the network. This is still the main limitation of using CNNs in deep RL.</p>
<h3 id="sec:recurrent-neural-networks"><span class="header-section-number">2.2.3</span> Recurrent neural networks</h3>
<p>Feedforward neural networks learn to efficiently map static inputs <span class="math inline">\(\mathbf{x}\)</span> to outputs <span class="math inline">\(\mathbf{y}\)</span> but have no memory or context: the output at time <span class="math inline">\(t\)</span> does not depend on the inputs at time <span class="math inline">\(t-1\)</span> or <span class="math inline">\(t-2\)</span>, only the one at time <span class="math inline">\(t\)</span>. This is problematic when dealing with video sequences for example: if the task is to classify videos into happy/sad, a frame by frame analysis is going to be inefficient (most frames a neutral). Concatenating all frames in a giant input vector would increase dramatically the complexity of the classifier and no generalization can be expected.</p>
<p>Recurrent Neural Networks (RNN) are designed to deal with time-varying inputs, where the relevant information to take a decision at time <span class="math inline">\(t\)</span> may have happened at different times in the past. The general structure of a RNN is depicted on Fig. <a href="#fig:rnn">13</a>:</p>
<figure>
<img src="img/RNN-unrolled.png" id="fig:rnn" style="width:90.0%" alt="" /><figcaption>Figure 13: Architecture of a RNN. Left: recurrent architecture. Right: unrolled network, showing that a RNN is equivalent to a deep network. Taken from <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs" class="uri">http://colah.github.io/posts/2015-08-Understanding-LSTMs</a>.</figcaption>
</figure>
<p>The output <span class="math inline">\(\mathbf{h}_t\)</span> of the RNN at time <span class="math inline">\(t\)</span> depends on its current input <span class="math inline">\(\mathbf{x}_t\)</span>, but also on its previous output <span class="math inline">\(\mathbf{h}_{t-1}\)</span>, which, by recursion, depends on the whole history of inputs <span class="math inline">\((x_0, x_1, \ldots, x_t)\)</span>.</p>
<p><span class="math display">\[
    \mathbf{h}_t = f(W_x \, \mathbf{x}_{t} + W_h \, \mathbf{h}_{t-1} + \mathbf{b})
\]</span></p>
<p>Once unrolled, a RNN is equivalent to a deep network, with <span class="math inline">\(t\)</span> layers of weights between the first input <span class="math inline">\(\mathbf{x}_0\)</span> and the current output <span class="math inline">\(\mathbf{h}_t\)</span>. The only difference with a feedforward network is that weights are reused between two time steps / layers. <strong>Backpropagation though time</strong> (BPTT) can be used to propagate the gradient of the loss function backwards in time and learn the weights <span class="math inline">\(W_x\)</span> and <span class="math inline">\(W_h\)</span> using the usual optimizer (SGD, Adam…).</p>
<p>However, this kind of RNN can only learn short-term dependencies because of the <strong>vanishing gradient problem</strong> <span class="citation" data-cites="Hochreiter1991">(Hochreiter, <a href="References.html#ref-Hochreiter1991" role="doc-biblioref">1991</a>)</span>. When the gradient of the loss function travels backwards from <span class="math inline">\(\mathbf{h}_t\)</span> to <span class="math inline">\(\mathbf{x}_0\)</span>, it will be multiplied <span class="math inline">\(t\)</span> times by the recurrent weights <span class="math inline">\(W_h\)</span>. If <span class="math inline">\(|W_h| &gt; 1\)</span>, the gradient will explode with increasing <span class="math inline">\(t\)</span>, while if <span class="math inline">\(|W_h| &lt; 1\)</span>, the gradient will vanish to 0.</p>
<p>The solution to this problem is provided by <strong>long short-term memory networks</strong> <span class="citation" data-cites="Hochreiter1997">(LSTM; Hochreiter and Schmidhuber, <a href="References.html#ref-Hochreiter1997" role="doc-biblioref">1997</a>)</span>. LSTM layers maintain additionally a state <span class="math inline">\(\mathbf{C}_t\)</span> (also called context or memory) which is manipulated by three learnable gates (input, forget and output gates). As in regular RNNs, a <em>candidate state</em> <span class="math inline">\(\tilde{\mathbf{C}_t}\)</span> is computed based on the current input and the previous output:</p>
<p><span class="math display">\[
    \tilde{\mathbf{C}_t} = f(W_x \, \mathbf{x}_{t} + W_h \, \mathbf{h}_{t-1} + \mathbf{b})
\]</span></p>
<figure>
<img src="img/LSTM.png" id="fig:lstm" style="width:40.0%" alt="" /><figcaption>Figure 14: Architecture of a LSTM layer. Taken from <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs" class="uri">http://colah.github.io/posts/2015-08-Understanding-LSTMs</a>.</figcaption>
</figure>
<p>The activation function <span class="math inline">\(f\)</span> is usually a tanh function. The input and forget learn to decide how the candidate state should be used to update the current state:</p>
<ul>
<li>The input gate decides which part of the candidate state <span class="math inline">\(\tilde{\mathbf{C}_t}\)</span> will be used to update the current state <span class="math inline">\(\mathbf{C}_t\)</span>:</li>
</ul>
<p><span class="math display">\[
    \mathbf{i}_t = \sigma(W^i_x \, \mathbf{x}_{t} + W^i_h \, \mathbf{h}_{t-1} + \mathbf{b}^i)
\]</span></p>
<p>The sigmoid activation function <span class="math inline">\(\sigma\)</span> is used to output a number between 0 and 1 for each neuron: 0 means the candidate state will not be used at all, 1 means completely.</p>
<ul>
<li>The forget gate decides which part of the current state should be kept or forgotten:</li>
</ul>
<p><span class="math display">\[
    \mathbf{f}_t = \sigma(W^f_x \, \mathbf{x}_{t} + W^f_h \, \mathbf{h}_{t-1} + \mathbf{b}^f)
\]</span></p>
<p>Similarly, 0 means that the corresponding element of the current state will be erased, 1 that it will be kept.</p>
<p>Once the input and forget gates are computed, the current state can be updated based on its previous value and the candidate state:</p>
<p><span class="math display">\[
   \mathbf{C}_t =  \mathbf{i}_t \odot \tilde{\mathbf{C}_t} + \mathbf{f}_t \odot \mathbf{C}_{t-1}
\]</span></p>
<p>where <span class="math inline">\(\odot\)</span> is the element-wise multiplication.</p>
<ul>
<li>The output gate finally learns to select which part of the current state <span class="math inline">\(\mathbf{C}_t\)</span> should be used to produce the current output <span class="math inline">\(\mathbf{h}_t\)</span>:</li>
</ul>
<p><span class="math display">\[
    \mathbf{o}_t = \sigma(W^o_x \, \mathbf{x}_{t} + W^o_h \, \mathbf{h}_{t-1} + \mathbf{b}^o)
\]</span></p>
<p><span class="math display">\[
    \mathbf{h}_t = \mathbf{o}_t \odot \tanh \mathbf{C}_t
\]</span></p>
<p>The architecture may seem complex, but everything is differentiable: backpropagation though time can be used to learn not only the input and recurrent weights for the candidate state, but also the weights and and biases of the gates. The main advantage of LSTMs is that they solve the vanishing gradient problem: if the input at time <span class="math inline">\(t=0\)</span> is important to produce a response at time <span class="math inline">\(t\)</span>, the input gate will learn to put it into the memory and the forget gate will learn to maintain in the current state until it is not needed anymore. During this “working memory” phase, the gradient is multiplied by exactly one as nothing changes: the dependency can be learned with arbitrary time delays!</p>
<p>There are alternatives to the classical LSTM layer such as the gated recurrent unit <span class="citation" data-cites="Cho2014">(GRU; Cho et al., <a href="References.html#ref-Cho2014" role="doc-biblioref">2014</a>)</span> or peephole connections <span class="citation" data-cites="Gers2001">(Gers, <a href="References.html#ref-Gers2001" role="doc-biblioref">2001</a>)</span>. See <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs" class="uri">http://colah.github.io/posts/2015-08-Understanding-LSTMs</a>, <a href="https://medium.com/mlreview/understanding-lstm-and-its-diagrams-37e2f46f1714" class="uri">https://medium.com/mlreview/understanding-lstm-and-its-diagrams-37e2f46f1714</a> or <a href="http://blog.echen.me/2017/05/30/exploring-lstms/" class="uri">http://blog.echen.me/2017/05/30/exploring-lstms/</a> for more visual explanations of LSTMs and their variants.</p>
<p>RNNs are particularly useful for deep RL when considering POMDPs, i.e. partially observable problems. If an observation does not contain enough information about the underlying state (e.g. a single image does not contain speed information), LSTM can integrate these observations over time and learn to implicitly represent speed in its context vector, allowing efficient policies to be learned.</p>

<br>
<div class="arrows">
<a href="BasicRL.html" class="previous">&laquo; Previous</a>
<a href="Valuebased.html" class="next">Next &raquo;</a>
</div>

</article>

</body>
</html>
