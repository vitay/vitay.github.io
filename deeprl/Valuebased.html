<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <meta name="author" content="Julien Vitay">
  <title>Deep Reinforcement Learning</title>


  <link rel="stylesheet" href="assets/syntax.css">
  <link rel="stylesheet" href="assets/menu.css">
  <link rel="stylesheet" href="assets/github.css">
  <script src="assets/menu.js" type="text/javascript"></script>
  <script src="/usr/share/mathjax2/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->

</head>

<body class="markdown-body">

<div class="navbar">
  <div class="dropdown" id="dropdown">
    <button class="dropbtn">
      <div class="dropicon-first"></div>
      <div class="dropicon"></div>
      <div class="dropicon"></div>
      <i class="fa fa-caret-down"></i>
    </button> 
    <span class="navbar-title">Deep Reinforcement Learning - Julien Vitay</span>
    <div class="dropdown-content">

<ul>
<li><a href="./Introduction.html#sec:introduction"><span class="toc-section-number">1</span> Introduction</a></li>
<li><a href="./BasicRL.html#sec:basics"><span class="toc-section-number">2</span> Basics</a><ul>
<li><a href="./BasicRL.html#sec:reinforcement-learning-and-markov-decision-process"><span class="toc-section-number">2.1</span> Reinforcement learning and Markov Decision Process</a><ul>
<li><a href="./BasicRL.html#sec:policy-and-value-functions"><span class="toc-section-number">2.1.1</span> Policy and value functions</a></li>
<li><a href="./BasicRL.html#sec:bellman-equations"><span class="toc-section-number">2.1.2</span> Bellman equations</a></li>
<li><a href="./BasicRL.html#sec:dynamic-programming"><span class="toc-section-number">2.1.3</span> Dynamic programming</a></li>
<li><a href="./BasicRL.html#sec:monte-carlo-sampling"><span class="toc-section-number">2.1.4</span> Monte-Carlo sampling</a></li>
<li><a href="./BasicRL.html#sec:temporal-difference"><span class="toc-section-number">2.1.5</span> Temporal Difference</a></li>
<li><a href="./BasicRL.html#sec:eligibility-traces"><span class="toc-section-number">2.1.6</span> Eligibility traces</a></li>
<li><a href="./BasicRL.html#sec:actor-critic-architectures"><span class="toc-section-number">2.1.7</span> Actor-critic architectures</a></li>
<li><a href="./BasicRL.html#sec:function-approximation"><span class="toc-section-number">2.1.8</span> Function approximation</a></li>
</ul></li>
<li><a href="./DeepLearning.html#sec:deep-learning"><span class="toc-section-number">2.2</span> Deep learning</a><ul>
<li><a href="./DeepLearning.html#sec:deep-neural-networks"><span class="toc-section-number">2.2.1</span> Deep neural networks</a></li>
<li><a href="./DeepLearning.html#sec:convolutional-networks"><span class="toc-section-number">2.2.2</span> Convolutional networks</a></li>
<li><a href="./DeepLearning.html#sec:recurrent-neural-networks"><span class="toc-section-number">2.2.3</span> Recurrent neural networks</a></li>
</ul></li>
</ul></li>
<li><a href="./Valuebased.html#sec:value-based-methods"><span class="toc-section-number">3</span> Value-based methods</a><ul>
<li><a href="./Valuebased.html#sec:limitations-of-deep-neural-networks-for-function-approximation"><span class="toc-section-number">3.1</span> Limitations of deep neural networks for function approximation</a></li>
<li><a href="./Valuebased.html#sec:deep-q-network-dqn"><span class="toc-section-number">3.2</span> Deep Q-Network (DQN)</a></li>
<li><a href="./Valuebased.html#sec:double-dqn"><span class="toc-section-number">3.3</span> Double DQN</a></li>
<li><a href="./Valuebased.html#sec:prioritized-experience-replay"><span class="toc-section-number">3.4</span> Prioritized experience replay</a></li>
<li><a href="./Valuebased.html#sec:duelling-network"><span class="toc-section-number">3.5</span> Duelling network</a></li>
<li><a href="./Valuebased.html#sec:distributed-dqn-gorila"><span class="toc-section-number">3.6</span> Distributed DQN (GORILA)</a></li>
<li><a href="./Valuebased.html#sec:deep-recurrent-q-learning-drqn"><span class="toc-section-number">3.7</span> Deep Recurrent Q-learning (DRQN)</a></li>
<li><a href="./Valuebased.html#sec:other-variants-of-dqn"><span class="toc-section-number">3.8</span> Other variants of DQN</a></li>
</ul></li>
<li><a href="./PolicyGradient.html#sec:policy-gradient-methods"><span class="toc-section-number">4</span> Policy Gradient methods</a><ul>
<li><a href="./PolicyGradient.html#sec:reinforce"><span class="toc-section-number">4.1</span> REINFORCE</a><ul>
<li><a href="./PolicyGradient.html#sec:estimating-the-policy-gradient"><span class="toc-section-number">4.1.1</span> Estimating the policy gradient</a></li>
<li><a href="./PolicyGradient.html#sec:reducing-the-variance"><span class="toc-section-number">4.1.2</span> Reducing the variance</a></li>
<li><a href="./PolicyGradient.html#sec:policy-gradient-theorem"><span class="toc-section-number">4.1.3</span> Policy Gradient theorem</a></li>
</ul></li>
<li><a href="./ActorCritic.html#sec:advantage-actor-critic-methods"><span class="toc-section-number">4.2</span> Advantage Actor-Critic methods</a><ul>
<li><a href="./ActorCritic.html#sec:advantage-actor-critic-a2c"><span class="toc-section-number">4.2.1</span> Advantage Actor-Critic (A2C)</a></li>
<li><a href="./ActorCritic.html#sec:asynchronous-advantage-actor-critic-a3c"><span class="toc-section-number">4.2.2</span> Asynchronous Advantage Actor-Critic (A3C)</a></li>
<li><a href="./ActorCritic.html#sec:generalized-advantage-estimation-gae"><span class="toc-section-number">4.2.3</span> Generalized Advantage Estimation (GAE)</a></li>
<li><a href="./ActorCritic.html#sec:stochastic-actor-critic-for-continuous-action-spaces"><span class="toc-section-number">4.2.4</span> Stochastic actor-critic for continuous action spaces</a></li>
</ul></li>
<li><a href="./ImportanceSampling.html#sec:off-policy-actor-critic"><span class="toc-section-number">4.3</span> Off-policy Actor-Critic</a><ul>
<li><a href="./ImportanceSampling.html#sec:importance-sampling"><span class="toc-section-number">4.3.1</span> Importance sampling</a></li>
<li><a href="./ImportanceSampling.html#sec:linear-off-policy-actor-critic-off-pac"><span class="toc-section-number">4.3.2</span> Linear Off-Policy Actor-Critic (Off-PAC)</a></li>
<li><a href="./ImportanceSampling.html#sec:retrace"><span class="toc-section-number">4.3.3</span> Retrace</a></li>
<li><a href="./ImportanceSampling.html#sec:self-imitation-learning-sil"><span class="toc-section-number">4.3.4</span> Self-Imitation Learning (SIL)</a></li>
</ul></li>
<li><a href="./DPG.html#sec:deterministic-policy-gradient-dpg"><span class="toc-section-number">4.4</span> Deterministic Policy Gradient (DPG)</a><ul>
<li><a href="./DPG.html#sec:deterministic-policy-gradient-theorem"><span class="toc-section-number">4.4.1</span> Deterministic policy gradient theorem</a></li>
<li><a href="./DPG.html#sec:deep-deterministic-policy-gradient-ddpg"><span class="toc-section-number">4.4.2</span> Deep Deterministic Policy Gradient (DDPG)</a></li>
<li><a href="./DPG.html#sec:distributed-distributional-ddpg-d4pg"><span class="toc-section-number">4.4.3</span> Distributed Distributional DDPG (D4PG)</a></li>
</ul></li>
<li><a href="./NaturalGradient.html#sec:natural-gradients"><span class="toc-section-number">4.5</span> Natural Gradients</a><ul>
<li><a href="./NaturalGradient.html#sec:principle-of-natural-gradients"><span class="toc-section-number">4.5.1</span> Principle of natural gradients</a></li>
<li><a href="./NaturalGradient.html#sec:natural-policy-gradient-and-natural-actor-critic-nac"><span class="toc-section-number">4.5.2</span> Natural policy gradient and Natural Actor Critic (NAC)</a></li>
<li><a href="./NaturalGradient.html#sec:trust-region-policy-optimization-trpo"><span class="toc-section-number">4.5.3</span> Trust Region Policy Optimization (TRPO)</a></li>
<li><a href="./NaturalGradient.html#sec:proximal-policy-optimization-ppo"><span class="toc-section-number">4.5.4</span> Proximal Policy Optimization (PPO)</a></li>
<li><a href="./NaturalGradient.html#sec:actor-critic-with-experience-replay-acer"><span class="toc-section-number">4.5.5</span> Actor-Critic with Experience Replay (ACER)</a></li>
</ul></li>
<li><a href="./EntropyRL.html#sec:maximum-entropy-rl"><span class="toc-section-number">4.6</span> Maximum Entropy RL</a><ul>
<li><a href="#sec:entropy-regularization-1"><span class="toc-section-number">4.6.1</span> Entropy regularization</a></li>
<li><a href="./EntropyRL.html#sec:soft-q-learning"><span class="toc-section-number">4.6.2</span> Soft Q-learning</a></li>
<li><a href="./EntropyRL.html#sec:soft-actor-critic-sac"><span class="toc-section-number">4.6.3</span> Soft Actor-Critic (SAC)</a></li>
</ul></li>
<li><a href="./DistributionalRL.html#sec:distributional-learning"><span class="toc-section-number">4.7</span> Distributional learning</a><ul>
<li><a href="./DistributionalRL.html#sec:categorical-dqn"><span class="toc-section-number">4.7.1</span> Categorical DQN</a></li>
<li><a href="./DistributionalRL.html#sec:the-reactor"><span class="toc-section-number">4.7.2</span> The Reactor</a></li>
</ul></li>
<li><a href="./OtherPolicyGradient.html#sec:miscellaneous-model-free-algorithm"><span class="toc-section-number">4.8</span> Miscellaneous model-free algorithm</a><ul>
<li><a href="./OtherPolicyGradient.html#sec:stochastic-value-gradient-svg"><span class="toc-section-number">4.8.1</span> Stochastic Value Gradient (SVG)</a></li>
<li><a href="./OtherPolicyGradient.html#sec:q-prop"><span class="toc-section-number">4.8.2</span> Q-Prop</a></li>
<li><a href="./OtherPolicyGradient.html#sec:normalized-advantage-function-naf"><span class="toc-section-number">4.8.3</span> Normalized Advantage Function (NAF)</a></li>
<li><a href="./OtherPolicyGradient.html#sec:fictitious-self-play-fsp"><span class="toc-section-number">4.8.4</span> Fictitious Self-Play (FSP)</a></li>
</ul></li>
<li><a href="./OtherPolicyGradient.html#sec:comparison-between-value-based-and-policy-gradient-methods"><span class="toc-section-number">4.9</span> Comparison between value-based and policy gradient methods</a></li>
<li><a href="./OtherPolicyGradient.html#sec:gradient-free-policy-search"><span class="toc-section-number">4.10</span> Gradient-free policy search</a><ul>
<li><a href="./OtherPolicyGradient.html#sec:cross-entropy-method-cem"><span class="toc-section-number">4.10.1</span> Cross-entropy Method (CEM)</a></li>
<li><a href="./OtherPolicyGradient.html#sec:evolutionary-search-es"><span class="toc-section-number">4.10.2</span> Evolutionary Search (ES)</a></li>
</ul></li>
</ul></li>
<li><a href="./ModelBased.html#sec:model-based-rl"><span class="toc-section-number">5</span> Model-based RL</a><ul>
<li><a href="./ModelBased.html#sec:dyna-q"><span class="toc-section-number">5.1</span> Dyna-Q</a></li>
<li><a href="./ModelBased.html#sec:unsorted-references"><span class="toc-section-number">5.2</span> Unsorted references</a></li>
</ul></li>
<li><a href="./Practice.html#sec:deep-rl-in-practice"><span class="toc-section-number">6</span> Deep RL in practice</a><ul>
<li><a href="./Practice.html#sec:limitations"><span class="toc-section-number">6.1</span> Limitations</a></li>
<li><a href="./Practice.html#sec:simulation-environments"><span class="toc-section-number">6.2</span> Simulation environments</a></li>
<li><a href="./Practice.html#sec:algorithm-implementations"><span class="toc-section-number">6.3</span> Algorithm implementations</a></li>
</ul></li>
<li><a href="./References.html#sec:references">References</a></li>
</ul>


    </div>
  </div>
</div>

<article>

<h1 id="sec:value-based-methods"><span class="header-section-number">3</span> Value-based methods</h1>
<h2 id="sec:limitations-of-deep-neural-networks-for-function-approximation"><span class="header-section-number">3.1</span> Limitations of deep neural networks for function approximation</h2>
<p>The goal of value-based deep RL is to approximate the Q-value of each possible state-action pair using a deep (convolutional) neural network. As shown on Fig. <a href="#fig:functionapprox2">15</a>, the network can either take a state-action pair as input and return a single output value, or take only the state as input and return the Q-value of all possible actions (only possible if the action space is discrete), In both cases, the goal is to learn estimates <span class="math inline">\(Q_\theta(s, a)\)</span> with a NN with parameters <span class="math inline">\(\theta\)</span>.</p>
<figure>
<img src="img/functionapprox.png" id="fig:functionapprox2" style="width:60.0%" alt="" /><figcaption>Figure 15: Function approximators can either associate a state-action pair <span class="math inline">\((s, a)\)</span> to its Q-value (left), or associate a state <span class="math inline">\(s\)</span> to the Q-values of all actions possible in that state (right).</figcaption>
</figure>
<p>When using Q-learning, we have already seen in Section <a href="./BasicRL.html#sec:function-approximation">2.1.8</a> that the problem is a regression problem, where the following mse loss function has to be minimized:</p>
<p><span class="math display">\[
    \mathcal{L}(\theta) = \mathbb{E}_\pi[(r_t + \gamma \, \max_{a&#39;} Q_\theta(s&#39;, a&#39;) - Q_\theta(s, a))^2]
\]</span></p>
<p>In short, we want to reduce the prediction error, i.e. the mismatch between the estimate of the value of an action <span class="math inline">\(Q_\theta(s, a)\)</span> and the real return, here approximated with <span class="math inline">\(r(s, a, s&#39;) + \gamma \, \text{max}_{a&#39;} Q_\theta(s&#39;, a&#39;)\)</span>.</p>
<p>We can compute this loss by gathering enough samples <span class="math inline">\((s, a, r, s&#39;)\)</span> (i.e. single transitions), concatenating them randomly in minibatches, and let the DNN learn to minimize the prediction error using backpropagation and SGD, indirectly improving the policy. The following pseudocode would describe the training procedure when gathering transitions <strong>online</strong>, i.e. when directly interacting with the environment:</p>
<hr />
<ul>
<li>Initialize value network <span class="math inline">\(Q_{\theta}\)</span> with random weights.</li>
<li>Initialize empty minibatch <span class="math inline">\(\mathcal{D}\)</span> of maximal size <span class="math inline">\(n\)</span>.</li>
<li>Observe the initial state <span class="math inline">\(s_0\)</span>.</li>
<li>for <span class="math inline">\(t \in [0, T_\text{total}]\)</span>:
<ul>
<li>Select the action <span class="math inline">\(a_t\)</span> based on the behavior policy derived from <span class="math inline">\(Q_\theta(s_t, a)\)</span> (e.g. softmax).</li>
<li>Perform the action <span class="math inline">\(a_t\)</span> and observe the next state <span class="math inline">\(s_{t+1}\)</span> and the reward <span class="math inline">\(r_{t+1}\)</span>.</li>
<li>Predict the Q-value of the greedy action in the next state <span class="math inline">\(\max_{a&#39;} Q_\theta(s_{t+1}, a&#39;)\)</span></li>
<li>Store <span class="math inline">\((s_t, a_t, r_{t+1} + \gamma \, \max_{a&#39;} Q_\theta(s_{t+1}, a&#39;))\)</span> in the minibatch.</li>
<li>If minibatch <span class="math inline">\(\mathcal{D}\)</span> is full:
<ul>
<li>Train the value network <span class="math inline">\(Q_{\theta}\)</span> on <span class="math inline">\(\mathcal{D}\)</span> to minimize <span class="math inline">\(\mathcal{L}(\theta) = \mathbb{E}_\mathcal{D}[(r(s, a, s&#39;) + \gamma \, \text{max}_{a&#39;} Q_\theta(s&#39;, a&#39;) - Q_\theta(s, a))^2]\)</span></li>
<li>Empty the minibatch <span class="math inline">\(\mathcal{D}\)</span>.</li>
</ul></li>
</ul></li>
</ul>
<hr />
<p>However, the definition of the loss function uses the mathematical expectation operator <span class="math inline">\(E\)</span> over all transitions, which can only be approximated by <strong>randomly</strong> sampling the distribution (the MDP). This implies that the samples concatenated in a minibatch should be independent from each other (i.i.d). When gathering transitions online, the samples are correlated: <span class="math inline">\((s_t, a_t, r_{t+1}, s_{t+1})\)</span> will be followed by <span class="math inline">\((s_{t+1}, a_{t+1}, r_{t+2}, s_{t+2})\)</span>, etc. When playing video games, two successive frames will be very similar (a few pixels will change, or even none if the sampling rate is too high) and the optimal action will likely not change either (to catch the ball in pong, you will need to perform the same action - going left - many times in a row).</p>
<p><strong>Correlated inputs/outputs</strong> are very bad for deep neural networks: the DNN will overfit and fall into a very bad local minimum. That is why stochastic gradient descent works so well: it randomly samples values from the training set to form minibatches and minimize the loss function on these uncorrelated samples (hopefully). If all samples of a minibatch were of the same class (e.g. zeros in MNIST), the network would converge poorly. This is the first problem preventing an easy use of deep neural networks as function approximators in RL.</p>
<p>The second major problem is the <strong>non-stationarity</strong> of the targets in the loss function. In classification or regression, the desired values <span class="math inline">\(\mathbf{t}\)</span> are fixed throughout learning: the class of an object does not change in the middle of the training phase.</p>
<p><span class="math display">\[
    \mathcal{L}(\theta) = - \mathbb{E}_{\mathbf{x}, \mathbf{t} \in \mathcal{D}}[ ||\mathbf{t} - \mathbf{y}||^2]
\]</span></p>
<p>In Q-learning, the target <span class="math inline">\(r(s, a, s&#39;) + \gamma \, \max_{a&#39;} Q_\theta(s&#39;, a&#39;)\)</span> will change during learning, as <span class="math inline">\(Q_\theta(s&#39;, a&#39;)\)</span> depends on the weights <span class="math inline">\(\theta\)</span> and will hopefully increase as the performance improves. This is the second problem of deep RL: deep NN are particularly bad on non-stationary problems, especially feedforward networks. They iteratively converge towards the desired value, but have troubles when the target also moves (like a dog chasing its tail).</p>
<h2 id="sec:deep-q-network-dqn"><span class="header-section-number">3.2</span> Deep Q-Network (DQN)</h2>
<p><span class="citation" data-cites="Mnih2015">Mnih et al. (<a href="References.html#ref-Mnih2015" role="doc-biblioref">2015</a>)</span> (originally arXived in <span class="citation" data-cites="Mnih2013">Mnih et al. (<a href="References.html#ref-Mnih2013" role="doc-biblioref">2013</a>)</span>) proposed an elegant solution to the problems of correlated inputs/outputs and non-stationarity inherent to RL. This article is a milestone of deep RL and it is fair to say that it started or at least strongly renewed the interest for deep RL.</p>
<p>The first idea proposed by <span class="citation" data-cites="Mnih2015">Mnih et al. (<a href="References.html#ref-Mnih2015" role="doc-biblioref">2015</a>)</span> solves the problem of correlated input/outputs and is actually quite simple: instead of feeding successive transitions into a minibatch and immediately training the NN on it, transitions are stored in a huge buffer called <strong>experience replay memory</strong> (ERM) or <strong>replay buffer</strong> able to store 100000 transitions. When the buffer is full, new transitions replace the old ones. SGD can now randomly sample the ERM to form minibatches and train the NN.</p>
<figure>
<img src="img/ERM.png" id="fig:erm" style="width:40.0%" alt="" /><figcaption>Figure 16: Experience replay memory. Interactions with the environment are stored in the ERM. Random minibatches are sampled from it to train the DQN value network.</figcaption>
</figure>
<p>The second idea solves the non-stationarity of the targets <span class="math inline">\(r(s, a, s&#39;) + \gamma \, \max_{a&#39;} Q_\theta(s&#39;, a&#39;)\)</span>. Instead of computing it with the current parameters <span class="math inline">\(\theta\)</span> of the NN, they are computed with an old version of the NN called the <strong>target network</strong> with parameters <span class="math inline">\(\theta&#39;\)</span>. The target network is updated only infrequently (every thousands of iterations or so) with the learned weights <span class="math inline">\(\theta\)</span>. As this target network does not change very often, the targets stay constant for a long period of time, and the problem becomes more stationary.</p>
<p>The resulting algorithm is called <strong>Deep Q-Network (DQN)</strong>. It is summarized by the following pseudocode:</p>
<hr />
<ul>
<li>Initialize value network <span class="math inline">\(Q_{\theta}\)</span> with random weights.</li>
<li>Copy <span class="math inline">\(Q_{\theta}\)</span> to create the target network <span class="math inline">\(Q_{\theta&#39;}\)</span>.</li>
<li>Initialize experience replay memory <span class="math inline">\(\mathcal{D}\)</span> of maximal size <span class="math inline">\(N\)</span>.</li>
<li>Observe the initial state <span class="math inline">\(s_0\)</span>.</li>
<li>for <span class="math inline">\(t \in [0, T_\text{total}]\)</span>:
<ul>
<li>Select the action <span class="math inline">\(a_t\)</span> based on the behavior policy derived from <span class="math inline">\(Q_\theta(s_t, a)\)</span> (e.g. softmax).</li>
<li>Perform the action <span class="math inline">\(a_t\)</span> and observe the next state <span class="math inline">\(s_{t+1}\)</span> and the reward <span class="math inline">\(r_{t+1}\)</span>.</li>
<li>Store <span class="math inline">\((s_t, a_t, r_{t+1}, s_{t+1})\)</span> in the experience replay memory.</li>
<li>Every <span class="math inline">\(T_\text{train}\)</span> steps:
<ul>
<li>Sample a minibatch <span class="math inline">\(\mathcal{D}_s\)</span> randomly from <span class="math inline">\(\mathcal{D}\)</span>.</li>
<li>For each transition <span class="math inline">\((s, a, r, s&#39;)\)</span> in the minibatch:
<ul>
<li>Predict the Q-value of the greedy action in the next state <span class="math inline">\(\max_{a&#39;} Q_{\theta&#39;}(s&#39;, a&#39;)\)</span> using the target network.</li>
<li>Compute the target value <span class="math inline">\(y = r + \gamma \, \max_{a&#39;} Q_{\theta&#39;}(s&#39;, a&#39;)\)</span>.</li>
</ul></li>
<li>Train the value network <span class="math inline">\(Q_{\theta}\)</span> on <span class="math inline">\(\mathcal{D}_s\)</span> to minimize <span class="math inline">\(\mathcal{L}(\theta) = \mathbb{E}_{\mathcal{D}_s}[(y - Q_\theta(s, a))^2]\)</span></li>
</ul></li>
<li>Every <span class="math inline">\(T_\text{target}\)</span> steps:
<ul>
<li>Update the target network with the trained value network: <span class="math inline">\(\theta&#39; \leftarrow \theta\)</span></li>
</ul></li>
</ul></li>
</ul>
<hr />
<p>In this document, pseudocode will omit many details to simplify the explanations (for example here, the case where a state is terminal - the game ends - and the next state has to be chosen from the distribution of possible starting states). Refer to the original publication for more exact algorithms.</p>
<p>The first thing to notice is that experienced transitions are not immediately used for learning, but simply stored in the ERM to be sampled later. Due to the huge size of the ERM, it is even likely that the recently experienced transition will only be used for learning hundreds or thousands of steps later. Meanwhile, very old transitions, generated using an initially bad policy, can be used to train the network for a very long time.</p>
<p>The second thing is that the target network is not updated very often (<span class="math inline">\(T_\text{target}=10000\)</span>), so the target values are going to be wrong a long time. More recent algorithms such as DDPG (Section <a href="./DPG.html#sec:deep-deterministic-policy-gradient-ddpg">4.4.2</a>) use a a smoothed version of the current weights, as proposed in <span class="citation" data-cites="Lillicrap2015">Lillicrap et al. (<a href="References.html#ref-Lillicrap2015" role="doc-biblioref">2015</a>)</span>:</p>
<p><span class="math display">\[
    \theta&#39; = \tau \, \theta + (1-\tau) \, \theta&#39;
\]</span></p>
<p>If this rule is applied after each step with a very small rate <span class="math inline">\(\tau\)</span>, the target network will slowly track the learned network, but never be the same.</p>
<p>These two facts make DQN extremely slow to learn: millions of transitions are needed to obtain a satisfying policy. This is called the <strong>sample complexity</strong>, i.e. the number of transitions needed to obtain a satisfying performance. DQN finds very good policies, but at the cost of a very long training time.</p>
<p>DQN was initially applied to solve various Atari 2600 games. Video frames were used as observations and the set of possible discrete actions was limited (left/right/up/down, shoot, etc). The CNN used is depicted on Fig. <a href="#fig:dqn">17</a>. It has two convolutional layers, no max-pooling, 2 fully-connected layer and one output layer representing the Q-value of all possible actions in the games.</p>
<figure>
<img src="img/dqn.png" id="fig:dqn" alt="" /><figcaption>Figure 17: Architecture of the CNN used in the original DQN paper. Taken from <span class="citation" data-cites="Mnih2015">Mnih et al. (<a href="References.html#ref-Mnih2015" role="doc-biblioref">2015</a>)</span>.</figcaption>
</figure>
<p>The problem of partial observability is solved by concatenating the four last video frames into a single tensor used as input to the CNN. The convolutional layers become able through learning to extract the speed information from it. Some of the Atari games (Pinball, Breakout) were solved with a performance well above human level, especially when they are mostly reactive. Games necessitating more long-term planning (Montezuma’ Revenge) were still poorly learned, though.</p>
<p>Beside being able to learn using delayed and sparse rewards in highly dimensional input spaces, the true <em>tour de force</em> of DQN is that it was able to learn the 49 Atari games using the same architecture and hyperparameters, showing the generality of the approach.</p>
<h2 id="sec:double-dqn"><span class="header-section-number">3.3</span> Double DQN</h2>
<p>In DQN, the experience replay memory and the target network were decisive in allowing the CNN to learn the tasks through RL. Their drawback is that they drastically slow down learning and increase the sample complexity. Additionally, DQN has stability issues: the same network may not converge the same way in different runs. One first improvement on DQN was proposed by <span class="citation" data-cites="vanHasselt2015">van Hasselt et al. (<a href="References.html#ref-vanHasselt2015" role="doc-biblioref">2015</a>)</span> and called <strong>double DQN</strong>.</p>
<p>The idea is that the target value <span class="math inline">\(y = r(s, a, s&#39;) + \gamma \, \max_{a&#39;} Q_{\theta&#39;}(s&#39;, a&#39;)\)</span> is frequently over-estimating the true return because of the max operator. Especially at the beginning of learning when Q-values are far from being correct, if an action is over-estimated (<span class="math inline">\(Q_{\theta&#39;}(s&#39;, a)\)</span> is higher that its true value) and selected by the target network as the next greedy action, the learned Q-value <span class="math inline">\(Q_{\theta}(s, a)\)</span> will also become over-estimated, what will propagate to all previous actions on the long-term. <span class="citation" data-cites="vanHasselt2010">van Hasselt (<a href="References.html#ref-vanHasselt2010" role="doc-biblioref">2010</a>)</span> showed that this over-estimation is inevitable in regular Q-learning and proposed <strong>double learning</strong>.</p>
<p>The idea is to train independently two value networks: one will be used to find the greedy action (the action with the maximal Q-value), the other to estimate the Q-value itself. Even if the first network choose an over-estimated action as the greedy action, the other might provide a less over-estimated value for it, solving the problem.</p>
<p>Applying double learning to DQN is particularly straightforward: there are already two value networks, the trained network and the target network. Instead of using the target network to both select the greedy action in the next state and estimate its Q-value, here the trained network <span class="math inline">\(\theta\)</span> is used to select the greedy action <span class="math inline">\(a^* = \text{argmax}_{a&#39;} Q_\theta (s&#39;, a&#39;)\)</span> while the target network only estimates its Q-value. The target value becomes:</p>
<p><span class="math display">\[
    y = r(s, a, s&#39;) + \gamma \, Q_{\theta&#39;}(s&#39;, \text{argmax}_{a&#39;} Q_\theta (s&#39;, a&#39;))
\]</span></p>
<p>This induces only a small modification of the DQN algorithm and significantly improves its performance and stability:</p>
<hr />
<ul>
<li>Every <span class="math inline">\(T_\text{train}\)</span> steps:
<ul>
<li>Sample a minibatch <span class="math inline">\(\mathcal{D}_s\)</span> randomly from <span class="math inline">\(\mathcal{D}\)</span>.</li>
<li>For each transition <span class="math inline">\((s, a, r, s&#39;)\)</span> in the minibatch:
<ul>
<li>Select the greedy action in the next state <span class="math inline">\(a^* = \text{argmax}_{a&#39;} Q_\theta (s&#39;, a&#39;)\)</span> using the trained network.</li>
<li>Predict its Q-value <span class="math inline">\(Q_{\theta&#39;}(s&#39;, a^*)\)</span> using the target network.</li>
<li>Compute the target value <span class="math inline">\(y = r + \gamma \, Q_{\theta&#39;}(s&#39;, a*)\)</span>.</li>
</ul></li>
</ul></li>
</ul>
<hr />
<h2 id="sec:prioritized-experience-replay"><span class="header-section-number">3.4</span> Prioritized experience replay</h2>
<p>Another drawback of the original DQN is that the experience replay memory is sampled uniformly. Novel and interesting transitions are selected with the same probability as old well-predicted transitions, what slows down learning. The main idea of <strong>prioritized experience replay</strong> <span class="citation" data-cites="Schaul2015">(Schaul et al., <a href="References.html#ref-Schaul2015" role="doc-biblioref">2015</a>)</span> is to order the transitions in the experience replay memory in decreasing order of their TD error:</p>
<p><span class="math display">\[
    \delta = r(s, a, s&#39;) + \gamma \, Q_{\theta&#39;}(s&#39;, \text{argmax}_{a&#39;} Q_\theta (s&#39;, a&#39;)) - Q_\theta(s, a)
\]</span></p>
<p>and sample with a higher probability those surprising transitions to form a minibatch. However, non-surprising transitions might become relevant again after enough training, as the <span class="math inline">\(Q_\theta(s, a)\)</span> change, so prioritized replay has a softmax function over the TD error to ensure “exploration” of memorized transitions. This data structure has of course a non-negligible computational cost, but accelerates learning so much that it is worth it. See <a href="https://jaromiru.com/2016/11/07/lets-make-a-dqn-double-learning-and-prioritized-experience-replay/" class="uri">https://jaromiru.com/2016/11/07/lets-make-a-dqn-double-learning-and-prioritized-experience-replay/</a> for a presentation of double DQN with prioritized replay.</p>
<h2 id="sec:duelling-network"><span class="header-section-number">3.5</span> Duelling network</h2>
<p>The classical DQN architecture uses a single NN to predict directly the value of all possible actions <span class="math inline">\(Q_\theta(s, a)\)</span>. The value of an action depends on two factors:</p>
<ul>
<li>the value of the underlying state <span class="math inline">\(s\)</span>: in some states, all actions are bad, you lose whatever you do.</li>
<li>the interest of that action: some actions are better than others for a given state.</li>
</ul>
<p>This leads to the definition of the <strong>advantage</strong> <span class="math inline">\(A^\pi(s,a)\)</span> of an action:</p>
<p><span id="eq:advantagefunction"><span class="math display">\[
    A^\pi(s, a) = Q^\pi(s, a) - V^\pi(s)
\qquad(5)\]</span></span></p>
<p>The advantage of the optimal action in <span class="math inline">\(s\)</span> is equal to zero: the expected return in <span class="math inline">\(s\)</span> is the same as the expected return when being in <span class="math inline">\(s\)</span> and taking <span class="math inline">\(a\)</span>, as the optimal policy will choose <span class="math inline">\(a\)</span> in <span class="math inline">\(s\)</span> anyway. The advantage of all other actions is negative: they bring less reward than the optimal action (by definition), so they are less advantageous. Note that this is only true if your estimate of <span class="math inline">\(V^\pi(s)\)</span> is correct.</p>
<p><span class="citation" data-cites="Baird1993">Baird (<a href="References.html#ref-Baird1993" role="doc-biblioref">1993</a>)</span> has shown that it is advantageous to decompose the Q-value of an action into the value of the state and the advantage of the action (<em>advantage updating</em>):</p>
<p><span class="math display">\[
    Q^\pi(s, a) = V^\pi(s) + A^\pi(s, a)
\]</span></p>
<p>If you already know that the value of a state is very low, you do not need to bother exploring and learning the value of all actions in that state, they will not bring much. Moreover, the advantage function has <strong>less variance</strong> than the Q-values, which is a very good property when using neural networks for function approximation. The variance of the Q-values comes from the fact that they are estimated based on other estimates, which themselves evolve during learning (non-stationarity of the targets) and can drastically change during exploration (stochastic policies). The advantages only track the <em>relative</em> change of the value of an action compared to its state, what is going to be much more stable over time.</p>
<p>The range of values taken by the advantages is also much smaller than the Q-values. Let’s suppose we have two states with values -10 and 10, and two actions with advantages 0 and -1 (it does not matter which one). The Q-values will vary between -11 (the worst action in the worst state) and 10 (the best action in the best state), while the advantage only varies between -1 and 0. It is therefore going to be much easier for a neural network to learn the advantages than the Q-values, as they are theoretically not bounded.</p>
<figure>
<img src="img/duelling.png" id="fig:duelling" style="width:60.0%" alt="" /><figcaption>Figure 18: Duelling network architecture. Top: classical feedforward architecture to predict Q-values. Bottom: Duelling networks predicting state values and advantage functions to form the Q-values. Taken from <span class="citation" data-cites="Wang2016">Wang et al. (<a href="References.html#ref-Wang2016" role="doc-biblioref">2016</a>)</span>.</figcaption>
</figure>
<p><span class="citation" data-cites="Wang2016">Wang et al. (<a href="References.html#ref-Wang2016" role="doc-biblioref">2016</a>)</span> incorporated the idea of <em>advantage updating</em> in a double DQN architecture with prioritized replay (Fig. <a href="#fig:duelling">18</a>). As in DQN, the last layer represents the Q-values of the possible actions and has to minimize the mse loss:</p>
<p><span class="math display">\[
    \mathcal{L}(\theta) = \mathbb{E}_\pi([r(s, a, s&#39;) + \gamma \, Q_{\theta&#39;, \alpha&#39;, \beta&#39;}(s&#39;, \text{argmax}_{a&#39;} Q_{\theta, \alpha, \beta} (s&#39;, a&#39;)) - Q_{\theta, \alpha, \beta}(s, a)]^2)
\]</span></p>
<p>The difference is that the previous fully-connected layer is forced to represent the value of the input state <span class="math inline">\(V_{\theta, \beta}(s)\)</span> and the advantage of each action <span class="math inline">\(A_{\theta, \alpha}(s, a)\)</span> separately. There are two separate sets of weights in the network, <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>, to predict these two values, sharing representations from the early convolutional layers through weights <span class="math inline">\(\theta\)</span>. The output layer performs simply a parameter-less summation of both sub-networks:</p>
<p><span class="math display">\[
    Q_{\theta, \alpha, \beta}(s, a) = V_{\theta, \beta}(s) + A_{\theta, \alpha}(s, a)
\]</span></p>
<p>The issue with this formulation is that one could add a constant to <span class="math inline">\(V_{\theta, \beta}(s)\)</span> and substract it from <span class="math inline">\(A_{\theta, \alpha}(s, a)\)</span> while obtaining the same result. An easy way to constrain the summation is to normalize the advantages, so that the greedy action has an advantage of zero as expected:</p>
<p><span class="math display">\[
    Q_{\theta, \alpha, \beta}(s, a) = V_{\theta, \beta}(s) + (A_{\theta, \alpha}(s, a) - \max_a A_{\theta, \alpha}(s, a))
\]</span></p>
<p>By doing this, the advantages are still free, but the state value will have to take the correct value. <span class="citation" data-cites="Wang2016">Wang et al. (<a href="References.html#ref-Wang2016" role="doc-biblioref">2016</a>)</span> found that it is actually better to replace the <span class="math inline">\(\max\)</span> operator by the mean of the advantages. In this case, the advantages only need to change as fast as their mean, instead of having to compensate quickly for any change in the greedy action as the policy improves:</p>
<p><span class="math display">\[
    Q_{\theta, \alpha, \beta}(s, a) = V_{\theta, \beta}(s) + (A_{\theta, \alpha}(s, a) - \frac{1}{|\mathcal{A}|} \sum_a A_{\theta, \alpha}(s, a))
\]</span></p>
<p>Apart from this specific output layer, everything works as usual, especially the gradient of the mse loss function can travel backwards using backpropagation to update the weights <span class="math inline">\(\theta\)</span>, <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>. The resulting architecture outperforms double DQN with prioritized replay on most Atari games, particularly games with repetitive actions.</p>
<h2 id="sec:distributed-dqn-gorila"><span class="header-section-number">3.6</span> Distributed DQN (GORILA)</h2>
<p>The main limitation of deep RL is the slowness of learning, which is mainly influenced by two factors:</p>
<ul>
<li>the <em>sample complexity</em>, i.e. the number of transitions needed to learn a satisfying policy.</li>
<li>the online interaction with the environment.</li>
</ul>
<p>The second factor is particularly critical in real-world applications like robotics: physical robots evolve in real time, so the acquisition speed of transitions will be limited. Even in simulation (video games, robot emulators), the simulator might turn out to be much slower than training the underlying neural network. Google Deepmind proposed the GORILA (General Reinforcement Learning Architecture) framework to speed up the training of DQN networks using distributed actors and learners <span class="citation" data-cites="Nair2015">(Nair et al., <a href="References.html#ref-Nair2015" role="doc-biblioref">2015</a>)</span>. The framework is quite general and the distribution granularity can change depending on the task.</p>
<figure>
<img src="img/gorila-global.png" id="fig:gorila" style="width:90.0%" alt="" /><figcaption>Figure 19: GORILA architecture. Multiple actors interact with multiple copies of the environment and store their experiences in a (distributed) experience replay memory. Multiple DQN learners sample from the ERM and compute the gradient of the loss function w.r.t the parameters <span class="math inline">\(\theta\)</span>. A master network (parameter server, possibly distributed) gathers the gradients, apply weight updates and synchronizes regularly both the actors and the learners with new parameters. Taken from <span class="citation" data-cites="Nair2015">Nair et al. (<a href="References.html#ref-Nair2015" role="doc-biblioref">2015</a>)</span>.</figcaption>
</figure>
<p>In GORILA, multiple actors interact with the environment to gather transitions. Each actor has an independent copy of the environment, so they can gather <span class="math inline">\(N\)</span> times more samples per second if there are <span class="math inline">\(N\)</span> actors. This is possible in simulation (starting <span class="math inline">\(N\)</span> instances of the same game in parallel) but much more complicated for real-world systems (but see <span class="citation" data-cites="Gu2017">Gu et al. (<a href="References.html#ref-Gu2017" role="doc-biblioref">2017</a>)</span> for an example where multiple identical robots are used to gather experiences in parallel).</p>
<p>The experienced transitions are sent as in DQN to an experience replay memory, which may be distributed or centralized. Multiple DQN learners will then sample a minibatch from the ERM and compute the DQN loss on this minibatch (also using a target network). All learners start with the same parameters <span class="math inline">\(\theta\)</span> and simply compute the gradient of the loss function <span class="math inline">\(\frac{\partial \mathcal{L}(\theta)}{\partial \theta}\)</span> on the minibatch. The gradients are sent to a parameter server (a master network) which uses the gradients to apply the optimizer (e.g. SGD) and find new values for the parameters <span class="math inline">\(\theta\)</span>. Weight updates can also be applied in a distributed manner. This distributed method to train a network using multiple learners is now quite standard in deep learning: on multiple GPU systems, each GPU has a copy of the network and computes gradients on a different minibatch, while a master network integrates these gradients and updates the slaves.</p>
<p>The parameter server regularly updates the actors (to gather samples with the new policy) and the learners (to compute gradients w.r.t the new parameter values). Such a distributed system can greatly accelerate learning, but it can be quite tricky to find the optimum number of actors and learners (too many learners might degrade the stability) or their update rate (if the learners are not updated frequently enough, the gradients might not be correct). A similar idea is at the core of the A3C algorithm (Section <a href="./ActorCritic.html#sec:asynchronous-advantage-actor-critic-a3c">4.2.2</a>).</p>
<h2 id="sec:deep-recurrent-q-learning-drqn"><span class="header-section-number">3.7</span> Deep Recurrent Q-learning (DRQN)</h2>
<p>The Atari games used as a benchmark for value-based methods are <strong>partially observable MDPs</strong> (POMDP), i.e. a single frame does not contain enough information to predict what is going to happen next (e.g. the speed and direction of the ball on the screen is not known). In DQN, partial observability is solved by stacking four consecutive frames and using the resulting tensor as an input to the CNN. if this approach worked well for most Atari games, it has several limitations (as explained in <a href="https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-6-partial-observability-and-deep-recurrent-q-68463e9aeefc" class="uri">https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-6-partial-observability-and-deep-recurrent-q-68463e9aeefc</a>):</p>
<ol type="1">
<li>It increases the size of the experience replay memory, as four video frames have to be stored for each transition.</li>
<li>It solves only short-term dependencies (instantaneous speeds). If the partial observability has long-term dependencies (an object has been hidden a long time ago but now becomes useful), the input to the neural network will not have that information. This is the main explanation why the original DQN performed so poorly on games necessitating long-term planning like Montezuma’s revenge.</li>
</ol>
<p>Building on previous ideas from the Schmidhuber’s group <span class="citation" data-cites="Bakker2001 Wierstra2007">(Bakker, <a href="References.html#ref-Bakker2001" role="doc-biblioref">2001</a>; Wierstra et al., <a href="References.html#ref-Wierstra2007" role="doc-biblioref">2007</a>)</span>, <span class="citation" data-cites="Hausknecht2015">Hausknecht and Stone (<a href="References.html#ref-Hausknecht2015" role="doc-biblioref">2015</a>)</span> replaced one of the fully-connected layers of the DQN network by a LSTM layer (see Section <a href="./DeepLearning.html#sec:recurrent-neural-networks">2.2.3</a>) while using single frames as inputs. The resulting <strong>deep recurrent q-learning</strong> (DRQN) network became able to solve POMDPs thanks to the astonishing learning abilities of LSTMs: the LSTM layer learn to remember which part of the sensory information will be useful to take decisions later.</p>
<p>However, LSTMs are not a magical solution either. They are trained using <em>truncated BPTT</em>, i.e. on a limited history of states. Long-term dependencies exceeding the truncation horizon cannot be learned. Additionally, all states in that horizon (i.e. all frames) have to be stored in the ERM to train the network, increasing drastically its size. Despite these limitations, DRQN is a much more elegant solution to the partial observability problem, letting the network decide which horizon it needs to solve long-term dependencies.</p>
<h2 id="sec:other-variants-of-dqn"><span class="header-section-number">3.8</span> Other variants of DQN</h2>
<p>Double duelling DQN with prioritized replay was the state-of-the-art method for value-based deep RL (see <span class="citation" data-cites="Hessel2017">Hessel et al. (<a href="References.html#ref-Hessel2017" role="doc-biblioref">2017</a>)</span> for an experimental study of the contribution of each mechanism and the corresponding <strong>Rainbow</strong> DQN network, using in addition distributional learning, see Section <a href="./DistributionalRL.html#sec:categorical-dqn">4.7.1</a>). Several improvements have been proposed since the corresponding milestone papers. This section provides some short explanations and links to the original papers (to be organized and extended).</p>
<p><strong>Average-DQN</strong> proposes to increase the stability and performance of DQN by replacing the single target network (a copy of the trained network) by an average of the last parameter values, in other words an average of many past target networks <span class="citation" data-cites="Anschel2016">(Anschel et al., <a href="References.html#ref-Anschel2016" role="doc-biblioref">2016</a>)</span>.</p>
<p><span class="citation" data-cites="He2016">He et al. (<a href="References.html#ref-He2016" role="doc-biblioref">2016</a>)</span> proposed <strong>fast reward propagation</strong> through optimality tightening to speedup learning: when rewards are sparse, they require a lot of episodes to propagate these rare rewards to all actions leading to it. Their method combines immediate rewards (single steps) with actual returns (as in Monte-Carlo) via a constrained optimization approach.</p>

<br>
<div class="arrows">
<a href="DeepLearning.html" class="previous">&laquo; Previous</a>
<a href="PolicyGradient.html" class="next">Next &raquo;</a>
</div>

</article>

</body>
</html>
