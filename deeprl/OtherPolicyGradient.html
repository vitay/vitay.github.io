<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <meta name="author" content="Julien Vitay">
  <title>Deep Reinforcement Learning</title>


  <link rel="stylesheet" href="assets/syntax.css">
  <link rel="stylesheet" href="assets/github.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->

</head>

<body class="markdown-body">

<header>
  <h1 class="title"><strong>Deep Reinforcement Learning</strong></h1>
  <p class="author">Julien Vitay - <a href="mailto:julien.vitay@informatik.tu-chemnitz.de">julien.vitay@informatik.tu-chemnitz.de</a></p>
</header>


<article>

<nav id="TOC" class ="toc">
  <h2><strong>Deep Reinforcement Learning</strong></h2>
  <p class="author">Julien Vitay</p>

<ul>
<li><a href="./Introduction.html#sec:introduction"><span class="toc-section-number">1</span> Introduction</a></li>
<li><a href="./BasicRL.html#sec:basics"><span class="toc-section-number">2</span> Basics</a><ul>
<li><a href="./BasicRL.html#sec:reinforcement-learning-and-markov-decision-process"><span class="toc-section-number">2.1</span> Reinforcement learning and Markov Decision Process</a><ul>
<li><a href="./BasicRL.html#sec:policy-and-value-functions"><span class="toc-section-number">2.1.1</span> Policy and value functions</a></li>
<li><a href="./BasicRL.html#sec:bellman-equations"><span class="toc-section-number">2.1.2</span> Bellman equations</a></li>
<li><a href="./BasicRL.html#sec:dynamic-programming"><span class="toc-section-number">2.1.3</span> Dynamic programming</a></li>
<li><a href="./BasicRL.html#sec:monte-carlo-sampling"><span class="toc-section-number">2.1.4</span> Monte-Carlo sampling</a></li>
<li><a href="./BasicRL.html#sec:temporal-difference"><span class="toc-section-number">2.1.5</span> Temporal Difference</a></li>
<li><a href="./BasicRL.html#sec:eligibility-traces"><span class="toc-section-number">2.1.6</span> Eligibility traces</a></li>
<li><a href="./BasicRL.html#sec:actor-critic-architectures"><span class="toc-section-number">2.1.7</span> Actor-critic architectures</a></li>
<li><a href="./BasicRL.html#sec:function-approximation"><span class="toc-section-number">2.1.8</span> Function approximation</a></li>
</ul></li>
<li><a href="./DeepLearning.html#sec:deep-learning"><span class="toc-section-number">2.2</span> Deep learning</a><ul>
<li><a href="./DeepLearning.html#sec:deep-neural-networks"><span class="toc-section-number">2.2.1</span> Deep neural networks</a></li>
<li><a href="./DeepLearning.html#sec:convolutional-networks"><span class="toc-section-number">2.2.2</span> Convolutional networks</a></li>
<li><a href="./DeepLearning.html#sec:recurrent-neural-networks"><span class="toc-section-number">2.2.3</span> Recurrent neural networks</a></li>
</ul></li>
</ul></li>
<li><a href="./Valuebased.html#sec:value-based-methods"><span class="toc-section-number">3</span> Value-based methods</a><ul>
<li><a href="./Valuebased.html#sec:limitations-of-deep-neural-networks-for-function-approximation"><span class="toc-section-number">3.1</span> Limitations of deep neural networks for function approximation</a></li>
<li><a href="./Valuebased.html#sec:deep-q-network-dqn"><span class="toc-section-number">3.2</span> Deep Q-Network (DQN)</a></li>
<li><a href="./Valuebased.html#sec:double-dqn"><span class="toc-section-number">3.3</span> Double DQN</a></li>
<li><a href="./Valuebased.html#sec:prioritized-experience-replay"><span class="toc-section-number">3.4</span> Prioritized experience replay</a></li>
<li><a href="./Valuebased.html#sec:duelling-network"><span class="toc-section-number">3.5</span> Duelling network</a></li>
<li><a href="./Valuebased.html#sec:distributed-dqn-gorila"><span class="toc-section-number">3.6</span> Distributed DQN (GORILA)</a></li>
<li><a href="./Valuebased.html#sec:deep-recurrent-q-learning-drqn"><span class="toc-section-number">3.7</span> Deep Recurrent Q-learning (DRQN)</a></li>
<li><a href="./Valuebased.html#sec:other-variants-of-dqn"><span class="toc-section-number">3.8</span> Other variants of DQN</a></li>
</ul></li>
<li><a href="./PolicyGradient.html#sec:policy-gradient-methods"><span class="toc-section-number">4</span> Policy Gradient methods</a><ul>
<li><a href="./PolicyGradient.html#sec:reinforce"><span class="toc-section-number">4.1</span> REINFORCE</a><ul>
<li><a href="./PolicyGradient.html#sec:estimating-the-policy-gradient"><span class="toc-section-number">4.1.1</span> Estimating the policy gradient</a></li>
<li><a href="./PolicyGradient.html#sec:reducing-the-variance"><span class="toc-section-number">4.1.2</span> Reducing the variance</a></li>
<li><a href="./PolicyGradient.html#sec:policy-gradient-theorem"><span class="toc-section-number">4.1.3</span> Policy Gradient theorem</a></li>
</ul></li>
<li><a href="./ActorCritic.html#sec:advantage-actor-critic-methods"><span class="toc-section-number">4.2</span> Advantage Actor-Critic methods</a><ul>
<li><a href="./ActorCritic.html#sec:advantage-actor-critic-a2c"><span class="toc-section-number">4.2.1</span> Advantage Actor-Critic (A2C)</a></li>
<li><a href="./ActorCritic.html#sec:asynchronous-advantage-actor-critic-a3c"><span class="toc-section-number">4.2.2</span> Asynchronous Advantage Actor-Critic (A3C)</a></li>
<li><a href="./ActorCritic.html#sec:generalized-advantage-estimation-gae"><span class="toc-section-number">4.2.3</span> Generalized Advantage Estimation (GAE)</a></li>
<li><a href="./ActorCritic.html#sec:stochastic-actor-critic-for-continuous-action-spaces"><span class="toc-section-number">4.2.4</span> Stochastic actor-critic for continuous action spaces</a></li>
</ul></li>
<li><a href="./ImportanceSampling.html#sec:off-policy-actor-critic"><span class="toc-section-number">4.3</span> Off-policy Actor-Critic</a><ul>
<li><a href="./ImportanceSampling.html#sec:importance-sampling"><span class="toc-section-number">4.3.1</span> Importance sampling</a></li>
<li><a href="./ImportanceSampling.html#sec:linear-off-policy-actor-critic-off-pac"><span class="toc-section-number">4.3.2</span> Linear Off-Policy Actor-Critic (Off-PAC)</a></li>
<li><a href="./ImportanceSampling.html#sec:retrace"><span class="toc-section-number">4.3.3</span> Retrace</a></li>
<li><a href="./ImportanceSampling.html#sec:self-imitation-learning-sil"><span class="toc-section-number">4.3.4</span> Self-Imitation Learning (SIL)</a></li>
</ul></li>
<li><a href="./DPG.html#sec:deterministic-policy-gradient-dpg"><span class="toc-section-number">4.4</span> Deterministic Policy Gradient (DPG)</a><ul>
<li><a href="./DPG.html#sec:deterministic-policy-gradient-theorem"><span class="toc-section-number">4.4.1</span> Deterministic policy gradient theorem</a></li>
<li><a href="./DPG.html#sec:deep-deterministic-policy-gradient-ddpg"><span class="toc-section-number">4.4.2</span> Deep Deterministic Policy Gradient (DDPG)</a></li>
<li><a href="./DPG.html#sec:distributed-distributional-ddpg-d4pg"><span class="toc-section-number">4.4.3</span> Distributed Distributional DDPG (D4PG)</a></li>
</ul></li>
<li><a href="./NaturalGradient.html#sec:natural-gradients"><span class="toc-section-number">4.5</span> Natural Gradients</a><ul>
<li><a href="./NaturalGradient.html#sec:principle-of-natural-gradients"><span class="toc-section-number">4.5.1</span> Principle of natural gradients</a></li>
<li><a href="./NaturalGradient.html#sec:natural-policy-gradient-and-natural-actor-critic-nac"><span class="toc-section-number">4.5.2</span> Natural policy gradient and Natural Actor Critic (NAC)</a></li>
<li><a href="./NaturalGradient.html#sec:trust-region-policy-optimization-trpo"><span class="toc-section-number">4.5.3</span> Trust Region Policy Optimization (TRPO)</a></li>
<li><a href="./NaturalGradient.html#sec:proximal-policy-optimization-ppo"><span class="toc-section-number">4.5.4</span> Proximal Policy Optimization (PPO)</a></li>
<li><a href="./NaturalGradient.html#sec:actor-critic-with-experience-replay-acer"><span class="toc-section-number">4.5.5</span> Actor-Critic with Experience Replay (ACER)</a></li>
</ul></li>
<li><a href="./OtherPolicyGradient.html#sec:maximum-entropy-rl"><span class="toc-section-number">4.6</span> Maximum Entropy RL</a><ul>
<li><a href="#sec:entropy-regularization-1"><span class="toc-section-number">4.6.1</span> Entropy regularization</a></li>
<li><a href="./OtherPolicyGradient.html#sec:soft-q-learning"><span class="toc-section-number">4.6.2</span> Soft Q-learning</a></li>
<li><a href="./OtherPolicyGradient.html#sec:soft-actor-critic-sac"><span class="toc-section-number">4.6.3</span> Soft Actor-Critic (SAC)</a></li>
</ul></li>
<li><a href="./OtherPolicyGradient.html#sec:distributional-learning"><span class="toc-section-number">4.7</span> Distributional learning</a><ul>
<li><a href="./OtherPolicyGradient.html#sec:the-reactor"><span class="toc-section-number">4.7.1</span> The Reactor</a></li>
</ul></li>
<li><a href="./OtherPolicyGradient.html#sec:other-policy-search-methods"><span class="toc-section-number">4.8</span> Other policy search methods</a><ul>
<li><a href="./OtherPolicyGradient.html#sec:stochastic-value-gradient-svg"><span class="toc-section-number">4.8.1</span> Stochastic Value Gradient (SVG)</a></li>
<li><a href="./OtherPolicyGradient.html#sec:q-prop"><span class="toc-section-number">4.8.2</span> Q-Prop</a></li>
<li><a href="./OtherPolicyGradient.html#sec:normalized-advantage-function-naf"><span class="toc-section-number">4.8.3</span> Normalized Advantage Function (NAF)</a></li>
<li><a href="./OtherPolicyGradient.html#sec:fictitious-self-play-fsp"><span class="toc-section-number">4.8.4</span> Fictitious Self-Play (FSP)</a></li>
</ul></li>
<li><a href="./OtherPolicyGradient.html#sec:comparison-between-value-based-and-policy-gradient-methods"><span class="toc-section-number">4.9</span> Comparison between value-based and policy gradient methods</a></li>
<li><a href="./OtherPolicyGradient.html#sec:gradient-free-policy-search"><span class="toc-section-number">4.10</span> Gradient-free policy search</a><ul>
<li><a href="./OtherPolicyGradient.html#sec:cross-entropy-method-cem"><span class="toc-section-number">4.10.1</span> Cross-entropy Method (CEM)</a></li>
<li><a href="./OtherPolicyGradient.html#sec:evolutionary-search-es"><span class="toc-section-number">4.10.2</span> Evolutionary Search (ES)</a></li>
</ul></li>
</ul></li>
<li><a href="./RAM.html#sec:recurrent-attention-models"><span class="toc-section-number">5</span> Recurrent Attention Models</a></li>
<li><a href="./ModelBased.html#sec:model-based-rl"><span class="toc-section-number">6</span> Model-based RL</a><ul>
<li><a href="./ModelBased.html#sec:dyna-q"><span class="toc-section-number">6.1</span> Dyna-Q</a></li>
<li><a href="./ModelBased.html#sec:unsorted-references"><span class="toc-section-number">6.2</span> Unsorted references</a></li>
</ul></li>
<li><a href="./Hierarchical.html#sec:hierarchical-reinforcement-learning"><span class="toc-section-number">7</span> Hierarchical Reinforcement Learning</a></li>
<li><a href="./Inverse.html#sec:inverse-reinforcement-learning"><span class="toc-section-number">8</span> Inverse Reinforcement Learning</a></li>
<li><a href="./Robotics.html#sec:deep-rl-for-robotics"><span class="toc-section-number">9</span> Deep RL for robotics</a></li>
<li><a href="./Practice.html#sec:deep-rl-in-practice"><span class="toc-section-number">10</span> Deep RL in practice</a><ul>
<li><a href="./Practice.html#sec:limitations"><span class="toc-section-number">10.1</span> Limitations</a></li>
<li><a href="./Practice.html#sec:simulation-environments"><span class="toc-section-number">10.2</span> Simulation environments</a></li>
<li><a href="./Practice.html#sec:algorithm-implementations"><span class="toc-section-number">10.3</span> Algorithm implementations</a></li>
</ul></li>
<li><a href="./References.html#sec:references">References</a></li>
</ul>


</nav>



<h2 id="sec:maximum-entropy-rl"><span class="header-section-number">4.6</span> Maximum Entropy RL</h2>
<p>All the methods seen sofar focus on finding a policy (or value functions) that maximizes the obtained return. This corresponds to the <strong>exploitation</strong> part of RL: we care only about the optimal policy. The <strong>exploration</strong> is ensured by external mechanisms, such as <span class="math inline">\(\epsilon\)</span>-greedy or softmax policies in value based methods, or adding exploratory noise to the actions as in DDPG. Exploration mechanisms typically add yet another free parameter (<span class="math inline">\(\epsilon\)</span>, softmax temperature, etc) that additionally need to be scheduled (more exploration at the beginning of learning than at the end).</p>
<p>The idea behind <strong>maximum entropy RL</strong> is to let the algorithm learn by itself how much exploration it needs to learn appropriately. There are several approaches to this problem (see for example <span class="citation" data-cites="Machado2018">(Machado et al., <a href="References.html#ref-Machado2018" role="doc-biblioref">2018</a>)</span> for an approach using successor representations), we focus first on methods using <strong>entropy regularization</strong>, a concept already seen briefly in A3C (Section <a href="./ActorCritic.html#sec:asynchronous-advantage-actor-critic-a3c">4.2.2</a>), before looking at soft methods such as Soft Q-learning and SAC.</p>
<h3 id="sec:entropy-regularization-1"><span class="header-section-number">4.6.1</span> Entropy regularization</h3>
<p><strong>Entropy regularization</strong> <span class="citation" data-cites="Williams1991">(Williams and Peng, <a href="References.html#ref-Williams1991" role="doc-biblioref">1991</a>)</span> adds a regularization term to the objective function:</p>
<p><span id="eq:entropy_reg"><span class="math display">\[
    J(\theta) =  \mathbb{E}_{s_t \sim \rho^\pi, a_t \sim \pi_\theta}[ R(s_t, a_t) + \beta \,  H(\pi_\theta(s_t))]
\qquad(15)\]</span></span></p>
<p>We will neglect here how the objective function is sampled (policy gradient, etc.) and focus on the second part.</p>
<p>The entropy of a discrete policy <span class="math inline">\(\pi_\theta\)</span> in a state <span class="math inline">\(s_t\)</span> is given by:</p>
<p><span class="math display">\[
    H(\pi_\theta(s_t)) = - \sum_a \pi_\theta(s_t, a) \, \log \pi_\theta(s_t, a)
\]</span></p>
<p>For continuous actions, one can replace the sum with an integral. The entropy of the policy measures its “randomness”:</p>
<ul>
<li>if the policy is fully deterministic (the same action is systematically selected), the entropy is zero as it carries no information.</li>
<li>if the policy is completely random (all actions are equally surprising), the entropy is maximal.</li>
</ul>
<p>By adding the entropy as a regularization term directly to the objective function, we force the policy to be as non-deterministic as possible, i.e. to explore as much as possible, while still getting as many rewards as possible. The parameter <span class="math inline">\(\beta\)</span> controls the level of regularization: we do not want the entropy to dominate either, as a purely random policy does not bring much reward. If <span class="math inline">\(\beta\)</span> is chosen too low, the entropy won’t play a significant role in the optimization and we may obtain a suboptimal deterministic policy early during training as there was not enough exploration. If <span class="math inline">\(\beta\)</span> is too high, the policy will be random and suboptimal.</p>
<p>Besides exploration, why would we want to learn a stochastic policy, while the solution to the Bellman equations is deterministic by definition? A first answer is that we rarely have a MDP: most interesting problems are POMDP, where the states are indirectly inferred through observations, which can be probabilistic. <span class="citation" data-cites="Todorov2008">Todorov (<a href="References.html#ref-Todorov2008" role="doc-biblioref">2008</a>)</span> showed that a stochastic policy emerges as the optimal answer when we consider the connection between optimal control and probabilistic inference <span class="citation" data-cites="Toussaint2009">(see also Toussaint, <a href="References.html#ref-Toussaint2009" role="doc-biblioref">2009</a>)</span>.</p>
<p>Consider a two-opponents game like chess: if you have a deterministic policy, you will always play the same moves in the same configuration. In particular, you will always play the same opening moves. Your game strategy becomes predictable for your opponent, who can adapt accordingly. Having a variety of opening moves (as long as they are not too stupid) is obviously a better strategy on the long term. This is due to the fact that chess is actually a POMDP: the opponent’s strategy and beliefs are not accessible.</p>
<p>Another way to view the interest of entropy regularization is to realize that learning a deterministic policy only leads to a single optimal solution to the problem. Learning a stochastic policy forces the agent to learn <strong>many</strong> optimal solutions to the same problem: the agent is somehow forced to learn as much information as possible for the experienced transitions, potentially reducing the sample complexity.</p>
<p>Entropy regularization is nowadays used very commonly used in deep RL networks <span class="citation" data-cites="ODonoghue2016">(e.g. O’Donoghue et al., <a href="References.html#ref-ODonoghue2016" role="doc-biblioref">2016</a>)</span>, as it is “only” an additional term to set in the objective function passed to the NN, adding a single hyperparameter <span class="math inline">\(\beta\)</span>.</p>
<h3 id="sec:soft-q-learning"><span class="header-section-number">4.6.2</span> Soft Q-learning</h3>
<p>Entropy regularization greedily maximizes the entropy of the policy in each state (the objective is the return plus the entropy in the current state). Building on the maximum entropy RL framework <span class="citation" data-cites="Ziebart2008 Schulman2017a Nachum2017">(Nachum et al., <a href="References.html#ref-Nachum2017" role="doc-biblioref">2017</a>; Schulman et al., <a href="References.html#ref-Schulman2017a" role="doc-biblioref">2017</a><a href="References.html#ref-Schulman2017a" role="doc-biblioref">a</a>; Ziebart et al., <a href="References.html#ref-Ziebart2008" role="doc-biblioref">2008</a>)</span>, <span class="citation" data-cites="Haarnoja2017">Haarnoja et al. (<a href="References.html#ref-Haarnoja2017" role="doc-biblioref">2017</a>)</span> proposed a version of <strong>soft-Q-learning</strong> by extending the definition of the objective:</p>
<p><span id="eq:softQ"><span class="math display">\[
    J(\theta) =  \sum_t \mathbb{E}_{s_t \sim \rho^\pi, a_t \sim \pi_\theta}[ r(s_t, a_t) + \beta \,  H(\pi_\theta(s_t))]
\qquad(16)\]</span></span></p>
<p>In this formulation based on trajectories, the agent seeks a policy that maximizes the entropy of the complete trajectories rather than the entropy of the policy in each state. This is a very important distinction: the agent does not only search a policy with a high entropy, but a policy that brings into states with a high entropy, i.e. where the agent is the most uncertain. This allows for very efficient exploration strategies, where the agent will try to reduce its uncertainty about the world and gather a lot more information than when simply searching for a good policy.</p>
<p>Note that it is always possible to fall back to classical Q-learning by setting <span class="math inline">\(\beta=0\)</span> and that it is possible to omit this hyperparameter by scaling the rewards with <span class="math inline">\(\frac{1}{\beta}\)</span>. The discount rate <span class="math inline">\(\gamma\)</span> is omitted here for simplicity, but it should be added back when the task has an infinite horizon.</p>
<p>In soft Q-learning, the policy can be defined by a softmax over the soft Q-values <span class="math inline">\(Q_\text{soft}(s, a)\)</span>, where <span class="math inline">\(\beta\)</span> plays the role of the temperature parameter:</p>
<p><span class="math display">\[
    \pi(s, a) \propto \exp(Q_\text{soft}(s_t, a_t) / \beta)
\]</span></p>
<p>Note that <span class="math inline">\(-Q_\text{soft}(s_t, a_t) / \beta\)</span> plays the role of the energy of the policy (as in Boltzmann machines), hence the name of the paper (<em>Reinforcement Learning with Deep Energy-Based Policies</em>). We will ignore this analogy here. The normalization term of the softmax (the log-partition function in energy-based models) is also omitted as it later disappears from the equations anyway.</p>
<p>The soft Q-values are defined by the following Bellman equation:</p>
<p><span id="eq:softQ_update"><span class="math display">\[
    Q_\text{soft}(s_t, a_t) = r(s_t, a_t) + \gamma \, \mathbb{E}_{s_{t+1} \in \rho} [V_\text{soft}(s_{t+1})]
\qquad(17)\]</span></span></p>
<p>This is the regular Bellman equation that can be turned into an update rule for the soft Q-values (minimizing the mse between the l.h.s and the r.h.s). The soft value of a state is given by:</p>
<p><span id="eq:softV_update"><span class="math display">\[
    V_\text{soft}(s_t) = \mathbb{E}_{a_{t} \in \pi} [Q_\text{soft}(s_{t}, a_{t}) - \log \, \pi(s_t, a_t)]
\qquad(18)\]</span></span></p>
<p>The notation in <span class="citation" data-cites="Haarnoja2017">Haarnoja et al. (<a href="References.html#ref-Haarnoja2017" role="doc-biblioref">2017</a>)</span> is much more complex than that (the paper includes the theoretical proofs), but it boils down to this in <span class="citation" data-cites="Haarnoja2018a">Haarnoja et al. (<a href="References.html#ref-Haarnoja2018a" role="doc-biblioref">2018</a><a href="References.html#ref-Haarnoja2018a" role="doc-biblioref">b</a>)</span>. When Eq. <a href="#eq:softQ_update">17</a> is applied repeatedly with the definition of Eq. <a href="#eq:softV_update">18</a>, it converges to the optimal solution of Eq. <a href="#eq:softQ">16</a>, at least in the tabular case.</p>
<p>The soft V-value of a state is the expectation of the Q-values in that state (as in regular RL) minus the log probability of each action. This last term measures the entropy of the policy in each state (when expanding the expectation over the policy, we obtain <span class="math inline">\(- \pi \log \pi\)</span>, which is the entropy).</p>
<p>In a nutshell, the soft Q-learning algorithm is:</p>
<ul>
<li>Sample transitions <span class="math inline">\((s, a, r, s&#39;)\)</span> and store them in a replay memory.</li>
<li>For each transition <span class="math inline">\((s, a, r, s&#39;)\)</span> in a minibatch of the replay memory:
<ul>
<li>Estimate <span class="math inline">\(V_\text{soft}(s&#39;)\)</span> with Eq. <a href="#eq:softV_update">18</a> by sampling several actions.</li>
<li>Update the soft Q-value of <span class="math inline">\((s,a)\)</span> with Eq. <a href="#eq:softQ_update">17</a>.</li>
<li>Update the policy (if not using the softmax over soft Q-values directly).</li>
</ul></li>
</ul>
<p>The main drawback of this approach is that several actions have to be sampled in the next state in order to estimate its current soft V-value, what makes it hard to implement in practice. The policy also has to be sampled from the Q-values, what is not practical for continuous action spaces.</p>
<p>But the real interesting thing is the policies that are learned in multi-goal settings, as in Fig. <a href="#fig:softql">33</a>. The agent starts in the middle of the environment and can obtain one of the four rewards (north, south, west, east). A regular RL agent would very quickly select only one of the rewards and stick to it. With soft Q-learning, the policy stays stochastic and the four rewards can be obtained even after convergence. This indicates that the soft agent has learned much more about its environment than its hard equivalent, thanks to its maximum entropy formulation.</p>
<figure>
<img src="img/softQL.png" alt="Figure 33: Policy learned by Soft Q-learning in a multi-goal setting. Taken from Haarnoja et al. (2017)." id="fig:softql" /><figcaption>Figure 33: Policy learned by Soft Q-learning in a multi-goal setting. Taken from <span class="citation" data-cites="Haarnoja2017">Haarnoja et al. (<a href="References.html#ref-Haarnoja2017" role="doc-biblioref">2017</a>)</span>.</figcaption>
</figure>
<h3 id="sec:soft-actor-critic-sac"><span class="header-section-number">4.6.3</span> Soft Actor-Critic (SAC)</h3>
<p><span class="citation" data-cites="Haarnoja2018a">Haarnoja et al. (<a href="References.html#ref-Haarnoja2018a" role="doc-biblioref">2018</a><a href="References.html#ref-Haarnoja2018a" role="doc-biblioref">b</a>)</span> proposed the <strong>Soft Actor-Critic</strong> (SAC), an off-policy actor-critic which allows to have a stochastic actor (contrary to DDPG) while being more optimal and sample efficient than on-policy methods such as A3C or PPO. It is also less sensible to hyperparameters than all these methods.</p>
<p>SAC builds on soft Q-learning to achieve these improvements. It relies on three different function approximators:</p>
<ul>
<li>a soft state value function <span class="math inline">\(V_\varphi(s)\)</span>.</li>
<li>a soft Q-value function <span class="math inline">\(Q_\psi(s,a)\)</span>.</li>
<li>a stochastic policy <span class="math inline">\(\pi_\theta(s, a)\)</span>.</li>
</ul>
<p>The paper uses a different notation for the parameters <span class="math inline">\(\theta, \varphi, \psi\)</span>, but I choose to be consistent with the rest of this document.</p>
<p>The soft state-value function <span class="math inline">\(V_\varphi(s)\)</span> is learned using Eq. <a href="#eq:softV_update">18</a> which is turned into a loss function:</p>
<p><span class="math display">\[
    \mathcal{L}(\varphi) = \mathbb{E}_{s_t \in \mathcal{D}} [\mathbb{E}_{a_{t} \in \pi} [(Q_\psi(s_{t}, a_{t}) - \log \, \pi_\theta(s_t, a_t)] - V_\varphi(s_t) )^2]
\]</span></p>
<p>In practice, we only need the gradient of this loss function to train the corresponding neural network. The expectation over the policy inside the loss function can be replaced by a single sample action <span class="math inline">\(a\)</span> using the current policy <span class="math inline">\(\pi_\theta\)</span> (but not <span class="math inline">\(a_{t+1}\)</span> in the replay memory <span class="math inline">\(\mathcal{D}\)</span>, which is only used for the states <span class="math inline">\(s_t\)</span>).</p>
<p><span class="math display">\[
    \nabla_\varphi \mathcal{L}(\varphi) = \nabla_\varphi V_\varphi(s_t) \, (V_\varphi(s_t) - Q_\psi(s_{t}, a) + \log \, \pi_\theta(s_t, a) )
\]</span></p>
<p>The soft Q-values <span class="math inline">\(Q_\psi(s_{t}, a_{t})\)</span> can be trained from the replay memory <span class="math inline">\(\mathcal{D}\)</span> on <span class="math inline">\((s_t, a_t, r_{t+1} , s_{t+1})\)</span> transitions by minimizing the mse:</p>
<p><span class="math display">\[
    \mathcal{L}(\psi) = \mathbb{E}_{s_t, a_t \in \mathcal{D}} [(r_{t+1} + \gamma \, V_\varphi(s_{t+1}) - Q_\psi(s_t, a_t))^2]
\]</span></p>
<p>Finally, the policy <span class="math inline">\(\pi_\theta\)</span> can be trained to maximize the obtained returns. There are many ways to do that, but <span class="citation" data-cites="Haarnoja2018a">Haarnoja et al. (<a href="References.html#ref-Haarnoja2018a" role="doc-biblioref">2018</a><a href="References.html#ref-Haarnoja2018a" role="doc-biblioref">b</a>)</span> proposes to minimize the Kullback-Leibler (KL) divergence (see Section <a href="./NaturalGradient.html#sec:principle-of-natural-gradients">4.5.1</a>) between the current policy <span class="math inline">\(\pi_\theta\)</span> and a softmax function over the soft Q-values:</p>
<p><span class="math display">\[
    \mathcal{L}(\theta) = \mathbb{E}_{s_t \in \mathcal{D}} [D_\text{KL}(\pi_\theta(s, \cdot) | \frac{\exp Q_\psi(s_t, \cdot)}{Z(s_t)})]
\]</span></p>
<p>where <span class="math inline">\(Z\)</span> is the partition function to normalize the softmax. Fortunately, it disappears when using the reparameterization trick and taking the gradient of this loss (see the paper for details).</p>
<p>There are additional tricks to make it more efficient and robust, such as target networks or the use of two independent function approximators for the soft Q-values in order to reduce the bias, but the gist of the algorithm is the following:</p>
<hr />
<ul>
<li>Sample a transition <span class="math inline">\((s_t, a_t, r_{t+1}, a_{t+1})\)</span> using the current policy <span class="math inline">\(\pi_\theta\)</span> and store it in the replay memory <span class="math inline">\(\mathcal{D}\)</span>.</li>
<li>For each transition <span class="math inline">\((s_t, a_t, r_{t+1}, a_{t+1})\)</span> of a minibatch of <span class="math inline">\(\mathcal{D}\)</span>:
<ul>
<li>Sample an action <span class="math inline">\(a \in \pi_\theta(s_t, \cdot)\)</span> from the current policy.</li>
<li>Update the soft state-value function <span class="math inline">\(V_\varphi(s_t)\)</span>: <span class="math display">\[
  \nabla_\varphi \mathcal{L}(\varphi) = \nabla_\varphi V_\varphi(s_t) \, (V_\varphi(s_t) - Q_\psi(s_{t}, a) + \log \, \pi_\theta(s_t, a) )
  \]</span></li>
<li>Update the soft Q-value function <span class="math inline">\(Q_\psi(s_t, a_t)\)</span>: <span class="math display">\[
  \nabla_\psi \mathcal{L}(\psi) = - \nabla_\psi Q_\psi(s_t, a_t) \, (r_{t+1} + \gamma \, V_\varphi(s_{t+1}) - Q_\psi(s_t, a_t))
  \]</span></li>
<li>Update the policy <span class="math inline">\(\pi_\theta(s_t, \cdot)\)</span>: <span class="math display">\[
  \nabla_\theta \mathcal{L}(\theta) = \nabla_\theta D_\text{KL}(\pi_\theta(s, \cdot) | \frac{\exp Q_\psi(s_t, \cdot)}{Z(s_t)})
  \]</span></li>
</ul></li>
</ul>
<hr />
<p>SAC was compared to DDPG, PPO, soft Q-learning and others on a set of gym and humanoid robotics tasks (with 21 joints!). It outperforms all these methods in both the final performance and the sample complexity, the difference being even more obvious for the complex tasks. The exploration bonus given by the maximum entropy allows the agent to discover better policies than its counterparts. SAC is an actor-critic architecture (the critic computing both V and Q) working off-policy (using an experience replay memory, so re-using past experiences) allowing to learn stochastic policies, even in high dimensional spaces.</p>
<h2 id="sec:distributional-learning"><span class="header-section-number">4.7</span> Distributional learning</h2>
<p><strong>Work in progress</strong></p>
<p>All RL methods based on the Bellman equations use the expectation operator to average returns and compute the values of states and actions:</p>
<p><span class="math display">\[
    Q^\pi(s, a) ) = \mathbb{E}_{\pi}[R(s, a)]
\]</span></p>
<p><span class="citation" data-cites="Bellemare2017">Bellemare et al. (<a href="References.html#ref-Bellemare2017" role="doc-biblioref">2017</a>)</span> propose to learn instead the <strong>value distribution</strong> through a modification of the Bellman equation. They show that learning the distribution of rewards rather than their mean leads to performance improvements.</p>
<p>See <a href="https://deepmind.com/blog/going-beyond-average-reinforcement-learning/" class="uri">https://deepmind.com/blog/going-beyond-average-reinforcement-learning/</a> for more explanations.</p>
<h3 id="sec:the-reactor"><span class="header-section-number">4.7.1</span> The Reactor</h3>
<p><span class="citation" data-cites="Gruslys2017">Gruslys et al. (<a href="References.html#ref-Gruslys2017" role="doc-biblioref">2017</a>)</span></p>
<h2 id="sec:other-policy-search-methods"><span class="header-section-number">4.8</span> Other policy search methods</h2>
<h3 id="sec:stochastic-value-gradient-svg"><span class="header-section-number">4.8.1</span> Stochastic Value Gradient (SVG)</h3>
<p><span class="citation" data-cites="Heess2015">Heess et al. (<a href="References.html#ref-Heess2015" role="doc-biblioref">2015</a>)</span></p>
<h3 id="sec:q-prop"><span class="header-section-number">4.8.2</span> Q-Prop</h3>
<p><span class="citation" data-cites="Gu2016">Gu et al. (<a href="References.html#ref-Gu2016" role="doc-biblioref">2016</a><a href="References.html#ref-Gu2016" role="doc-biblioref">a</a>)</span></p>
<h3 id="sec:normalized-advantage-function-naf"><span class="header-section-number">4.8.3</span> Normalized Advantage Function (NAF)</h3>
<p><span class="citation" data-cites="Gu2016a">Gu et al. (<a href="References.html#ref-Gu2016a" role="doc-biblioref">2016</a><a href="References.html#ref-Gu2016a" role="doc-biblioref">b</a>)</span></p>
<h3 id="sec:fictitious-self-play-fsp"><span class="header-section-number">4.8.4</span> Fictitious Self-Play (FSP)</h3>
<p><span class="citation" data-cites="Heinrich2015">Heinrich et al. (<a href="References.html#ref-Heinrich2015" role="doc-biblioref">2015</a>)</span> <span class="citation" data-cites="Heinrich2016">Heinrich and Silver (<a href="References.html#ref-Heinrich2016" role="doc-biblioref">2016</a>)</span></p>
<h2 id="sec:comparison-between-value-based-and-policy-gradient-methods"><span class="header-section-number">4.9</span> Comparison between value-based and policy gradient methods</h2>
<p>Having now reviewed both value-based methods (DQN and its variants) and policy gradient methods (A3C, DDPG, PPO), the question is which method to choose? While not much happens right now for value-based methods, policy gradient methods are attracting a lot of attention, as they are able to learn policies in continuous action spaces, what is very important in robotics. <a href="https://flyyufelix.github.io/2017/10/12/dqn-vs-pg.html" class="uri">https://flyyufelix.github.io/2017/10/12/dqn-vs-pg.html</a> summarizes the advantages and inconvenients of policy gradient methods.</p>
<p>Advantages of PG:</p>
<ul>
<li>Better convergence properties, more stable <span class="citation" data-cites="Duan2016">(Duan et al., <a href="References.html#ref-Duan2016" role="doc-biblioref">2016</a>)</span>.</li>
<li>Effective in high-dimensional or continuous action spaces.</li>
<li>Can learn stochastic policies.</li>
</ul>
<p>Disadvantages of PG:</p>
<ul>
<li>Typically converge to a local rather than global optimum.</li>
<li>Evaluating a policy is often inefficient and having a high variance.</li>
<li>Worse sample efficiency (but it is getting better).</li>
</ul>
<h2 id="sec:gradient-free-policy-search"><span class="header-section-number">4.10</span> Gradient-free policy search</h2>
<p>The policy gradient methods presented above rely on backpropagation and gradient descent/ascent to update the parameters of the policy and maximize the objective function. Gradient descent is generally slow, sample inefficient and subject to local minima, but is nevertheless the go-to method in neural networks. However, it is not the only optimization that can be used in deep RL. This section presents some of the alternatives.</p>
<h3 id="sec:cross-entropy-method-cem"><span class="header-section-number">4.10.1</span> Cross-entropy Method (CEM)</h3>
<p><span class="citation" data-cites="Szita2006">Szita and Lörincz (<a href="References.html#ref-Szita2006" role="doc-biblioref">2006</a>)</span></p>
<h3 id="sec:evolutionary-search-es"><span class="header-section-number">4.10.2</span> Evolutionary Search (ES)</h3>
<p><span class="citation" data-cites="Salimans2017">Salimans et al. (<a href="References.html#ref-Salimans2017" role="doc-biblioref">2017</a>)</span></p>
<p>Explanations from OpenAI: <a href="https://blog.openai.com/evolution-strategies/" class="uri">https://blog.openai.com/evolution-strategies/</a></p>
<p>Deep neuroevolution at Uber: <a href="https://eng.uber.com/deep-neuroevolution/" class="uri">https://eng.uber.com/deep-neuroevolution/</a></p>

<br>
<div class="arrows">
<a href="NaturalGradient.html" class="previous">&laquo; Previous</a>
<a href="RAM.html" class="next">Next &raquo;</a>
</div>


</article>
</body>
</html>
