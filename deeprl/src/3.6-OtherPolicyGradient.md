
## Maximum Entropy RL

**Work in progress**

### Entropy regularization

@Todorov2008 a stochastic policy emerges as the optimal answer when we consider the connection between optimal control and probabilistic inference

@Toussaint2009 Maximum entropy RL

@ODonoghue2016 Combining policy gradient and Q-learning

@Haarnoja2017 Reinforcement Learning with Deep Energy-Based Policies

### Soft Actor-Critic (SAC)


* Main limitations of on-policy algorithms: high sample complexity and poor convergence properties, leading to difficult tuining of hyperparameters.

* Off-policy reduces the sample complexity, but increases the variance = instability.

@Haarnoja2018a

## Distributional learning

All RL methods based on the Bellman equations use the expectation operator to average returns and compute the values of states and actions:

$$
    Q^\pi(s, a) ) = \mathbb{E}_{\pi}[R(s, a)]
$$

@Bellemare2017 propose to learn instead the **value distribution** through a modification of the Bellman equation. They show that learning the distribution of rewards rather than their mean leads to performance improvements.

See <https://deepmind.com/blog/going-beyond-average-reinforcement-learning/> for more explanations.


### The Reactor

@Gruslys2017


## Other policy search methods

### Stochastic Value Gradient (SVG)

@Heess2015


### Q-Prop

@Gu2016

### Normalized Advantage Function (NAF)

@Gu2016a


### Fictitious Self-Play (FSP)

@Heinrich2015 @Heinrich2016


## Comparison between value-based and policy gradient methods

Having now reviewed both value-based methods (DQN and its variants) and policy gradient methods (A3C, DDPG, PPO), the question is which method to choose? While not much happens right now for value-based methods, policy gradient methods are attracting a lot of attention, as they are able to learn policies in continuous action spaces, what is very important in robotics.  <https://flyyufelix.github.io/2017/10/12/dqn-vs-pg.html> summarizes the advantages and inconvenients of policy gradient methods.

Advantages of PG:

* Better convergence properties, more stable [@Duan2016].
* Effective in high-dimensional or continuous action spaces.
* Can learn stochastic policies.

Disadvantages of PG:

* Typically converge to a local rather than global optimum.
* Evaluating a policy is often inefficient and having a high variance.
* Worse sample efficiency (but it is getting better).

## Gradient-free policy search

The policy gradient methods presented above rely on backpropagation and gradient descent/ascent to update the parameters of the policy and maximize the objective function. Gradient descent is generally slow, sample inefficient and subject to local minima, but is nevertheless the go-to method in neural networks. However, it is not the only optimization that can be used in deep RL. This section presents some of the alternatives.

### Cross-entropy Method (CEM)

@Szita2006

### Evolutionary Search (ES)

@Salimans2017

Explanations from OpenAI: <https://blog.openai.com/evolution-strategies/>

Deep neuroevolution at Uber: <https://eng.uber.com/deep-neuroevolution/>
