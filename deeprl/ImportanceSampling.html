<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <meta name="author" content="Julien Vitay">
  <title>Deep Reinforcement Learning</title>


  <link rel="stylesheet" href="assets/syntax.css">
  <link rel="stylesheet" href="assets/github.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->

</head>

<body class="markdown-body">

<header>
  <h1 class="title"><strong>Deep Reinforcement Learning</strong></h1>
  <p class="author">Julien Vitay - <a href="mailto:julien.vitay@informatik.tu-chemnitz.de">julien.vitay@informatik.tu-chemnitz.de</a></p>
</header>


<article>

<nav id="TOC" class ="toc">
  <h2><strong>Deep Reinforcement Learning</strong></h2>
  <p class="author">Julien Vitay</p>

<ul>
<li><a href="./Introduction.html#sec:introduction"><span class="toc-section-number">1</span> Introduction</a></li>
<li><a href="./BasicRL.html#sec:basics"><span class="toc-section-number">2</span> Basics</a><ul>
<li><a href="./BasicRL.html#sec:reinforcement-learning-and-markov-decision-process"><span class="toc-section-number">2.1</span> Reinforcement learning and Markov Decision Process</a><ul>
<li><a href="./BasicRL.html#sec:policy-and-value-functions"><span class="toc-section-number">2.1.1</span> Policy and value functions</a></li>
<li><a href="./BasicRL.html#sec:bellman-equations"><span class="toc-section-number">2.1.2</span> Bellman equations</a></li>
<li><a href="./BasicRL.html#sec:dynamic-programming"><span class="toc-section-number">2.1.3</span> Dynamic programming</a></li>
<li><a href="./BasicRL.html#sec:monte-carlo-sampling"><span class="toc-section-number">2.1.4</span> Monte-Carlo sampling</a></li>
<li><a href="./BasicRL.html#sec:temporal-difference"><span class="toc-section-number">2.1.5</span> Temporal Difference</a></li>
<li><a href="./BasicRL.html#sec:eligibility-traces"><span class="toc-section-number">2.1.6</span> Eligibility traces</a></li>
<li><a href="./BasicRL.html#sec:actor-critic-architectures"><span class="toc-section-number">2.1.7</span> Actor-critic architectures</a></li>
<li><a href="./BasicRL.html#sec:function-approximation"><span class="toc-section-number">2.1.8</span> Function approximation</a></li>
</ul></li>
<li><a href="./DeepLearning.html#sec:deep-learning"><span class="toc-section-number">2.2</span> Deep learning</a><ul>
<li><a href="./DeepLearning.html#sec:deep-neural-networks"><span class="toc-section-number">2.2.1</span> Deep neural networks</a></li>
<li><a href="./DeepLearning.html#sec:convolutional-networks"><span class="toc-section-number">2.2.2</span> Convolutional networks</a></li>
<li><a href="./DeepLearning.html#sec:recurrent-neural-networks"><span class="toc-section-number">2.2.3</span> Recurrent neural networks</a></li>
</ul></li>
</ul></li>
<li><a href="./Valuebased.html#sec:value-based-methods"><span class="toc-section-number">3</span> Value-based methods</a><ul>
<li><a href="./Valuebased.html#sec:limitations-of-deep-neural-networks-for-function-approximation"><span class="toc-section-number">3.1</span> Limitations of deep neural networks for function approximation</a></li>
<li><a href="./Valuebased.html#sec:deep-q-network-dqn"><span class="toc-section-number">3.2</span> Deep Q-Network (DQN)</a></li>
<li><a href="./Valuebased.html#sec:double-dqn"><span class="toc-section-number">3.3</span> Double DQN</a></li>
<li><a href="./Valuebased.html#sec:prioritized-experience-replay"><span class="toc-section-number">3.4</span> Prioritized experience replay</a></li>
<li><a href="./Valuebased.html#sec:duelling-network"><span class="toc-section-number">3.5</span> Duelling network</a></li>
<li><a href="./Valuebased.html#sec:distributed-dqn-gorila"><span class="toc-section-number">3.6</span> Distributed DQN (GORILA)</a></li>
<li><a href="./Valuebased.html#sec:deep-recurrent-q-learning-drqn"><span class="toc-section-number">3.7</span> Deep Recurrent Q-learning (DRQN)</a></li>
<li><a href="./Valuebased.html#sec:other-variants-of-dqn"><span class="toc-section-number">3.8</span> Other variants of DQN</a></li>
</ul></li>
<li><a href="./PolicyGradient.html#sec:policy-gradient-methods"><span class="toc-section-number">4</span> Policy Gradient methods</a><ul>
<li><a href="./PolicyGradient.html#sec:reinforce"><span class="toc-section-number">4.1</span> REINFORCE</a><ul>
<li><a href="./PolicyGradient.html#sec:estimating-the-policy-gradient"><span class="toc-section-number">4.1.1</span> Estimating the policy gradient</a></li>
<li><a href="./PolicyGradient.html#sec:reducing-the-variance"><span class="toc-section-number">4.1.2</span> Reducing the variance</a></li>
<li><a href="./PolicyGradient.html#sec:policy-gradient-theorem"><span class="toc-section-number">4.1.3</span> Policy Gradient theorem</a></li>
</ul></li>
<li><a href="./ActorCritic.html#sec:advantage-actor-critic-methods"><span class="toc-section-number">4.2</span> Advantage Actor-Critic methods</a><ul>
<li><a href="./ActorCritic.html#sec:advantage-actor-critic-a2c"><span class="toc-section-number">4.2.1</span> Advantage Actor-Critic (A2C)</a></li>
<li><a href="./ActorCritic.html#sec:asynchronous-advantage-actor-critic-a3c"><span class="toc-section-number">4.2.2</span> Asynchronous Advantage Actor-Critic (A3C)</a></li>
<li><a href="./ActorCritic.html#sec:generalized-advantage-estimation-gae"><span class="toc-section-number">4.2.3</span> Generalized Advantage Estimation (GAE)</a></li>
<li><a href="./ActorCritic.html#sec:stochastic-actor-critic-for-continuous-action-spaces"><span class="toc-section-number">4.2.4</span> Stochastic actor-critic for continuous action spaces</a></li>
</ul></li>
<li><a href="./ImportanceSampling.html#sec:off-policy-actor-critic"><span class="toc-section-number">4.3</span> Off-policy Actor-Critic</a><ul>
<li><a href="./ImportanceSampling.html#sec:importance-sampling"><span class="toc-section-number">4.3.1</span> Importance sampling</a></li>
<li><a href="./ImportanceSampling.html#sec:linear-off-policy-actor-critic-off-pac"><span class="toc-section-number">4.3.2</span> Linear Off-Policy Actor-Critic (Off-PAC)</a></li>
<li><a href="./ImportanceSampling.html#sec:retrace"><span class="toc-section-number">4.3.3</span> Retrace</a></li>
<li><a href="./ImportanceSampling.html#sec:self-imitation-learning-sil"><span class="toc-section-number">4.3.4</span> Self-Imitation Learning (SIL)</a></li>
</ul></li>
<li><a href="./DPG.html#sec:deterministic-policy-gradient-dpg"><span class="toc-section-number">4.4</span> Deterministic Policy Gradient (DPG)</a><ul>
<li><a href="./DPG.html#sec:deterministic-policy-gradient-theorem"><span class="toc-section-number">4.4.1</span> Deterministic policy gradient theorem</a></li>
<li><a href="./DPG.html#sec:deep-deterministic-policy-gradient-ddpg"><span class="toc-section-number">4.4.2</span> Deep Deterministic Policy Gradient (DDPG)</a></li>
<li><a href="./DPG.html#sec:distributed-distributional-ddpg-d4pg"><span class="toc-section-number">4.4.3</span> Distributed Distributional DDPG (D4PG)</a></li>
</ul></li>
<li><a href="./NaturalGradient.html#sec:natural-gradients"><span class="toc-section-number">4.5</span> Natural Gradients</a><ul>
<li><a href="./NaturalGradient.html#sec:principle-of-natural-gradients"><span class="toc-section-number">4.5.1</span> Principle of natural gradients</a></li>
<li><a href="./NaturalGradient.html#sec:natural-policy-gradient-and-natural-actor-critic-nac"><span class="toc-section-number">4.5.2</span> Natural policy gradient and Natural Actor Critic (NAC)</a></li>
<li><a href="./NaturalGradient.html#sec:trust-region-policy-optimization-trpo"><span class="toc-section-number">4.5.3</span> Trust Region Policy Optimization (TRPO)</a></li>
<li><a href="./NaturalGradient.html#sec:proximal-policy-optimization-ppo"><span class="toc-section-number">4.5.4</span> Proximal Policy Optimization (PPO)</a></li>
<li><a href="./NaturalGradient.html#sec:actor-critic-with-experience-replay-acer"><span class="toc-section-number">4.5.5</span> Actor-Critic with Experience Replay (ACER)</a></li>
</ul></li>
<li><a href="./OtherPolicyGradient.html#sec:maximum-entropy-rl"><span class="toc-section-number">4.6</span> Maximum Entropy RL</a><ul>
<li><a href="#sec:entropy-regularization-1"><span class="toc-section-number">4.6.1</span> Entropy regularization</a></li>
<li><a href="./OtherPolicyGradient.html#sec:soft-actor-critic-sac"><span class="toc-section-number">4.6.2</span> Soft Actor-Critic (SAC)</a></li>
</ul></li>
<li><a href="./OtherPolicyGradient.html#sec:distributional-learning"><span class="toc-section-number">4.7</span> Distributional learning</a><ul>
<li><a href="./OtherPolicyGradient.html#sec:the-reactor"><span class="toc-section-number">4.7.1</span> The Reactor</a></li>
</ul></li>
<li><a href="./OtherPolicyGradient.html#sec:other-policy-search-methods"><span class="toc-section-number">4.8</span> Other policy search methods</a><ul>
<li><a href="./OtherPolicyGradient.html#sec:stochastic-value-gradient-svg"><span class="toc-section-number">4.8.1</span> Stochastic Value Gradient (SVG)</a></li>
<li><a href="./OtherPolicyGradient.html#sec:q-prop"><span class="toc-section-number">4.8.2</span> Q-Prop</a></li>
<li><a href="./OtherPolicyGradient.html#sec:normalized-advantage-function-naf"><span class="toc-section-number">4.8.3</span> Normalized Advantage Function (NAF)</a></li>
<li><a href="./OtherPolicyGradient.html#sec:fictitious-self-play-fsp"><span class="toc-section-number">4.8.4</span> Fictitious Self-Play (FSP)</a></li>
</ul></li>
<li><a href="./OtherPolicyGradient.html#sec:comparison-between-value-based-and-policy-gradient-methods"><span class="toc-section-number">4.9</span> Comparison between value-based and policy gradient methods</a></li>
<li><a href="./OtherPolicyGradient.html#sec:gradient-free-policy-search"><span class="toc-section-number">4.10</span> Gradient-free policy search</a><ul>
<li><a href="./OtherPolicyGradient.html#sec:cross-entropy-method-cem"><span class="toc-section-number">4.10.1</span> Cross-entropy Method (CEM)</a></li>
<li><a href="./OtherPolicyGradient.html#sec:evolutionary-search-es"><span class="toc-section-number">4.10.2</span> Evolutionary Search (ES)</a></li>
</ul></li>
</ul></li>
<li><a href="./Practice.html#sec:deep-rl-in-practice"><span class="toc-section-number">5</span> Deep RL in practice</a><ul>
<li><a href="./Practice.html#sec:limitations"><span class="toc-section-number">5.1</span> Limitations</a></li>
<li><a href="./Practice.html#sec:reward-shaping"><span class="toc-section-number">5.2</span> Reward shaping</a></li>
<li><a href="./Practice.html#sec:simulation-environments"><span class="toc-section-number">5.3</span> Simulation environments</a></li>
<li><a href="./Practice.html#sec:algorithm-implementations"><span class="toc-section-number">5.4</span> Algorithm implementations</a></li>
</ul></li>
<li><a href="./References.html#sec:references">References</a></li>
</ul>


</nav>



<h2 id="sec:off-policy-actor-critic"><span class="header-section-number">4.3</span> Off-policy Actor-Critic</h2>
<p>Actor-critic architectures are generally <strong>on-policy</strong> algorithms: the actions used to explore the environment must have been generated by the actor, otherwise the feedback provided by the critic (the advantage) will introduce a huge bias (i.e. an error) in the policy gradient. This comes from the definition of the policy gradient theorem (Section <a href="./PolicyGradient.html#sec:policy-gradient-theorem">4.1.3</a>, Eq. <a href="#eq:policygradienttheorem">9</a>):</p>
<p><span class="math display">\[
    \nabla_\theta J(\theta) =  \mathbb{E}_{s \sim \rho^\pi, a \sim \pi_\theta}[\nabla_\theta \log \pi_\theta(s, a) \, Q^{\pi_\theta}(s, a)]
\]</span></p>
<p>The state distribution <span class="math inline">\(\rho^\pi\)</span> defines the ensemble of states that can be visited using the actor policy <span class="math inline">\(\pi_\theta\)</span>. If, during Monte-Carlo sampling of the policy gradient, the states <span class="math inline">\(s\)</span> do not come from this state distribution, the approximated policy gradient will be wrong (high bias) and the resulting policy will be suboptimal.</p>
<p>The major drawback of on-policy methods is their sample complexity: it is difficult to ensure that the <em>“interesting”</em> regions of the policy are actually discovered by the actor (see Fig. <a href="#fig:onlinepolicyexploration">24</a>). If the actor is initialized in a flat region of the reward space (where there is not a lot of rewards), policy gradient updates will only change slightly the policy and it may take a lot of iterations until interesting policies are discovered and fine-tuned.</p>
<figure>
<img src="img/onpolicyexploration.png" alt="Figure 24: Illustration of the sample complexity inherent to on-policy methods, where the actor has only two parameters \theta_1 and \theta_2. If only very small regions of the actor parameters are associated with high rewards, the policy might wander randomly for a very long time before “hitting”the interesting regions." id="fig:onlinepolicyexploration" style="width:50.0%" /><figcaption>Figure 24: Illustration of the sample complexity inherent to on-policy methods, where the actor has only two parameters <span class="math inline">\(\theta_1\)</span> and <span class="math inline">\(\theta_2\)</span>. If only very small regions of the actor parameters are associated with high rewards, the policy might wander randomly for a very long time before “hitting”the interesting regions.</figcaption>
</figure>
<p>The problem becomes even worse when the state or action spaces are highly dimensional, or when rewards are sparse. Imagine the scenario where you are searching for your lost keys at home (a sparse reward is delivered only once you find them): you could spend hours trying randomly each action at your disposal (looking in your jackets, on your counter, but also jumping around, cooking something, watching TV…) until finally you explore the action “look behind the curtains” and find them. (Note: with deep RL, you would even have to do that one million times in order to allow gradient descent to train your brain…). If you had somebody telling you “if I were you, I would first search in your jackets, then on your counter and finally behind the curtains, but forget about watching TV, you will never find anything by doing that”, this would certainly reduce your exploration time.</p>
<p>This is somehow the idea behind <strong>off-policy</strong> algorithms: they use a <strong>behavior policy</strong> <span class="math inline">\(b(s, a)\)</span> to explore the environment and train the <strong>target policy</strong> <span class="math inline">\(\pi(s, a)\)</span> to reproduce the best ones by estimating how good they are. This does not come without caveats: if the behavior policy does not explore the optimal actions, the target policy will likely not be able to find it by itself, except by chance. But if the behavior policy is good enough, this can drastically reduce the amount of exploration needed to obtain a satisfying policy. <span class="citation" data-cites="Sutton2017">Sutton and Barto (<a href="References.html#ref-Sutton2017" role="doc-biblioref">2017</a>)</span> noted that:</p>
<p><em>“On-policy methods are generally simpler and are considered first. Off-policy methods require additional concepts and notation, and because the data is due to a different policy, off-policy methods are often of greater variance and are slower to converge.”</em></p>
<p>The most famous off-policy method is Q-learning (Section <a href="./BasicRL.html#sec:temporal-difference">2.1.5</a>). The reason why it is off-policy is that it does not use the next executed action (<span class="math inline">\(a_{t+1}\)</span>) to update the value of an action, but the greedy ation in the next state, which is independent from exploration:</p>
<p><span class="math display">\[
    \delta = r(s, a, s&#39;) + \gamma \, \max_{a&#39;} Q^\pi(s&#39;, a&#39;) - Q^\pi(s, a)
\]</span></p>
<p>The only condition for Q-learning to work (in the tabular case) is that the behavior policy <span class="math inline">\(b(s,a)\)</span> must be able to explore actions which are selected by the target policy:</p>
<p><span id="eq:qlearningcondition"><span class="math display">\[
    \pi(s, a) &gt; 0 \rightarrow b(s, a) &gt; 0
\qquad(13)\]</span></span></p>
<p>Actions which would be selected by the target policy should be selected at least from time to time by the behavior policy in order to allow their update: if the target policy thinks this action should be executed, the behavior policy should try it to confirm or infirm this assumption. In mathematical terms, there is an assumption of <em>coverage</em> of <span class="math inline">\(\pi\)</span> by <span class="math inline">\(b\)</span> (the support of <span class="math inline">\(b\)</span> includes the one of <span class="math inline">\(\pi\)</span>).</p>
<p>There are mostly two ways to create the behavior policy:</p>
<ol type="1">
<li><p><em>Use expert knowledge / human demonstrations</em>. Not all available actions should be explored: the programmer already knows they do not belong to the optimal policy. When an agent learns to play chess, for example, the behavior policy could consist of the moves typically played by human experts: if chess masters play this move, it is likely to be a good action, so it should be tried out, valued and possibly incorporated into the target policy (if it is indeed a good action, experts might be wrong). A similar idea was used to bootstrap early versions of AlphaGo <span class="citation" data-cites="Silver2016">(Silver et al. <a href="References.html#ref-Silver2016" role="doc-biblioref">2016</a>)</span>. In robotics, one could for example use “classical” engineering methods to control the exploration of the robot, while learning (hopefully) a better policy. It is also possible to perform <em>imitation learning</em>, where the agent learns from human demonstrations (e.g. <span class="citation" data-cites="Levine2013">Levine and Koltun (<a href="References.html#ref-Levine2013" role="doc-biblioref">2013</a>)</span>).</p></li>
<li><p><em>Derive it from the target policy</em>. In Q-learning, the target policy can be <strong>deterministic</strong>, i.e. always select the greedy action (with the maximum Q-value). The behavior policy can be derived from the target policy by making it <em><span class="math inline">\(\epsilon\)</span>-soft</em>, for example using a <span class="math inline">\(\epsilon\)</span>-greedy or softmax action selection scheme on the Q-values learned by the target policy (see Section <a href="./BasicRL.html#sec:monte-carlo-sampling">2.1.4</a>).</p></li>
</ol>
<p>The second option allows to control the level of exploration during learning (by controlling <span class="math inline">\(\epsilon\)</span> or the softmax temperature) while making sure that the target policy (the one used in production) is deterministic and optimal. It furthermore makes sure that Eq. <a href="#eq:qlearningcondition">13</a> is respected: the greedy action of the target policy always has a non-zero probability of being selected by an <span class="math inline">\(\epsilon\)</span>-greedy or softmax action selection. This is harder to ensure using expert knowledge.</p>
<p>Q-learning methods such as DQN use this second option. The target policy in DQN is actually a greedy policy with respect to the Q-values (i.e. the action with the maximum Q-value will be deterministically chosen), but an <span class="math inline">\(\epsilon\)</span>-soft behavior policy is derived from it to ensure exploration. This explains now the following comment in the description of the DQN algorithm (Section <a href="./Valuebased.html#sec:deep-q-network-dqn">3.2</a>):</p>
<ul>
<li>Select the action <span class="math inline">\(a_t\)</span> based on the behavior policy derived from <span class="math inline">\(Q_\theta(s_t, a)\)</span> (e.g. softmax).</li>
</ul>
<p>Off-policy learning furthermore allows the use of an <strong>experience replay memory</strong>: in this case, the transitions used for training the target policy were generated by an older version of it (sometimes much older). Only off-policy methods can work with replay buffers. A3C is for example on-policy: it relies on multiple parallel learners to fight against the correlation of inputs and outputs.</p>
<h3 id="sec:importance-sampling"><span class="header-section-number">4.3.1</span> Importance sampling</h3>
<p>Off-policy methods learn a target policy <span class="math inline">\(\pi(s,a)\)</span> while exploring with a behavior policy <span class="math inline">\(b(s,a)\)</span>. The environment is sampled using the behavior policy to form estimates of the state or action values (for value-based methods) or of the policy gradient (for policy gradient methods). But is it mathematically correct?</p>
<p>In policy gradient methods, we want to maximize the expected return of trajectories:</p>
<p><span class="math display">\[
    J(\theta) = \mathbb{E}_{\tau \sim \rho_\theta}[R(\tau)] = \int_\tau \rho_\theta(\tau) \, R(\tau) \, d\tau \approx \frac{1}{N} \sum_{i=1}^N R(\tau_i)
\]</span></p>
<p>where <span class="math inline">\(\rho_\theta\)</span> is the distribution of trajectories <span class="math inline">\(\tau\)</span> generated by the <strong>target</strong> policy <span class="math inline">\(\pi_\theta\)</span>. Mathematical expectations can be approximating by an average of enough samples of the estimator (Monte-Carlo). In policy gradient, we estimate the gradient, but let’s consider we sample the objective function for now. If we use a behavior policy to generate the trajectories, what we are actually estimating is:</p>
<p><span class="math display">\[
    \hat{J}(\theta) = \mathbb{E}_{\tau \sim \rho_b}[R(\tau)] = \int_\tau \rho_b(\tau) \, R(\tau) \, d\tau
\]</span></p>
<p>where <span class="math inline">\(\rho_b\)</span> is the distribution of trajectories generated by the <strong>behavior</strong> policy. In the general case, there is no reason why <span class="math inline">\(\hat{J}(\theta)\)</span> should be close from <span class="math inline">\(J(\theta)\)</span>, even when taking their gradient.</p>
<p><strong>Importance sampling</strong> is a classical statistical method used to estimate properties of a distribution (here the expected return of the trajectories of the target policy) while only having samples generated from a different distribution (here the trajectories of the behavior policy). See for example <a href="https://statweb.stanford.edu/~owen/mc/Ch-var-is.pdf" class="uri">https://statweb.stanford.edu/~owen/mc/Ch-var-is.pdf</a> and <a href="http://timvieira.github.io/blog/post/2014/12/21/importance-sampling" class="uri">http://timvieira.github.io/blog/post/2014/12/21/importance-sampling</a> for more generic explanations.</p>
<p>The trick is simply to rewrite the objective function as:</p>
<p><span class="math display">\[
\begin{aligned}
    J(\theta) &amp; = \mathbb{E}_{\tau \sim \rho_\theta}[R(\tau)]  \\
              &amp; = \int_\tau \rho_\theta(\tau) \, R(\tau) \, d\tau \\
              &amp; = \int_\tau \frac{\rho_b(\tau)}{\rho_b(\tau)} \, \rho_\theta(\tau) \, R(\tau) \, d\tau \\
              &amp; = \int_\tau \rho_b(\tau) \frac{\rho_\theta(\tau)}{\rho_b(\tau)} \, R(\tau) \, d\tau \\
              &amp; = \mathbb{E}_{\tau \sim \rho_b}[\frac{\rho_\theta(\tau)}{\rho_b(\tau)} \, R(\tau)]  \\
\end{aligned}
\]</span></p>
<p>The ratio <span class="math inline">\(\frac{\rho_\theta(\tau)}{\rho_b(\tau)}\)</span> is called the <strong>importance sampling weight</strong> for the trajectory. If a trajectory generated by <span class="math inline">\(b\)</span> is associated with a lot of rewards <span class="math inline">\(R(\tau)\)</span> (with <span class="math inline">\(\rho_b(\tau)\)</span> significantly high), the actor should learn to reproduce that trajectory with a high probability <span class="math inline">\(\rho_\theta(\tau)\)</span>, as its goal is to maximize <span class="math inline">\(J(\theta)\)</span>. Conversely, if the associated reward is low (<span class="math inline">\(R(\tau)\approx 0\)</span>), the target policy can forget about it (by setting <span class="math inline">\(\rho_\theta(\tau) = 0\)</span>), even though the behavior policy still generates it!</p>
<p>The problem is now to estimate the importance sampling weight. Using the definition of the likelihood of a trajectory, the importance sampling weight only depends on the policies, not the dynamics of the environment (they cancel out):</p>
<p><span class="math display">\[
    \frac{\rho_\theta(\tau)}{\rho_b(\tau)} = \frac{p_0 (s_0) \, \prod_{t=0}^T \pi_\theta(s_t, a_t) p(s_{t+1} | s_t, a_t)}{p_0 (s_0) \, \prod_{t=0}^T b(s_t, a_t) p(s_{t+1} | s_t, a_t)} = \frac{\prod_{t=0}^T \pi_\theta(s_t, a_t)}{\prod_{t=0}^T b(s_t, a_t)} = \prod_{t=0}^T \frac{\pi_\theta(s_t, a_t)}{b(s_t, a_t)}
\]</span></p>
<p>This allows to estimate the objective function <span class="math inline">\(J(\theta)\)</span> using Monte Carlo sampling <span class="citation" data-cites="Meuleau2000 Peshkin2002">(Meuleau et al. <a href="References.html#ref-Meuleau2000" role="doc-biblioref">2000</a>; Peshkin and Shelton <a href="References.html#ref-Peshkin2002" role="doc-biblioref">2002</a>)</span>:</p>
<p><span class="math display">\[
  J(\theta) \approx \frac{1}{m} \, \sum_{i=1}^m \frac{\rho_\theta(\tau_i)}{\rho_b(\tau_i)} \, R(\tau_i)
\]</span></p>
<p>All one needs to do is to repeatedly apply the following algorithm:</p>
<hr />
<ol type="1">
<li><p>Generate <span class="math inline">\(m\)</span> trajectories <span class="math inline">\(\tau_i\)</span> using the behavior policy:</p>
<ul>
<li><p>For each transition <span class="math inline">\((s_t, a_t, s_{t+1})\)</span> of each trajectory, store:</p>
<ol type="1">
<li><p>The received reward <span class="math inline">\(r_{t+1}\)</span>.</p></li>
<li><p>The probability <span class="math inline">\(b(s_t, a_t)\)</span> that the behavior policy generates this transition.</p></li>
<li><p>The probability <span class="math inline">\(\pi_\theta(s_t, a_t)\)</span> that the target policy generates this transition.</p></li>
</ol></li>
</ul></li>
<li><p>Estimate the objective function with:</p></li>
</ol>
<p><span class="math display">\[
  \hat{J}(\theta) = \frac{1}{m} \, \sum_{i=1}^m \left(\prod_{t=0}^T \frac{\pi_\theta(s_t, a_t)}{b(s_t, a_t)} \right) \, \left(\sum_{t=0}^T \gamma^t \, r_{t+1} \right)
\]</span></p>
<ol start="3" type="1">
<li>Update the target policy to maximize <span class="math inline">\(\hat{J}(\theta)\)</span>.</li>
</ol>
<hr />
<p><span class="citation" data-cites="Tang2010">Tang and Abbeel (<a href="References.html#ref-Tang2010" role="doc-biblioref">2010</a>)</span> showed that the same idea can be applied to the policy gradient, under assumptions often met in practice:</p>
<p><span class="math display">\[
    \nabla_\theta J(\theta) =  \mathbb{E}_{\tau \sim \rho_b}[ \nabla_\theta \log \rho_\theta(\tau) \, \frac{\rho_\theta(\tau)}{\rho_b(\tau)} \, R(\tau)]
\]</span></p>
<p>When decomposing the policy gradient for each state encountered, one can also use the <strong>causality</strong> principle to simplify the terms:</p>
<ol type="1">
<li><p>The return after being in a state <span class="math inline">\(s_t\)</span> only depends on future states.</p></li>
<li><p>The importance sampling weight (relative probability of arriving in <span class="math inline">\(s_t\)</span> using the behavior and target policies) only depends on the past weights.</p></li>
</ol>
<p>This gives the following approximation of the policy gradient, used for example in Guided policy search <span class="citation" data-cites="Levine2013">(Levine and Koltun <a href="References.html#ref-Levine2013" role="doc-biblioref">2013</a>)</span>:</p>
<p><span class="math display">\[
    \nabla_\theta J(\theta) =  \mathbb{E}_{\tau \sim \rho_b}[ \sum_{t=0}^T \nabla_\theta \log \pi_\theta(s_t, a_t) \, \left(\prod_{t&#39;=0}^t \frac{\pi_\theta(s_{t&#39;}, a_{t&#39;})}{b(s_{t&#39;}, a_{t&#39;})} \right) \, \left(\sum_{t&#39;=t}^T \gamma^{t&#39;-t} \, r(s_{t&#39;}, a_{t&#39;}) \right)]
\]</span></p>
<h3 id="sec:linear-off-policy-actor-critic-off-pac"><span class="header-section-number">4.3.2</span> Linear Off-Policy Actor-Critic (Off-PAC)</h3>
<p>The first off-policy actor-critic method was proposed by <span class="citation" data-cites="Degris2012">Degris, White, and Sutton (<a href="References.html#ref-Degris2012" role="doc-biblioref">2012</a>)</span> for linear approximators. Another way to express the objective function in policy search is by using the Bellman equation (here in the off-policy setting):</p>
<p><span class="math display">\[
    J(\theta) = \mathbb{E}_{s \sim \rho_b} [V^{\pi_\theta}(s)] = \mathbb{E}_{s \sim \rho_b} [\sum_{a\in\mathcal{A}} \pi(s, a) \, Q^{\pi_\theta}(s, a)]
\]</span></p>
<p>Maximizing the value of all states reachable by the policy is the same as finding the optimal policy: the encoutered states bring the maximum return. The policy gradient becomes:</p>
<p><span class="math display">\[
    \nabla_\theta J(\theta) = \mathbb{E}_{s \sim \rho_b} [\sum_{a\in\mathcal{A}} \nabla_\theta  (\pi_\theta(s, a) \, Q^{\pi_\theta}(s, a))]
\]</span></p>
<p>Because both <span class="math inline">\(\pi(s, a)\)</span> and <span class="math inline">\(Q^\pi(s, a)\)</span> depend on the target policy <span class="math inline">\(\pi_\theta\)</span> (hence its parameters <span class="math inline">\(\theta\)</span>), one should normally write:</p>
<p><span class="math display">\[
    \nabla_\theta  (\pi_\theta(s, a) \, Q^{\pi_\theta}(s, a)) = Q^{\pi_\theta}(s, a) \, \nabla_\theta  \pi_\theta(s, a) + \pi_\theta(s, a) \, \nabla_\theta Q^{\pi_\theta}(s, a)
\]</span></p>
<p>The second term depends on <span class="math inline">\(\nabla_\theta Q^{\pi_\theta}(s, a)\)</span>, which is very difficult to estimate. <span class="citation" data-cites="Degris2012">Degris, White, and Sutton (<a href="References.html#ref-Degris2012" role="doc-biblioref">2012</a>)</span> showed that when the Q-values are estimated by an unbiased <strong>critic</strong> <span class="math inline">\(Q_\varphi(s, a)\)</span>, this second term can be omitted. Using the log-trick and importance sampling, the policy gradient can be expressed as:</p>
<p><span class="math display">\[
\begin{aligned}
    \nabla_\theta J(\theta) &amp; = \mathbb{E}_{s \sim \rho_b} [\sum_{a\in\mathcal{A}} Q_\varphi(s, a) \, \nabla_\theta \pi_\theta(s, a)] \\
                            &amp; = \mathbb{E}_{s \sim \rho_b} [\sum_{a\in\mathcal{A}} b(s, a) \, \frac{\pi_\theta(s, a)}{b(s, a)} \, Q_\varphi(s, a) \, \frac{\nabla_\theta \pi_\theta(s, a)}{\pi_\theta(s, a)}] \\
                            &amp; = \mathbb{E}_{s,a \sim \rho_b} [\frac{\pi_\theta(s, a)}{b(s, a)} \, Q_\varphi(s, a) \, \nabla_\theta \log \pi_\theta(s, a)] \\
\end{aligned}
\]</span></p>
<p>We now have an <strong>actor-critic</strong> architecture (actor <span class="math inline">\(\pi_\theta(s, a)\)</span>, critic <span class="math inline">\(Q_\varphi(s, a)\)</span>) able to learn from single transitions <span class="math inline">\((s,a)\)</span> (<strong>online update</strong> instead of complete trajectories) generated <strong>off-policy</strong> (behavior policy <span class="math inline">\(b(s,a)\)</span> and importance sampling weight <span class="math inline">\(\frac{\pi_\theta(s, a)}{b(s, a)}\)</span>). The off-policy actor-critic (Off-PAC) algorithm of <span class="citation" data-cites="Degris2012">Degris, White, and Sutton (<a href="References.html#ref-Degris2012" role="doc-biblioref">2012</a>)</span> furthermore uses <strong>eligibility traces</strong> to stabilize learning. However, it was limited to linear function approximators because its variance is too high to train deep neural networks.</p>
<h3 id="sec:retrace"><span class="header-section-number">4.3.3</span> Retrace</h3>
<p>For a good deep RL algorithm, we need the two following properties:</p>
<ol type="1">
<li><p><strong>Off-policy</strong> learning: it allows to learn from transitions stored in a replay buffer (i.e. generated with an older policy). As NN need many iterations to converge, it is important to be able to re-use old transitions for its training, instead of constantly sampling new ones (sample complexity). Multiple parallel actors as in A3C allow to mitigate this problem, but it is still too complex.</p></li>
<li><p><strong>Multi-step returns</strong>: the two extremes of RL are TD (using a single “real” reward for the update, the rest is estimated) amd Monte-Carlo (use only “real” rewards, no estimation). TD has a smaller variance, but a high bias (errors in estimates propagate to all other values), while MC has a small bias but a high variance (learns from many real rewrads, but the returns may vary a lot between two almost identical episodes). Eligibility traces and n-step returns (used in A3C) are the most common trade-off between TD and MC.</p></li>
</ol>
<p>The <strong>Retrace</strong> algorithm <span class="citation" data-cites="Munos2016">(Munos et al. <a href="References.html#ref-Munos2016" role="doc-biblioref">2016</a>)</span> is designed to exhibit both properties when learning Q-values. It can therefore be used to train the critic (instead of classical Q-learning) and provide the actor with safe, efficient and low-variance values.</p>
<p>In the generic form, Q-learning updates the Q-value of a transition <span class="math inline">\((s_t, a_t)\)</span> using the TD error:</p>
<p><span class="math display">\[
    \Delta Q^\pi(s_t, a_t) = \alpha \, \delta_t = \alpha \, (r_{t+1} + \gamma \, \max_a Q^\pi(s_{t+1}, a_{t+1}) - Q^\pi(s_t, a_t))
\]</span></p>
<p>When using eligibility traces in the forward view (Section <a href="./BasicRL.html#sec:eligibility-traces">2.1.6</a>), the change in Q-value depends also on the TD error of future transitions at times <span class="math inline">\(t&#39; &gt; t\)</span>. A parameter <span class="math inline">\(\lambda\)</span> ensures the stability of the update:</p>
<p><span class="math display">\[
    \Delta Q^\pi(s_t, a_t) = \alpha \, \sum_{t&#39;=t}^T (\gamma \lambda)^{t&#39;-t} \delta_{t&#39;}
\]</span></p>
<p>The Retrace algorithm proposes to generalize this formula using a parameter <span class="math inline">\(c_s\)</span> for each timestep between <span class="math inline">\(t\)</span> and <span class="math inline">\(t&#39;\)</span>:</p>
<p><span class="math display">\[
    \Delta Q^\pi(s_t, a_t) = \alpha \, \sum_{t&#39;=t}^T (\gamma)^{t&#39;-t} \left(\prod_{s=t+1}^{t&#39;} c_s \right) \, \delta_{t&#39;}
\]</span></p>
<p>Depending on the choice of <span class="math inline">\(c_s\)</span>, the formula covers different existing methods:</p>
<ol type="1">
<li><span class="math inline">\(c_s = \lambda\)</span> is the classical <strong>eligibility trace</strong> mechanism (<span class="math inline">\(Q(\lambda)\)</span>) in its forward view, which is not safe: the behavior policy <span class="math inline">\(b\)</span> must be very close from the target policy <span class="math inline">\(\tau\)</span>:</li>
</ol>
<p><span class="math display">\[
    || \pi - b ||_1 \leq \frac{1 - \gamma}{\lambda \gamma}
\]</span></p>
<p>As <span class="math inline">\(\gamma\)</span> is typically chosen very close from 1 (e.g. 0.99), this does not leave much room for the target policy to differ from the behavior policy <span class="citation" data-cites="Harutyunyan2016">(see Harutyunyan et al. <a href="References.html#ref-Harutyunyan2016" role="doc-biblioref">2016</a> for the proof)</span>.</p>
<ol start="2" type="1">
<li><p><span class="math inline">\(c_s = \frac{\pi(s_s, a_s)}{b(s_s, a_s)}\)</span> is the importance sampling weight. As shown in Section <a href="./ImportanceSampling.html#sec:importance-sampling">4.3.1</a>, importance sampling is unbiased in off-policy settings, but can have a very large variance: the product of ratios <span class="math inline">\(\prod_{s=t+1}^{t&#39;} \frac{\pi(s_s, a_s)}{b(s_s, a_s)}\)</span> can quickly vary between two episodes.</p></li>
<li><p><span class="math inline">\(c_s = \pi(s_s, a_s)\)</span> corresponds to the tree-backup algorithm <span class="math inline">\(TB(\lambda)\)</span> <span class="citation" data-cites="Precup2000">(Precup, Sutton, and Singh <a href="References.html#ref-Precup2000" role="doc-biblioref">2000</a>)</span>. It has the advantage to work for arbitrary policies <span class="math inline">\(\pi\)</span> and <span class="math inline">\(b\)</span>, but the product of such probabilities decays very fast to zero when the time difference <span class="math inline">\(t&#39; - t\)</span> increases: TD errors will be efficiently shared over a couple of steps only.</p></li>
</ol>
<p>For Retrace, <span class="citation" data-cites="Munos2016">Munos et al. (<a href="References.html#ref-Munos2016" role="doc-biblioref">2016</a>)</span> showed that a much better value for <span class="math inline">\(c_s\)</span> is:</p>
<p><span class="math display">\[
    c_s = \lambda \min (1, \frac{\pi(s_s, a_s)}{b(s_s, a_s)})
\]</span></p>
<p>The importance sampling weight is clipped to 1, and decays exponentially with the parameter <span class="math inline">\(\lambda\)</span>. It can be seen as a trade-off between importance sampling and eligibility traces. The authors showed that Retrace(<span class="math inline">\(\lambda\)</span>) has a low variance (as it uses multiple returns), is safe (works for all <span class="math inline">\(\pi\)</span> and <span class="math inline">\(b\)</span>) and efficient (it can propagate rewards over many time steps). They used retrace to learn Atari games and compared it positively with DQN, both in terms of optimality and speed of learning. These properties make Retrace particularly suited for deep RL and actor-critic architectures: it is for example used in ACER (Section <a href="./NaturalGradient.html#sec:actor-critic-with-experience-replay-acer">4.5.5</a>) and the Reactor (Section <a href="./OtherPolicyGradient.html#sec:the-reactor">4.7.1</a>).</p>
<p>Rémi Munos uploaded some slides explaining Retrace in a simpler manner than in the original paper: <a href="https://ewrl.files.wordpress.com/2016/12/munos.pdf" class="uri">https://ewrl.files.wordpress.com/2016/12/munos.pdf</a>.</p>
<h3 id="sec:self-imitation-learning-sil"><span class="header-section-number">4.3.4</span> Self-Imitation Learning (SIL)</h3>
<p>We have discussed or now only strictly on-policy or off-policy methods. Off-policy methods are much more stable and efficient, but they learn generally a deterministic policy, what can be problematic in stochastic environments (e.g. two players games: being predictable is clearly an issue). Hybrid methods combining on- and off-policy mechanisms have clearly a great potential.</p>
<p><span class="citation" data-cites="Oh2018">Oh et al. (<a href="References.html#ref-Oh2018" role="doc-biblioref">2018</a>)</span> proposed a <strong>Self-Imitation Learning</strong> (SIL) method that can extend on-policy actor-critic algorithms (e.g. A2C, Section <a href="./ActorCritic.html#sec:advantage-actor-critic-a2c">4.2.1</a>) with a replay buffer able to feed past <em>good</em> experiences to the NN to speed up learning.</p>
<p>The main idea is to use <strong>prioritized experience replay</strong> (<span class="citation" data-cites="Schaul2015">Schaul et al. (<a href="References.html#ref-Schaul2015" role="doc-biblioref">2015</a>)</span>, see Section <a href="./Valuebased.html#sec:prioritized-experience-replay">3.4</a>) to select only transitions whose actual return is higher than their current expected value. This defines two additional losses for the actor and the critic:</p>
<p><span class="math display">\[
    \mathcal{L}^\text{SIL}_\text{actor}(\theta) = \mathbb{E}_{s, a \in \mathcal{D}}[\log \pi_\theta(s, a) \, (R(s, a) - V_\varphi(s))^+]
\]</span> <span class="math display">\[
    \mathcal{L}^\text{SIL}_\text{critic}(\varphi) = \mathbb{E}_{s, a \in \mathcal{D}}[((R(s, a) - V_\varphi(s))^+)^2]
\]</span></p>
<p>where <span class="math inline">\((x)^+ = \max(0, x)\)</span> is the positive function. Transitions sampled from the replay buffer will participate to the off-policy learning only if their return is higher that the current value of the state, i.e. if they are good experiences compared to what is currently known (<span class="math inline">\(V_\varphi(s)\)</span>). The pseudo-algorithm is actually quite simple and simply extends the A2C procedure:</p>
<hr />
<ul>
<li><p>Initialize the actor <span class="math inline">\(\pi_\theta\)</span> and the critic <span class="math inline">\(V_\varphi\)</span> with random weights.</p></li>
<li><p>Initialize the prioritized experience replay buffer <span class="math inline">\(\mathcal{D}\)</span>.</p></li>
<li><p>Observe the initial state <span class="math inline">\(s_0\)</span>.</p></li>
<li><p>for <span class="math inline">\(t \in [0, T_\text{total}]\)</span>:</p>
<ul>
<li><p>Initialize empty episode minibatch.</p></li>
<li><p>for <span class="math inline">\(k \in [0, n]\)</span>: # Sample episode</p>
<ul>
<li><p>Select a action <span class="math inline">\(a_k\)</span> using the actor <span class="math inline">\(\pi_\theta\)</span>.</p></li>
<li><p>Perform the action <span class="math inline">\(a_k\)</span> and observe the next state <span class="math inline">\(s_{k+1}\)</span> and the reward <span class="math inline">\(r_{k+1}\)</span>.</p></li>
<li><p>Store <span class="math inline">\((s_k, a_k, r_{k+1})\)</span> in the episode minibatch.</p></li>
</ul></li>
<li><p>if <span class="math inline">\(s_n\)</span> is not terminal: set <span class="math inline">\(R_n = V_\varphi(s_n)\)</span> with the critic, else <span class="math inline">\(R_n=0\)</span>.</p></li>
<li><p>for <span class="math inline">\(k \in [n-1, 0]\)</span>: # Backwards iteration over the episode</p>
<ul>
<li>Update the discounted sum of rewards <span class="math inline">\(R_k = r_k + \gamma \, R_{k+1}\)</span> <strong>and store it in the replay buffer <span class="math inline">\(\mathcal{D}\)</span></strong>.</li>
</ul></li>
<li><p>Update the actor and the critic <strong>on-policy</strong> with the episode:</p></li>
</ul>
<p><span class="math display">\[
      \theta \leftarrow \theta + \eta \, \sum_k \nabla_\theta \log \pi_\theta(s_k, a_k) \, (R_k - V_\varphi(s_k))
  \]</span></p>
<p><span class="math display">\[
      \varphi \leftarrow \varphi + \eta \, \sum_k \nabla_\varphi (R - V_\varphi(s_k))^2
  \]</span></p>
<ul>
<li><p>for <span class="math inline">\(m \in [0, M]\)</span>:</p>
<ul>
<li><p>Sample a minibatch of K transitions <span class="math inline">\((s_k, a_k, R_k)\)</span> from the replay buffer <span class="math inline">\(\mathcal{D}\)</span> prioritized with high <span class="math inline">\((R_k - V_\varphi(s_k))\)</span>.</p></li>
<li><p>Update the actor and the critic <strong>off-policy</strong> with self-imitation.</p></li>
</ul>
<p><span class="math display">\[
      \theta \leftarrow \theta + \eta \, \sum_k \nabla_\theta \log \pi_\theta(s_k, a_k) \, (R_k - V_\varphi(s_k))^+
  \]</span></p>
<p><span class="math display">\[
      \varphi \leftarrow \varphi + \eta \, \sum_k \nabla_\varphi ((R_k - V_\varphi(s_k))^+)^2
  \]</span></p></li>
</ul></li>
</ul>
<hr />
<p>In the paper, they furthermore used entropy regularization as in A3C. They showed that A2C+SIL has a better performance both on Atari games and continuous control problems (Mujoco) than state-of-the art methods (A3C, TRPO, Reactor, PPO). It shows that self-imitation learning can be very useful in problems where exploration is hard: a proper level of exploitation of past experiences actually fosters a deeper exploration of environment.</p>

<br>
<div class="arrows">
<a href="ActorCritic.html" class="previous">&laquo; Previous</a>
<a href="DPG.html" class="next">Next &raquo;</a>
</div>


</article>
</body>
</html>
