<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <meta name="author" content="Julien Vitay">
  <title>Deep Reinforcement Learning</title>


  <link rel="stylesheet" href="assets/syntax.css">
  <link rel="stylesheet" href="assets/github.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->

</head>

<body class="markdown-body">

<header>
  <h1 class="title"><strong>Deep Reinforcement Learning</strong></h1>
  <p class="author">Julien Vitay - <a href="mailto:julien.vitay@informatik.tu-chemnitz.de">julien.vitay@informatik.tu-chemnitz.de</a></p>
</header>


<article>

<nav id="TOC" class ="toc">
  <h2><strong>Deep Reinforcement Learning</strong></h2>
  <p class="author">Julien Vitay</p>

<ul>
<li><a href="./Introduction.html#sec:introduction"><span class="toc-section-number">1</span> Introduction</a></li>
<li><a href="./BasicRL.html#sec:basics"><span class="toc-section-number">2</span> Basics</a><ul>
<li><a href="./BasicRL.html#sec:reinforcement-learning-and-markov-decision-process"><span class="toc-section-number">2.1</span> Reinforcement learning and Markov Decision Process</a><ul>
<li><a href="./BasicRL.html#sec:policy-and-value-functions"><span class="toc-section-number">2.1.1</span> Policy and value functions</a></li>
<li><a href="./BasicRL.html#sec:bellman-equations"><span class="toc-section-number">2.1.2</span> Bellman equations</a></li>
<li><a href="./BasicRL.html#sec:dynamic-programming"><span class="toc-section-number">2.1.3</span> Dynamic programming</a></li>
<li><a href="./BasicRL.html#sec:monte-carlo-sampling"><span class="toc-section-number">2.1.4</span> Monte-Carlo sampling</a></li>
<li><a href="./BasicRL.html#sec:temporal-difference"><span class="toc-section-number">2.1.5</span> Temporal Difference</a></li>
<li><a href="./BasicRL.html#sec:eligibility-traces"><span class="toc-section-number">2.1.6</span> Eligibility traces</a></li>
<li><a href="./BasicRL.html#sec:actor-critic-architectures"><span class="toc-section-number">2.1.7</span> Actor-critic architectures</a></li>
<li><a href="./BasicRL.html#sec:function-approximation"><span class="toc-section-number">2.1.8</span> Function approximation</a></li>
</ul></li>
<li><a href="./DeepLearning.html#sec:deep-learning"><span class="toc-section-number">2.2</span> Deep learning</a><ul>
<li><a href="./DeepLearning.html#sec:deep-neural-networks"><span class="toc-section-number">2.2.1</span> Deep neural networks</a></li>
<li><a href="./DeepLearning.html#sec:convolutional-networks"><span class="toc-section-number">2.2.2</span> Convolutional networks</a></li>
<li><a href="./DeepLearning.html#sec:recurrent-neural-networks"><span class="toc-section-number">2.2.3</span> Recurrent neural networks</a></li>
</ul></li>
</ul></li>
<li><a href="./Valuebased.html#sec:value-based-methods"><span class="toc-section-number">3</span> Value-based methods</a><ul>
<li><a href="./Valuebased.html#sec:limitations-of-deep-neural-networks-for-function-approximation"><span class="toc-section-number">3.1</span> Limitations of deep neural networks for function approximation</a></li>
<li><a href="./Valuebased.html#sec:deep-q-network-dqn"><span class="toc-section-number">3.2</span> Deep Q-Network (DQN)</a></li>
<li><a href="./Valuebased.html#sec:double-dqn"><span class="toc-section-number">3.3</span> Double DQN</a></li>
<li><a href="./Valuebased.html#sec:prioritized-experience-replay"><span class="toc-section-number">3.4</span> Prioritized experience replay</a></li>
<li><a href="./Valuebased.html#sec:duelling-network"><span class="toc-section-number">3.5</span> Duelling network</a></li>
<li><a href="./Valuebased.html#sec:distributed-dqn-gorila"><span class="toc-section-number">3.6</span> Distributed DQN (GORILA)</a></li>
<li><a href="./Valuebased.html#sec:deep-recurrent-q-learning-drqn"><span class="toc-section-number">3.7</span> Deep Recurrent Q-learning (DRQN)</a></li>
<li><a href="./Valuebased.html#sec:other-variants-of-dqn"><span class="toc-section-number">3.8</span> Other variants of DQN</a></li>
</ul></li>
<li><a href="./PolicyGradient.html#sec:policy-gradient-methods"><span class="toc-section-number">4</span> Policy Gradient methods</a><ul>
<li><a href="./PolicyGradient.html#sec:reinforce"><span class="toc-section-number">4.1</span> REINFORCE</a><ul>
<li><a href="./PolicyGradient.html#sec:estimating-the-policy-gradient"><span class="toc-section-number">4.1.1</span> Estimating the policy gradient</a></li>
<li><a href="./PolicyGradient.html#sec:reducing-the-variance"><span class="toc-section-number">4.1.2</span> Reducing the variance</a></li>
<li><a href="./PolicyGradient.html#sec:policy-gradient-theorem"><span class="toc-section-number">4.1.3</span> Policy Gradient theorem</a></li>
</ul></li>
<li><a href="./ActorCritic.html#sec:advantage-actor-critic-methods"><span class="toc-section-number">4.2</span> Advantage Actor-Critic methods</a><ul>
<li><a href="./ActorCritic.html#sec:advantage-actor-critic-a2c"><span class="toc-section-number">4.2.1</span> Advantage Actor-Critic (A2C)</a></li>
<li><a href="./ActorCritic.html#sec:asynchronous-advantage-actor-critic-a3c"><span class="toc-section-number">4.2.2</span> Asynchronous Advantage Actor-Critic (A3C)</a></li>
<li><a href="./ActorCritic.html#sec:generalized-advantage-estimation-gae"><span class="toc-section-number">4.2.3</span> Generalized Advantage Estimation (GAE)</a></li>
<li><a href="./ActorCritic.html#sec:stochastic-actor-critic-for-continuous-action-spaces"><span class="toc-section-number">4.2.4</span> Stochastic actor-critic for continuous action spaces</a></li>
</ul></li>
<li><a href="./ImportanceSampling.html#sec:off-policy-actor-critic"><span class="toc-section-number">4.3</span> Off-policy Actor-Critic</a><ul>
<li><a href="./ImportanceSampling.html#sec:importance-sampling"><span class="toc-section-number">4.3.1</span> Importance sampling</a></li>
<li><a href="./ImportanceSampling.html#sec:linear-off-policy-actor-critic-off-pac"><span class="toc-section-number">4.3.2</span> Linear Off-Policy Actor-Critic (Off-PAC)</a></li>
<li><a href="./ImportanceSampling.html#sec:retrace"><span class="toc-section-number">4.3.3</span> Retrace</a></li>
<li><a href="./ImportanceSampling.html#sec:self-imitation-learning-sil"><span class="toc-section-number">4.3.4</span> Self-Imitation Learning (SIL)</a></li>
</ul></li>
<li><a href="./DPG.html#sec:deterministic-policy-gradient-dpg"><span class="toc-section-number">4.4</span> Deterministic Policy Gradient (DPG)</a><ul>
<li><a href="./DPG.html#sec:deterministic-policy-gradient-theorem"><span class="toc-section-number">4.4.1</span> Deterministic policy gradient theorem</a></li>
<li><a href="./DPG.html#sec:deep-deterministic-policy-gradient-ddpg"><span class="toc-section-number">4.4.2</span> Deep Deterministic Policy Gradient (DDPG)</a></li>
<li><a href="./DPG.html#sec:distributed-distributional-ddpg-d4pg"><span class="toc-section-number">4.4.3</span> Distributed Distributional DDPG (D4PG)</a></li>
</ul></li>
<li><a href="./NaturalGradient.html#sec:natural-gradients"><span class="toc-section-number">4.5</span> Natural Gradients</a><ul>
<li><a href="./NaturalGradient.html#sec:principle-of-natural-gradients"><span class="toc-section-number">4.5.1</span> Principle of natural gradients</a></li>
<li><a href="./NaturalGradient.html#sec:natural-policy-gradient-and-natural-actor-critic-nac"><span class="toc-section-number">4.5.2</span> Natural policy gradient and Natural Actor Critic (NAC)</a></li>
<li><a href="./NaturalGradient.html#sec:trust-region-policy-optimization-trpo"><span class="toc-section-number">4.5.3</span> Trust Region Policy Optimization (TRPO)</a></li>
<li><a href="./NaturalGradient.html#sec:proximal-policy-optimization-ppo"><span class="toc-section-number">4.5.4</span> Proximal Policy Optimization (PPO)</a></li>
<li><a href="./NaturalGradient.html#sec:actor-critic-with-experience-replay-acer"><span class="toc-section-number">4.5.5</span> Actor-Critic with Experience Replay (ACER)</a></li>
</ul></li>
<li><a href="./EntropyRL.html#sec:maximum-entropy-rl"><span class="toc-section-number">4.6</span> Maximum Entropy RL</a><ul>
<li><a href="#sec:entropy-regularization-1"><span class="toc-section-number">4.6.1</span> Entropy regularization</a></li>
<li><a href="./EntropyRL.html#sec:soft-q-learning"><span class="toc-section-number">4.6.2</span> Soft Q-learning</a></li>
<li><a href="./EntropyRL.html#sec:soft-actor-critic-sac"><span class="toc-section-number">4.6.3</span> Soft Actor-Critic (SAC)</a></li>
</ul></li>
<li><a href="./DistributionalRL.html#sec:distributional-learning"><span class="toc-section-number">4.7</span> Distributional learning</a><ul>
<li><a href="./DistributionalRL.html#sec:categorical-dqn"><span class="toc-section-number">4.7.1</span> Categorical DQN</a></li>
<li><a href="./DistributionalRL.html#sec:the-reactor"><span class="toc-section-number">4.7.2</span> The Reactor</a></li>
</ul></li>
<li><a href="./OtherPolicyGradient.html#sec:miscellaneous-model-free-algorithm"><span class="toc-section-number">4.8</span> Miscellaneous model-free algorithm</a><ul>
<li><a href="./OtherPolicyGradient.html#sec:stochastic-value-gradient-svg"><span class="toc-section-number">4.8.1</span> Stochastic Value Gradient (SVG)</a></li>
<li><a href="./OtherPolicyGradient.html#sec:q-prop"><span class="toc-section-number">4.8.2</span> Q-Prop</a></li>
<li><a href="./OtherPolicyGradient.html#sec:normalized-advantage-function-naf"><span class="toc-section-number">4.8.3</span> Normalized Advantage Function (NAF)</a></li>
<li><a href="./OtherPolicyGradient.html#sec:fictitious-self-play-fsp"><span class="toc-section-number">4.8.4</span> Fictitious Self-Play (FSP)</a></li>
</ul></li>
<li><a href="./OtherPolicyGradient.html#sec:comparison-between-value-based-and-policy-gradient-methods"><span class="toc-section-number">4.9</span> Comparison between value-based and policy gradient methods</a></li>
<li><a href="./OtherPolicyGradient.html#sec:gradient-free-policy-search"><span class="toc-section-number">4.10</span> Gradient-free policy search</a><ul>
<li><a href="./OtherPolicyGradient.html#sec:cross-entropy-method-cem"><span class="toc-section-number">4.10.1</span> Cross-entropy Method (CEM)</a></li>
<li><a href="./OtherPolicyGradient.html#sec:evolutionary-search-es"><span class="toc-section-number">4.10.2</span> Evolutionary Search (ES)</a></li>
</ul></li>
</ul></li>
<li><a href="./ModelBased.html#sec:model-based-rl"><span class="toc-section-number">5</span> Model-based RL</a><ul>
<li><a href="./ModelBased.html#sec:dyna-q"><span class="toc-section-number">5.1</span> Dyna-Q</a></li>
<li><a href="./ModelBased.html#sec:unsorted-references"><span class="toc-section-number">5.2</span> Unsorted references</a></li>
</ul></li>
<li><a href="./Practice.html#sec:deep-rl-in-practice"><span class="toc-section-number">6</span> Deep RL in practice</a><ul>
<li><a href="./Practice.html#sec:limitations"><span class="toc-section-number">6.1</span> Limitations</a></li>
<li><a href="./Practice.html#sec:simulation-environments"><span class="toc-section-number">6.2</span> Simulation environments</a></li>
<li><a href="./Practice.html#sec:algorithm-implementations"><span class="toc-section-number">6.3</span> Algorithm implementations</a></li>
</ul></li>
<li><a href="./References.html#sec:references">References</a></li>
</ul>


</nav>



<h2 id="sec:advantage-actor-critic-methods"><span class="header-section-number">4.2</span> Advantage Actor-Critic methods</h2>
<p>The <em>policy gradient theorem</em> provides an actor-critic architecture able to learn parameterized policies. In comparison to REINFORCE, the policy gradient depends on the Q-values of the actions taken during the trajectory rather than on the obtained returns <span class="math inline">\(R(\tau)\)</span>. Quite obviously, it will also suffer from the high variance of the gradient (Section <a href="./PolicyGradient.html#sec:reducing-the-variance">4.1.2</a>), requiring the use of baselines. In this section, the baseline is state-dependent and chosen equal to the value of the state <span class="math inline">\(V^\pi(s)\)</span>, so the factor multiplying the log-likelihood of the policy is:</p>
<p><span class="math display">\[
    A^{\pi}(s, a) = Q^{\pi}(s, a) - V^\pi(s)
\]</span></p>
<p>which is the <strong>advantage</strong> of the action <span class="math inline">\(a\)</span> in <span class="math inline">\(s\)</span>, as already seen in <em>duelling networks</em> (Section <a href="./Valuebased.html#sec:duelling-network">3.5</a>).</p>
<p>Now the problem is that the critic would have to approximate two functions: <span class="math inline">\(Q^{\pi}(s, a)\)</span> and <span class="math inline">\(V^{\pi}(s)\)</span>. <strong>Advantage actor-critic</strong> methods presented in this section (A2C, A3C, GAE) approximate the advantage of an action:</p>
<p><span class="math display">\[
    \nabla_\theta J(\theta) =  \mathbb{E}_{s \sim \rho^\pi, a \sim \pi_\theta}[\nabla_\theta \log \pi_\theta(s, a) \, A_\varphi(s, a)]
\]</span></p>
<p><span class="math inline">\(A_\varphi(s, a)\)</span> is called the <strong>advantage estimate</strong> and should be equal to the real advantage <em>in expectation</em>.</p>
<p>Different methods could be used to compute the advantage estimate:</p>
<ul>
<li><p><span class="math inline">\(A_\varphi(s, a) = R(s, a) - V_\varphi(s)\)</span> is the <strong>MC advantage estimate</strong>, the Q-value of the action being replaced by the actual return.</p></li>
<li><p><span class="math inline">\(A_\varphi(s, a) = r(s, a, s&#39;) + \gamma \, V_\varphi(s&#39;) - V_\varphi(s)\)</span> is the <strong>TD advantage estimate</strong> or TD error.</p></li>
<li><p><span class="math inline">\(A_\varphi(s, a) = \sum_{k=0}^{n-1} \gamma^{k} \, r_{t+k+1} + \gamma^n \, V_\varphi(s_{t+n+1}) - V_\varphi(s_t)\)</span> is the <strong>n-step advantage estimate</strong>.</p></li>
</ul>
<p>The most popular approach is the n-step advantage, which is at the core of the methods A2C and A3C, and can be understood as a trade-off between MC and TD. MC and TD advantages could be used as well, but come with the respective disadvantages of MC (need for finite episodes, slow updates) and TD (unstable). Generalized Advantage Estimation (GAE, Section <a href="./ActorCritic.html#sec:generalized-advantage-estimation-gae">4.2.3</a>) takes another interesting approach to estimate the advantage.</p>
<p><em>Note:</em> A2C is actually derived from the A3C algorithm presented later, but it is simpler to explain it first. See <a href="https://blog.openai.com/baselines-acktr-a2c/" class="uri">https://blog.openai.com/baselines-acktr-a2c/</a> for an explanation of the reasons. A good explanation of A2C and A3C with Python code is available at <a href="https://cgnicholls.github.io/reinforcement-learning/2017/03/27/a3c.html" class="uri">https://cgnicholls.github.io/reinforcement-learning/2017/03/27/a3c.html</a>.</p>
<h3 id="sec:advantage-actor-critic-a2c"><span class="header-section-number">4.2.1</span> Advantage Actor-Critic (A2C)</h3>
<p>The first aspect of A2C is that it relies on n-step updating, which is a trade-off between MC and TD:</p>
<ul>
<li>MC waits until the end of an episode to update the value of an action using the reward to-go (sum of obtained rewards) <span class="math inline">\(R(s, a)\)</span>.</li>
<li>TD updates immediately the action using the immediate reward <span class="math inline">\(r(s, a, s&#39;)\)</span> and approximates the rest with the value of the next state <span class="math inline">\(V^\pi(s)\)</span>.</li>
<li>n-step uses the <span class="math inline">\(n\)</span> next immediate rewards and approximates the rest with the value of the state visited <span class="math inline">\(n\)</span> steps later.</li>
</ul>
<p><span id="eq:a2c"><span class="math display">\[
    \nabla_\theta J(\theta) =  \mathbb{E}_{s_t \sim \rho^\pi, a_t \sim \pi_\theta}[\nabla_\theta \log \pi_\theta(s_t, a_t) \, ( \sum_{k=0}^{n-1} \gamma^{k} \, r_{t+k+1} + \gamma^n \, V_\varphi(s_{t+n+1}) - V_\varphi(s_t))]
\qquad(11)\]</span></span></p>
<p>TD can be therefore be seen as a 1-step algorithm. For sparse rewards (mostly zero, +1 or -1 at the end of a game for example), this allows to update the <span class="math inline">\(n\)</span> last actions which lead to a win/loss, instead of only the last one in TD, speeding up learning. However, there is no need for finite episodes as in MC. In other words, n-step estimation ensures a trade-off between bias (wrong updates based on estimated values as in TD) and variance (variability of the obtained returns as in MC). An alternative to n-step updating is the use of <em>eligibility traces</em> (see Section <a href="./BasicRL.html#sec:eligibility-traces">2.1.6</a>, <span class="citation" data-cites="Sutton1998">Sutton and Barto (<a href="References.html#ref-Sutton1998" role="doc-biblioref">1998</a>)</span>).</p>
<p>A2C has an actor-critic architecture (Fig. <a href="#fig:a3c">22</a>):</p>
<ul>
<li>The actor outputs the policy <span class="math inline">\(\pi_\theta\)</span> for a state <span class="math inline">\(s\)</span>, i.e. a vector of probabilities for each action.</li>
<li>The critic outputs the value <span class="math inline">\(V_\varphi(s)\)</span> of a state <span class="math inline">\(s\)</span>.</li>
</ul>
<figure>
<img src="img/a3c.png" id="fig:a3c" style="width:70.0%" alt="" /><figcaption>Figure 22: Advantage actor-critic architecture.</figcaption>
</figure>
<p>Having a computable formula for the policy gradient, the algorithm is rather simple:</p>
<ol type="1">
<li><p>Acquire a batch of transitions <span class="math inline">\((s, a, r, s&#39;)\)</span> using the current policy <span class="math inline">\(\pi_\theta\)</span> (either a finite episode or a truncated one).</p></li>
<li><p>For each state encountered, compute the discounted sum of the next <span class="math inline">\(n\)</span> rewards <span class="math inline">\(\sum_{k=0}^{n} \gamma^{k} \, r_{t+k+1}\)</span> and use the critic to estimate the value of the state encountered <span class="math inline">\(n\)</span> steps later <span class="math inline">\(V_\varphi(s_{t+n+1})\)</span>.</p></li>
</ol>
<p><span class="math display">\[
    R_t = \sum_{k=0}^{n-1} \gamma^{k} \, r_{t+k+1} + \gamma^n \, V_\varphi(s_{t+n+1})
\]</span></p>
<ol start="3" type="1">
<li>Update the actor using Eq. <a href="#eq:a2c">11</a>.</li>
</ol>
<p><span class="math display">\[
    \nabla_\theta J(\theta) =  \sum_t \nabla_\theta \log \pi_\theta(s_t, a_t) \, (R_t - V_\varphi(s_t))
\]</span></p>
<ol start="4" type="1">
<li>Update the critic to minimize the TD error between the estimated value of a state and its true value.</li>
</ol>
<p><span class="math display">\[
    \mathcal{L}(\varphi) = \sum_t (R_t - V_\varphi(s_t))^2
\]</span></p>
<ol start="5" type="1">
<li>Repeat.</li>
</ol>
<p>This is not very different in essence from REINFORCE (sample transitions, compute the return, update the policy), apart from the facts that episodes do not need to be finite and that a critic has to be learned in parallel. A more detailed pseudo-algorithm for a single A2C learner is the following:</p>
<hr />
<ul>
<li>Initialize the actor <span class="math inline">\(\pi_\theta\)</span> and the critic <span class="math inline">\(V_\varphi\)</span> with random weights.</li>
<li>Observe the initial state <span class="math inline">\(s_0\)</span>.</li>
<li>for <span class="math inline">\(t \in [0, T_\text{total}]\)</span>:
<ul>
<li><p>Initialize empty episode minibatch.</p></li>
<li><p>for <span class="math inline">\(k \in [0, n]\)</span>: # Sample episode</p>
<ul>
<li>Select a action <span class="math inline">\(a_k\)</span> using the actor <span class="math inline">\(\pi_\theta\)</span>.</li>
<li>Perform the action <span class="math inline">\(a_k\)</span> and observe the next state <span class="math inline">\(s_{k+1}\)</span> and the reward <span class="math inline">\(r_{k+1}\)</span>.</li>
<li>Store <span class="math inline">\((s_k, a_k, r_{k+1})\)</span> in the episode minibatch.</li>
</ul></li>
<li><p>if <span class="math inline">\(s_n\)</span> is not terminal: set <span class="math inline">\(R = V_\varphi(s_n)\)</span> with the critic, else <span class="math inline">\(R=0\)</span>.</p></li>
<li><p>Reset gradient <span class="math inline">\(d\theta\)</span> and <span class="math inline">\(d\varphi\)</span> to 0.</p></li>
<li><p>for <span class="math inline">\(k \in [n-1, 0]\)</span>: # Backwards iteration over the episode</p>
<ul>
<li>Update the discounted sum of rewards <span class="math inline">\(R = r_k + \gamma \, R\)</span></li>
<li>Accumulate the policy gradient using the critic: <span class="math display">\[
  d\theta \leftarrow d\theta + \nabla_\theta \log \pi_\theta(s_k, a_k) \, (R - V_\varphi(s_k))
  \]</span></li>
<li>Accumulate the critic gradient: <span class="math display">\[
  d\varphi \leftarrow d\varphi + \nabla_\varphi (R - V_\varphi(s_k))^2
  \]</span></li>
</ul></li>
<li><p>Update the actor and the critic with the accumulated gradients using gradient descent or similar: <span class="math display">\[
  \theta \leftarrow \theta + \eta \, d\theta \qquad \varphi \leftarrow \varphi + \eta \, d\varphi
  \]</span></p></li>
</ul></li>
</ul>
<hr />
<p>Note that not all states are updated with the same horizon <span class="math inline">\(n\)</span>: the last action encountered in the sampled episode will only use the last reward and the value of the final state (TD learning), while the very first action will use the <span class="math inline">\(n\)</span> accumulated rewards. In practice it does not really matter, but the choice of the discount rate <span class="math inline">\(\gamma\)</span> will have a significant influence on the results.</p>
<p>As many actor-critic methods, A2C performs online learning: a couple of transitions are explored using the current policy, which is immediately updated. As for value-based networks (e.g. DQN, Section <a href="./Valuebased.html#sec:deep-q-network-dqn">3.2</a>), the underlying NN will be affected by the correlated inputs and outputs: a single batch contains similar states and action (e.g. consecutive frames of a video game). The solution retained in A2C and A3C does not depend on an <em>experience replay memory</em> as DQN, but rather on the use of <strong>multiple parallel actors and learners</strong>.</p>
<p>The idea is depicted on Fig. <a href="#fig:a3carchi">23</a> (actually for A3C, but works with A2C). The actor and critic are stored in a global network. Multiple instances of the environment are created in different parallel threads (the <strong>workers</strong> or <strong>actor-learners</strong>). At the beginning of an episode, each worker receives a copy of the actor and critic weights from the global network. Each worker samples an episode (starting from different initial states, so the episodes are uncorrelated), computes the accumulated gradients and sends them back to the global network. The global networks merges the gradients and uses them to update the parameters of the policy and critic networks. The new parameters are send to each worker again, until it converges.</p>
<hr />
<ul>
<li>Initialize the actor <span class="math inline">\(\pi_\theta\)</span> and the critic <span class="math inline">\(V_\varphi\)</span> in the global network.</li>
<li>repeat:
<ul>
<li>for each worker <span class="math inline">\(i\)</span> in <strong>parallel</strong>:
<ul>
<li>Get a copy of the global actor <span class="math inline">\(\pi_\theta\)</span> and critic <span class="math inline">\(V_\varphi\)</span>.</li>
<li>Sample an episode of <span class="math inline">\(n\)</span> steps.</li>
<li>Return the accumulated gradients <span class="math inline">\(d\theta_i\)</span> and <span class="math inline">\(d\varphi_i\)</span>.</li>
</ul></li>
<li>Wait for all workers to terminate.</li>
<li>Merge all accumulated gradients into <span class="math inline">\(d\theta\)</span> and <span class="math inline">\(d\varphi\)</span>.</li>
<li>Update the global actor and critic networks.</li>
</ul></li>
</ul>
<hr />
<p>This solves the problem of correlated inputs and outputs, as each worker explores different regions of the environment (one can set different initial states in each worker, vary the exploration rate, etc), so the final batch of transitions used for training the global networks is much less correlated. The only drawback of this approach is that it has to be possible to explore multiple environments in parallel. This is easy to achieve in simulated environments (e.g. video games) but much harder in real-world systems like robots. A brute-force solution for robotics is simply to buy enough robots and let them learn in parallel <span class="citation" data-cites="Gu2017">(Gu et al., <a href="References.html#ref-Gu2017" role="doc-biblioref">2017</a>)</span>.</p>
<h3 id="sec:asynchronous-advantage-actor-critic-a3c"><span class="header-section-number">4.2.2</span> Asynchronous Advantage Actor-Critic (A3C)</h3>
<figure>
<img src="img/A3C_architecture.png" id="fig:a3carchi" style="width:70.0%" alt="" /><figcaption>Figure 23: Architecture of A3C. A master network interacts asynchronously with several workers, each having a copy of the network and interacting with a separate environment. At the end of an episode, the accumulated gradients are sent back to the master network, and a new value of the parameters is sent to the workers. Taken from <a href="https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2" class="uri">https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2</a>.</figcaption>
</figure>
<p>Asynchronous Advantage Actor-Critic <span class="citation" data-cites="Mnih2016">(A3C, Mnih et al., <a href="References.html#ref-Mnih2016" role="doc-biblioref">2016</a>)</span> extends the approach of A2C by removing the need of synchronization between the workers at the end of each episode before applying the gradients. The rationale behind this is that each worker may need different times to complete its task, so they need to be synchronized. Some workers might then be idle most of the time, what is a waste of resources. Gradient merging and parameter updates are sequential operations, so no significant speedup is to be expected even if one increases the number of workers.</p>
<p>The solution retained in A3C is to simply skip the synchronization step: each worker reads and writes the network parameters whenever it wants. Without synchronization barriers, there is of course a risk that one worker tries to read the network parameters while another writes them: the obtained parameters would be a mix of two different networks. Surprisingly, it does not matter: if the learning rate is small enough, there is anyway not a big difference between two successive versions of the network parameters. This kind of “dirty” parameter sharing is called <em>HogWild!</em> updating <span class="citation" data-cites="Niu2011">(Niu et al., <a href="References.html#ref-Niu2011" role="doc-biblioref">2011</a>)</span> and has been proven to work under certain conditions which are met here.</p>
<p>The resulting A3C pseudocode is summarized here:</p>
<hr />
<ul>
<li><p>Initialize the actor <span class="math inline">\(\pi_\theta\)</span> and the critic <span class="math inline">\(V_\varphi\)</span> in the global network.</p></li>
<li><p>for each worker <span class="math inline">\(i\)</span> in <strong>parallel</strong>:</p>
<ul>
<li>repeat:
<ul>
<li>Get a copy of the global actor <span class="math inline">\(\pi_\theta\)</span> and critic <span class="math inline">\(V_\varphi\)</span>.</li>
<li>Sample an episode of <span class="math inline">\(n\)</span> steps.</li>
<li>Compute the accumulated gradients <span class="math inline">\(d\theta_i\)</span> and <span class="math inline">\(d\varphi_i\)</span>.</li>
<li>Update the global actor and critic networks asynchronously (HogWild!).</li>
</ul></li>
</ul></li>
</ul>
<hr />
<p>The workers are fully independent: their only communication is through the <strong>asynchronous</strong> updating of the global networks. This can lead to very efficient parallel implementations: in the original A3C paper <span class="citation" data-cites="Mnih2016">(Mnih et al., <a href="References.html#ref-Mnih2016" role="doc-biblioref">2016</a>)</span>, they solved the same Atari games than DQN using 16 CPU cores instead of a powerful GPU, while achieving a better performance in less training time (1 day instead of 8). The speedup is almost linear: the more workers, the faster the computations, the better the performance (as the policy updates are less correlated).</p>
<h4 id="sec:entropy-regularization" class="unnumbered">Entropy regularization</h4>
<p>An interesting addition in A3C is the way they enforce exploration during learning. In actor-critic methods, exploration classically relies on the fact that the learned policies are stochastic (<strong>on-policy</strong>): <span class="math inline">\(\pi(s, a)\)</span> describes the probability of taking the action <span class="math inline">\(a\)</span> in the state <span class="math inline">\(s\)</span>. In discrete action spaces, the output of the actor is usually a softmax layer, ensuring that all actions get a non-zero probability of being selected during training. In continuous action spaces, the executed action is sampled from the output probability distribution. However, this is often not sufficient and hard to control.</p>
<p>In A3C, the authors added an <strong>entropy regularization</strong> term <span class="citation" data-cites="Williams1991">(Williams and Peng, <a href="References.html#ref-Williams1991" role="doc-biblioref">1991</a>,Section <a href="./EntropyRL.html#sec:maximum-entropy-rl" role="doc-biblioref">4.6</a>)</span> to the policy gradient update:</p>
<p><span id="eq:a3c_entropy"><span class="math display">\[
    \nabla_\theta J(\theta) =  \mathbb{E}_{s_t \sim \rho^\pi, a_t \sim \pi_\theta}[\nabla_\theta \log \pi_\theta(s_t, a_t) \, ( R_t - V_\varphi(s_t)) + \beta \, \nabla_\theta H(\pi_\theta(s_t))]
\qquad(12)\]</span></span></p>
<p>For discrete actions, the entropy of the policy for a state <span class="math inline">\(s_t\)</span> is simple to compute: <span class="math inline">\(H(\pi_\theta(s_t)) = - \sum_a \pi_\theta(s_t, a) \, \log \pi_\theta(s_t, a)\)</span>. For continuous actions, replace the sum with an integral. It measures the “randomness” of the policy: if the policy is fully deterministic (the same action is systematically selected), the entropy is zero as it carries no information. If the policy is completely random, the entropy is maximal. Maximizing the entropy at the same time as the returns improves exploration by forcing the policy to be as non-deterministic as possible.</p>
<p>The parameter <span class="math inline">\(\beta\)</span> controls the level of regularization: we do not want the entropy to dominate either, as a purely random policy does not bring much reward. If <span class="math inline">\(\beta\)</span> is chosen too low, the entropy won’t play a significant role in the optimization and we may obtain a suboptimal deterministic policy early during training as there was not enough exploration. If <span class="math inline">\(\beta\)</span> is too high, the policy will be random. Entropy regularization adds yet another hyperparameter to the problem, but can be really useful for convergence when adequately chosen.</p>
<h4 id="sec:comparison-between-a3c-and-dqn" class="unnumbered">Comparison between A3C and DQN</h4>
<ol type="1">
<li>DQN uses an experience replay memory to solve the correlation of inputs/outputs problem, while A3C uses parallel actor-learners. If multiple copies of the environment are available, A3C should be preferred because the ERM slows down learning (very old transitions are still used for learning) and requires a lot of memory.</li>
<li>A3C is on-policy: the learned policy must be used to explore the environment. DQN is off-policy: a behavior policy can be used for exploration, allowing to guide externally which regions of the state-action space should be explored. Off-policy are often preferred when expert knowledge is available.</li>
<li>DQN has to use target networks to fight the non-stationarity of the Q-values. A3C uses state-values and advantages, which are much more stable over time than Q-values, so there is no need for target networks.</li>
<li>A3C can deal with continuous action spaces easily, as it uses a parameterized policy. DQN has to be strongly modified to deal with this problem.</li>
<li>Both can deal with POMDP by using LSTMs in the actor network: A3C <span class="citation" data-cites="Mnih2016 Mirowski2016">(Mirowski et al., <a href="References.html#ref-Mirowski2016" role="doc-biblioref">2016</a>; Mnih et al., <a href="References.html#ref-Mnih2016" role="doc-biblioref">2016</a>)</span>, DQN <span class="citation" data-cites="Hausknecht2015">(Hausknecht and Stone, <a href="References.html#ref-Hausknecht2015" role="doc-biblioref">2015</a>, see Section <a href="./Valuebased.html#sec:deep-recurrent-q-learning-drqn" role="doc-biblioref">3.7</a>)</span>.</li>
</ol>
<h3 id="sec:generalized-advantage-estimation-gae"><span class="header-section-number">4.2.3</span> Generalized Advantage Estimation (GAE)</h3>
<p>The different versions of the policy gradient seen so far take the form:</p>
<p><span class="math display">\[
    \nabla_\theta J(\theta) =  \mathbb{E}_{s_t \sim \rho^\pi, a_t \sim \pi_\theta}[\nabla_\theta \log \pi_\theta (s_t, a_t) \, \psi_t ]
\]</span></p>
<p>where:</p>
<ul>
<li><p><span class="math inline">\(\psi_t = R_t\)</span> is the <em>REINFORCE</em> algorithm (MC sampling).</p></li>
<li><p><span class="math inline">\(\psi_t = R_t - b\)</span> is the <em>REINFORCE with baseline</em> algorithm.</p></li>
<li><p><span class="math inline">\(\psi_t = Q^\pi(s_t, a_t)\)</span> is the <em>policy gradient theorem</em>.</p></li>
<li><p><span class="math inline">\(\psi_t = A^\pi(s_t, a_t)\)</span> is the <em>advantage actor critic</em>.</p></li>
<li><p><span class="math inline">\(\psi_t = r_{t+1} + \gamma \, V^\pi(s_{t+1}) - V^\pi(s_t)\)</span> is the <em>TD actor critic</em>.</p></li>
<li><p><span class="math inline">\(\psi_t = \sum_{k=0}^{n-1} \gamma^{k} \, r_{t+k+1} + \gamma^n \, V^\pi(s_{t+n+1}) - V^\pi(s_t)\)</span> is the <em>n-step algorithm</em> (A2C).</p></li>
</ul>
<p>Generally speaking:</p>
<ul>
<li>the more <span class="math inline">\(\psi_t\)</span> relies on real rewards (e.g. <span class="math inline">\(R_t\)</span>), the more the gradient will be correct on average (small bias), but the more it will vary (high variance). This increases the sample complexity: we need to average more samples to correctly estimate the gradient.</li>
<li>the more <span class="math inline">\(\psi_t\)</span> relies on estimations (e.g. the TD error), the more stable the gradient (small variance), but the more incorrect it is (high bias). This can lead to suboptimal policies, i.e. local optima of the objective function.</li>
</ul>
<p>This is the classical bias/variance trade-off in machine learning (see Section <a href="./PolicyGradient.html#sec:reducing-the-variance">4.1.2</a>). The n-step algorithm used in A2C is an attempt to mitigate between these extrema. <span class="citation" data-cites="Schulman2015a">Schulman et al. (<a href="References.html#ref-Schulman2015a" role="doc-biblioref">2015</a><a href="References.html#ref-Schulman2015a" role="doc-biblioref">b</a>)</span> proposed the <strong>Generalized Advantage Estimate</strong> (GAE) to further control the bias/variance trade-off.</p>
<p>Let’s define the n-step advantage:</p>
<p><span class="math display">\[
    A^{n}_t = \sum_{k=0}^{n-1} \gamma^{k} \, r_{t+k+1} + \gamma^n \, V^\pi(s_{t+n+1}) - V^\pi(s_t)
\]</span></p>
<p>It is easy to show recursively that it depends on the TD error <span class="math inline">\(\delta_t = r_{t+1} + \gamma \, V^\pi(s_{t+1}) - V^\pi(s_t)\)</span> of the <span class="math inline">\(n\)</span> next steps:</p>
<p><span class="math display">\[
    A^{n}_t = \sum_{l=0}^{n-1} \gamma^l \, \delta_{t+l}
\]</span></p>
<p>In other words, the prediction error over <span class="math inline">\(n\)</span> steps is the (discounted) sum of the prediction errors between two successive steps. Now, what is the optimal value of <span class="math inline">\(n\)</span>? GAE decides not to choose and to simply average all n-step advantages and to weight them with a discount parameter <span class="math inline">\(\lambda\)</span>. This defines the <strong>Generalized Advantage Estimator</strong> <span class="math inline">\(A^{\text{GAE}(\gamma, \lambda)}_t\)</span>:</p>
<p><span class="math display">\[
    A^{\text{GAE}(\gamma, \lambda)}_t = (1-\lambda) \, \sum_{l=0}^\infty \lambda^l A^l_t = \sum_{l=0}^\infty (\gamma \lambda)^l \delta_{t+l}
\]</span></p>
<p>The GAE is the discounted sum of all n-step advantages. When <span class="math inline">\(\lambda=0\)</span>, we have <span class="math inline">\(A^{\text{GAE}(\gamma, 0)}_t = A^{0}_t = \delta_t\)</span>, i.e. the TD advantage (high bias, low variance). When <span class="math inline">\(\lambda=1\)</span>, we have (at the limit) <span class="math inline">\(A^{\text{GAE}(\gamma, 1)}_t = R_t\)</span>, i.e. the MC advantage (low bias, high variance). Choosing the right value of <span class="math inline">\(\lambda\)</span> between 0 and 1 allows to control the bias/variance trade-off.</p>
<p><span class="math inline">\(\gamma\)</span> and <span class="math inline">\(\lambda\)</span> play different roles in GAE: <span class="math inline">\(\gamma\)</span> determines the scale or horizon of the value functions: how much future rewards rewards are to be taken into account. The higher <span class="math inline">\(\gamma &lt;1\)</span>, the smaller the bias, but the higher the variance. Empirically, <span class="citation" data-cites="Schulman2015a">Schulman et al. (<a href="References.html#ref-Schulman2015a" role="doc-biblioref">2015</a><a href="References.html#ref-Schulman2015a" role="doc-biblioref">b</a>)</span> found that small <span class="math inline">\(\lambda\)</span> values introduce less bias than <span class="math inline">\(\gamma\)</span>, so <span class="math inline">\(\lambda\)</span> can be chosen smaller than <span class="math inline">\(\gamma\)</span> (which is typically 0.99).</p>
<p>The policy gradient for Generalized Advantage Estimation is therefore:</p>
<p><span class="math display">\[
    \nabla_\theta J(\theta) =  \mathbb{E}_{s_t \sim \rho^\pi, a_t \sim \pi_\theta}[\nabla_\theta \log \pi_\theta (s_t, a_t) \, \sum_{l=0}^\infty (\gamma \lambda)^l \delta_{t+l} ]
\]</span></p>
<p>Note that <span class="citation" data-cites="Schulman2015a">Schulman et al. (<a href="References.html#ref-Schulman2015a" role="doc-biblioref">2015</a><a href="References.html#ref-Schulman2015a" role="doc-biblioref">b</a>)</span> additionally use <em>trust region optimization</em> to stabilize learning and further reduce the bias (see Section <a href="./NaturalGradient.html#sec:trust-region-policy-optimization-trpo">4.5.3</a>), for now just consider it is a better optimization method than gradient descent. The GAE algorithm is summarized here:</p>
<hr />
<ul>
<li>Initialize the actor <span class="math inline">\(\pi_\theta\)</span> and the critic <span class="math inline">\(V_\varphi\)</span> with random weights.</li>
<li>for <span class="math inline">\(t \in [0, T_\text{total}]\)</span>:
<ul>
<li><p>Initialize empty minibatch.</p></li>
<li><p>for <span class="math inline">\(k \in [0, n]\)</span>:</p>
<ul>
<li>Select a action <span class="math inline">\(a_k\)</span> using the actor <span class="math inline">\(\pi_\theta\)</span>.</li>
<li>Perform the action <span class="math inline">\(a_k\)</span> and observe the next state <span class="math inline">\(s_{k+1}\)</span> and the reward <span class="math inline">\(r_{k+1}\)</span>.</li>
<li>Store <span class="math inline">\((s_k, a_k, r_{k+1})\)</span> in the minibatch.</li>
</ul></li>
<li><p>for <span class="math inline">\(k \in [0, n]\)</span>:</p>
<ul>
<li>Compute the TD error <span class="math inline">\(\delta_k = r_{k+1} + \gamma \, V_\varphi(s_{k+1}) - V_\varphi(s_k)\)</span></li>
</ul></li>
<li><p>for <span class="math inline">\(k \in [0, n]\)</span>:</p>
<ul>
<li>Compute the GAE advantage <span class="math inline">\(A^{\text{GAE}(\gamma, \lambda)}_k = \sum_{l=0}^\infty (\gamma \lambda)^l \delta_{k+l}\)</span></li>
</ul></li>
<li><p>Update the actor using the GAE advantage and natural gradients (TRPO).</p></li>
<li><p>Update the critic using natural gradients (TRPO)</p></li>
</ul></li>
</ul>
<hr />
<h3 id="sec:stochastic-actor-critic-for-continuous-action-spaces"><span class="header-section-number">4.2.4</span> Stochastic actor-critic for continuous action spaces</h3>
<p>The actor-critic method presented above use <strong>stochastic policies</strong> <span class="math inline">\(\pi_\theta(s, a)\)</span> assigning parameterized probabilities of being selecting to each <span class="math inline">\((s, a)\)</span> pair.</p>
<ul>
<li><p>When the action space is discrete, the output layer of the actor is simply a <em>softmax</em> layer with as many neurons as possible actions in each state, making sure the probabilities sum to one. It is then straightforward to sample an action from this layer.</p></li>
<li><p>When the action space is continuous (e.g. the different joints of a robotic arm), one has to make an assumption on the underlying distribution. The actor learns the parameters of the distribution (for example the mean and variance of a Gaussian distribution) and the executed action is simply sampled from the parameterized distribution.</p></li>
</ul>
<p>The most used distribution is the Gaussian distribution, leading to <strong>Gaussian policies</strong>. In this case, the output of the actor is a mean vector <span class="math inline">\(\mu_\theta(s)\)</span> and possibly a variance vector <span class="math inline">\(\sigma_\theta(s)\)</span>. The policy is then simply defined as:</p>
<p><span class="math display">\[
    \pi_\theta(s, a) = \frac{1}{\sqrt{2\pi\sigma_\theta(s)}} \, \exp -\frac{(a - \mu_\theta(s))^2}{2\sigma_\theta(s)^2}
\]</span></p>
<p>In order to use backpropagation on the policy gradient (i.e. getting an analytical form of the score function <span class="math inline">\(\nabla_\theta \log \pi_\theta (s, a)\)</span>), one can use the <em>re-parameterization trick</em> <span class="citation" data-cites="Heess2015">(Heess et al., <a href="References.html#ref-Heess2015" role="doc-biblioref">2015</a>)</span> by rewriting the policy as:</p>
<p><span class="math display">\[
    a = \mu_\theta(s) + \sigma_\theta(s) \times \xi \qquad \text{where} \qquad \xi \sim \mathcal{N}(0,1)
\]</span></p>
<p>To select an action, we only need to sample <span class="math inline">\(\xi\)</span> from the unit normal distribution, multiply it by the standard deviation and add the mean. To compute the score function, we use the following partial derivatives:</p>
<p><span class="math display">\[
    \nabla_\mu \log \pi_\theta (s, a) = \frac{a - \mu_\theta(s)}{\sigma_\theta(s)^2} \qquad \nabla_\sigma \log \pi_\theta (s, a) = \frac{(a - \mu_\theta(s))^2}{\sigma_\theta(s)^3} - \frac{1}{\sigma_\theta(s)}
\]</span></p>
<p>and use the chain rule to obtain the score function. The <em>re-parameterization trick</em> is a cool trick to apply backpropagation on stochastic problems: it is for example used in the variational auto-encoders <span class="citation" data-cites="Kingma2013">(VAR; Kingma and Welling, <a href="References.html#ref-Kingma2013" role="doc-biblioref">2013</a>)</span>.</p>
<p>Depending on the problem, one could use: 1) a fixed <span class="math inline">\(\sigma\)</span> for the whole action space, 2) a fixed <span class="math inline">\(\sigma\)</span> per DoF, 3) a learnable <span class="math inline">\(\sigma\)</span> per DoF (assuming all action dimensions to be mutually independent) or even 4) a covariance matrix <span class="math inline">\(\Sigma\)</span> when the action dimensions are dependent.</p>
<p>One limitation of Gaussian policies is that their support is infinite: even with a small variance, samples actions can deviate a lot (albeit rarely) from the mean. This is particularly a problem when action must have a limited range: the torque of an effector, the linear or angular speed of a car, etc. Clipping the sampled action to minimal and maximal values introduces a bias which can impair learning. <span class="citation" data-cites="Chou2017">Chou et al. (<a href="References.html#ref-Chou2017" role="doc-biblioref">2017</a>)</span> proposed to use <strong>beta-distributions</strong> instead of Gaussian ones in the actor. Sampled values have a <span class="math inline">\([0,1]\)</span> support, which can rescaled to <span class="math inline">\([v_\text{min},v_\text{max}]\)</span> easily. They show that beta policies have less bias than Gaussian policies in most continuous problems.</p>

<br>
<div class="arrows">
<a href="PolicyGradient.html" class="previous">&laquo; Previous</a>
<a href="ImportanceSampling.html" class="next">Next &raquo;</a>
</div>


</article>
</body>
</html>
