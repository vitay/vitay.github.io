<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <meta name="author" content="Julien Vitay">
  <title>Deep Reinforcement Learning</title>


  <link rel="stylesheet" href="assets/syntax.css">
  <link rel="stylesheet" href="assets/menu.css">
  <link rel="stylesheet" href="assets/github.css">
  <script src="assets/menu.js" type="text/javascript"></script>
  <script src="/usr/share/mathjax2/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->

</head>

<body class="markdown-body">

<div class="navbar">
  <div class="dropdown" id="dropdown">
    <button class="dropbtn">
      <div class="dropicon-first"></div>
      <div class="dropicon"></div>
      <div class="dropicon"></div>
      <i class="fa fa-caret-down"></i>
    </button> 
    <span class="navbar-title">Deep Reinforcement Learning - Julien Vitay</span>
    <div class="dropdown-content">

<ul>
<li><a href="./Introduction.html#sec:introduction"><span class="toc-section-number">1</span> Introduction</a></li>
<li><a href="./BasicRL.html#sec:basics"><span class="toc-section-number">2</span> Basics</a><ul>
<li><a href="./BasicRL.html#sec:reinforcement-learning-and-markov-decision-process"><span class="toc-section-number">2.1</span> Reinforcement learning and Markov Decision Process</a><ul>
<li><a href="./BasicRL.html#sec:policy-and-value-functions"><span class="toc-section-number">2.1.1</span> Policy and value functions</a></li>
<li><a href="./BasicRL.html#sec:bellman-equations"><span class="toc-section-number">2.1.2</span> Bellman equations</a></li>
<li><a href="./BasicRL.html#sec:dynamic-programming"><span class="toc-section-number">2.1.3</span> Dynamic programming</a></li>
<li><a href="./BasicRL.html#sec:monte-carlo-sampling"><span class="toc-section-number">2.1.4</span> Monte-Carlo sampling</a></li>
<li><a href="./BasicRL.html#sec:temporal-difference"><span class="toc-section-number">2.1.5</span> Temporal Difference</a></li>
<li><a href="./BasicRL.html#sec:eligibility-traces"><span class="toc-section-number">2.1.6</span> Eligibility traces</a></li>
<li><a href="./BasicRL.html#sec:actor-critic-architectures"><span class="toc-section-number">2.1.7</span> Actor-critic architectures</a></li>
<li><a href="./BasicRL.html#sec:function-approximation"><span class="toc-section-number">2.1.8</span> Function approximation</a></li>
</ul></li>
<li><a href="./DeepLearning.html#sec:deep-learning"><span class="toc-section-number">2.2</span> Deep learning</a><ul>
<li><a href="./DeepLearning.html#sec:deep-neural-networks"><span class="toc-section-number">2.2.1</span> Deep neural networks</a></li>
<li><a href="./DeepLearning.html#sec:convolutional-networks"><span class="toc-section-number">2.2.2</span> Convolutional networks</a></li>
<li><a href="./DeepLearning.html#sec:recurrent-neural-networks"><span class="toc-section-number">2.2.3</span> Recurrent neural networks</a></li>
</ul></li>
</ul></li>
<li><a href="./Valuebased.html#sec:value-based-methods"><span class="toc-section-number">3</span> Value-based methods</a><ul>
<li><a href="./Valuebased.html#sec:limitations-of-deep-neural-networks-for-function-approximation"><span class="toc-section-number">3.1</span> Limitations of deep neural networks for function approximation</a></li>
<li><a href="./Valuebased.html#sec:deep-q-network-dqn"><span class="toc-section-number">3.2</span> Deep Q-Network (DQN)</a></li>
<li><a href="./Valuebased.html#sec:double-dqn"><span class="toc-section-number">3.3</span> Double DQN</a></li>
<li><a href="./Valuebased.html#sec:prioritized-experience-replay"><span class="toc-section-number">3.4</span> Prioritized experience replay</a></li>
<li><a href="./Valuebased.html#sec:duelling-network"><span class="toc-section-number">3.5</span> Duelling network</a></li>
<li><a href="./Valuebased.html#sec:distributed-dqn-gorila"><span class="toc-section-number">3.6</span> Distributed DQN (GORILA)</a></li>
<li><a href="./Valuebased.html#sec:deep-recurrent-q-learning-drqn"><span class="toc-section-number">3.7</span> Deep Recurrent Q-learning (DRQN)</a></li>
<li><a href="./Valuebased.html#sec:other-variants-of-dqn"><span class="toc-section-number">3.8</span> Other variants of DQN</a></li>
</ul></li>
<li><a href="./PolicyGradient.html#sec:policy-gradient-methods"><span class="toc-section-number">4</span> Policy Gradient methods</a><ul>
<li><a href="./PolicyGradient.html#sec:reinforce"><span class="toc-section-number">4.1</span> REINFORCE</a><ul>
<li><a href="./PolicyGradient.html#sec:estimating-the-policy-gradient"><span class="toc-section-number">4.1.1</span> Estimating the policy gradient</a></li>
<li><a href="./PolicyGradient.html#sec:reducing-the-variance"><span class="toc-section-number">4.1.2</span> Reducing the variance</a></li>
<li><a href="./PolicyGradient.html#sec:policy-gradient-theorem"><span class="toc-section-number">4.1.3</span> Policy Gradient theorem</a></li>
</ul></li>
<li><a href="./ActorCritic.html#sec:advantage-actor-critic-methods"><span class="toc-section-number">4.2</span> Advantage Actor-Critic methods</a><ul>
<li><a href="./ActorCritic.html#sec:advantage-actor-critic-a2c"><span class="toc-section-number">4.2.1</span> Advantage Actor-Critic (A2C)</a></li>
<li><a href="./ActorCritic.html#sec:asynchronous-advantage-actor-critic-a3c"><span class="toc-section-number">4.2.2</span> Asynchronous Advantage Actor-Critic (A3C)</a></li>
<li><a href="./ActorCritic.html#sec:generalized-advantage-estimation-gae"><span class="toc-section-number">4.2.3</span> Generalized Advantage Estimation (GAE)</a></li>
<li><a href="./ActorCritic.html#sec:stochastic-actor-critic-for-continuous-action-spaces"><span class="toc-section-number">4.2.4</span> Stochastic actor-critic for continuous action spaces</a></li>
</ul></li>
<li><a href="./ImportanceSampling.html#sec:off-policy-actor-critic"><span class="toc-section-number">4.3</span> Off-policy Actor-Critic</a><ul>
<li><a href="./ImportanceSampling.html#sec:importance-sampling"><span class="toc-section-number">4.3.1</span> Importance sampling</a></li>
<li><a href="./ImportanceSampling.html#sec:linear-off-policy-actor-critic-off-pac"><span class="toc-section-number">4.3.2</span> Linear Off-Policy Actor-Critic (Off-PAC)</a></li>
<li><a href="./ImportanceSampling.html#sec:retrace"><span class="toc-section-number">4.3.3</span> Retrace</a></li>
<li><a href="./ImportanceSampling.html#sec:self-imitation-learning-sil"><span class="toc-section-number">4.3.4</span> Self-Imitation Learning (SIL)</a></li>
</ul></li>
<li><a href="./DPG.html#sec:deterministic-policy-gradient-dpg"><span class="toc-section-number">4.4</span> Deterministic Policy Gradient (DPG)</a><ul>
<li><a href="./DPG.html#sec:deterministic-policy-gradient-theorem"><span class="toc-section-number">4.4.1</span> Deterministic policy gradient theorem</a></li>
<li><a href="./DPG.html#sec:deep-deterministic-policy-gradient-ddpg"><span class="toc-section-number">4.4.2</span> Deep Deterministic Policy Gradient (DDPG)</a></li>
<li><a href="./DPG.html#sec:distributed-distributional-ddpg-d4pg"><span class="toc-section-number">4.4.3</span> Distributed Distributional DDPG (D4PG)</a></li>
</ul></li>
<li><a href="./NaturalGradient.html#sec:natural-gradients"><span class="toc-section-number">4.5</span> Natural Gradients</a><ul>
<li><a href="./NaturalGradient.html#sec:principle-of-natural-gradients"><span class="toc-section-number">4.5.1</span> Principle of natural gradients</a></li>
<li><a href="./NaturalGradient.html#sec:natural-policy-gradient-and-natural-actor-critic-nac"><span class="toc-section-number">4.5.2</span> Natural policy gradient and Natural Actor Critic (NAC)</a></li>
<li><a href="./NaturalGradient.html#sec:trust-region-policy-optimization-trpo"><span class="toc-section-number">4.5.3</span> Trust Region Policy Optimization (TRPO)</a></li>
<li><a href="./NaturalGradient.html#sec:proximal-policy-optimization-ppo"><span class="toc-section-number">4.5.4</span> Proximal Policy Optimization (PPO)</a></li>
<li><a href="./NaturalGradient.html#sec:actor-critic-with-experience-replay-acer"><span class="toc-section-number">4.5.5</span> Actor-Critic with Experience Replay (ACER)</a></li>
</ul></li>
<li><a href="./EntropyRL.html#sec:maximum-entropy-rl"><span class="toc-section-number">4.6</span> Maximum Entropy RL</a><ul>
<li><a href="#sec:entropy-regularization-1"><span class="toc-section-number">4.6.1</span> Entropy regularization</a></li>
<li><a href="./EntropyRL.html#sec:soft-q-learning"><span class="toc-section-number">4.6.2</span> Soft Q-learning</a></li>
<li><a href="./EntropyRL.html#sec:soft-actor-critic-sac"><span class="toc-section-number">4.6.3</span> Soft Actor-Critic (SAC)</a></li>
</ul></li>
<li><a href="./DistributionalRL.html#sec:distributional-learning"><span class="toc-section-number">4.7</span> Distributional learning</a><ul>
<li><a href="./DistributionalRL.html#sec:categorical-dqn"><span class="toc-section-number">4.7.1</span> Categorical DQN</a></li>
<li><a href="./DistributionalRL.html#sec:the-reactor"><span class="toc-section-number">4.7.2</span> The Reactor</a></li>
</ul></li>
<li><a href="./OtherPolicyGradient.html#sec:miscellaneous-model-free-algorithm"><span class="toc-section-number">4.8</span> Miscellaneous model-free algorithm</a><ul>
<li><a href="./OtherPolicyGradient.html#sec:stochastic-value-gradient-svg"><span class="toc-section-number">4.8.1</span> Stochastic Value Gradient (SVG)</a></li>
<li><a href="./OtherPolicyGradient.html#sec:q-prop"><span class="toc-section-number">4.8.2</span> Q-Prop</a></li>
<li><a href="./OtherPolicyGradient.html#sec:normalized-advantage-function-naf"><span class="toc-section-number">4.8.3</span> Normalized Advantage Function (NAF)</a></li>
<li><a href="./OtherPolicyGradient.html#sec:fictitious-self-play-fsp"><span class="toc-section-number">4.8.4</span> Fictitious Self-Play (FSP)</a></li>
</ul></li>
<li><a href="./OtherPolicyGradient.html#sec:comparison-between-value-based-and-policy-gradient-methods"><span class="toc-section-number">4.9</span> Comparison between value-based and policy gradient methods</a></li>
<li><a href="./OtherPolicyGradient.html#sec:gradient-free-policy-search"><span class="toc-section-number">4.10</span> Gradient-free policy search</a><ul>
<li><a href="./OtherPolicyGradient.html#sec:cross-entropy-method-cem"><span class="toc-section-number">4.10.1</span> Cross-entropy Method (CEM)</a></li>
<li><a href="./OtherPolicyGradient.html#sec:evolutionary-search-es"><span class="toc-section-number">4.10.2</span> Evolutionary Search (ES)</a></li>
</ul></li>
</ul></li>
<li><a href="./ModelBased.html#sec:model-based-rl"><span class="toc-section-number">5</span> Model-based RL</a><ul>
<li><a href="./ModelBased.html#sec:dyna-q"><span class="toc-section-number">5.1</span> Dyna-Q</a></li>
<li><a href="./ModelBased.html#sec:unsorted-references"><span class="toc-section-number">5.2</span> Unsorted references</a></li>
</ul></li>
<li><a href="./Practice.html#sec:deep-rl-in-practice"><span class="toc-section-number">6</span> Deep RL in practice</a><ul>
<li><a href="./Practice.html#sec:limitations"><span class="toc-section-number">6.1</span> Limitations</a></li>
<li><a href="./Practice.html#sec:simulation-environments"><span class="toc-section-number">6.2</span> Simulation environments</a></li>
<li><a href="./Practice.html#sec:algorithm-implementations"><span class="toc-section-number">6.3</span> Algorithm implementations</a></li>
</ul></li>
<li><a href="./References.html#sec:references">References</a></li>
</ul>


    </div>
  </div>
</div>

<article>

<h1 id="sec:policy-gradient-methods"><span class="header-section-number">4</span> Policy Gradient methods</h1>
<p><strong>Policy search</strong> methods directly learn to estimate the policy <span class="math inline">\(\pi_\theta\)</span> with a parameterized function estimator. The goal of the neural network is to maximize an objective function representing the <em>return</em> (sum of rewards, noted <span class="math inline">\(R(\tau)\)</span> for simplicity) of the trajectories <span class="math inline">\(\tau = (s_0, a_0, s_1, a_1, \ldots, s_T, a_T)\)</span> selected by the policy <span class="math inline">\(\pi_\theta\)</span>:</p>
<p><span class="math display">\[
    J(\theta) = \mathbb{E}_{\tau \sim \rho_\theta}[R(\tau)] = \mathbb{E}_{\tau \sim \rho_\theta}[\sum_{t=0}^T \gamma^t \, r(s_t, a_t, s_{t+1}) ]
\]</span></p>
<p>To maximize this objective function, the policy <span class="math inline">\(\pi_\theta\)</span> should only generate trajectories <span class="math inline">\(\tau\)</span> associated with high returns <span class="math inline">\(R(\tau)\)</span> and avoid those with low return, which is exactly what we want.</p>
<p>The objective function uses the mathematical expectation of the return over all possible trajectories. The likelihood that a trajectory is generated by the policy <span class="math inline">\(\pi_\theta\)</span> is noted <span class="math inline">\(\rho_\theta(\tau)\)</span> and given by:</p>
<p><span id="eq:likelihood_trajectory"><span class="math display">\[
    \rho_\theta(\tau) = p_\theta(s_0, a_0, \ldots, s_T, a_T) = p_0 (s_0) \, \prod_{t=0}^T \pi_\theta(s_t, a_t) p(s_{t+1} | s_t, a_t)
\qquad(6)\]</span></span></p>
<p><span class="math inline">\(p_0 (s_0)\)</span> is the initial probability of starting in <span class="math inline">\(s_0\)</span> (independent from the policy) and <span class="math inline">\(p(s_{t+1} | s_t, a_t)\)</span> is the transition probability defining the MDP. Having the probability distribution of the trajectories, we can expand the mathematical expectation in the objective function:</p>
<p><span class="math display">\[
    J(\theta) = \int_\tau \rho_\theta (\tau) \, R(\tau) \, d\tau
\]</span></p>
<p>Monte-Carlo sampling could be used to estimate the objective function. One basically would have to sample multiple trajectories <span class="math inline">\(\{\tau_i\}\)</span> and average the obtained returns:</p>
<p><span class="math display">\[
    J(\theta) \approx \frac{1}{N} \, \sum_i \rho_\theta(\tau_i) \, R(\tau_i)
\]</span></p>
<p>However, this approach would suffer from several problems:</p>
<ol type="1">
<li>The trajectory space is extremely huge, so one would need a lot of sampled trajectories to have a correct estimate of the objective function (<strong>high variance</strong>).</li>
<li>For stability reasons, only small changes can be made to the policy at each iteration, so it would necessitate a lot of episodes (<strong>sample complexity</strong>).</li>
<li>The probability of a trajectory is difficult to estimate: the initial probability distribution <span class="math inline">\(p_0\)</span> has to be known, as well as the dynamics of the MDP (<span class="math inline">\(p(s_{t+1} | s_t, a_t)\)</span>). Those could be approximated or learned (<em>model-based RL</em>), but it would limit the applicability of the method and approximation errors would accumulate quickly over a trajectory.</li>
<li>For continuing tasks (<span class="math inline">\(T = \infty\)</span>), the return can not be estimated.</li>
</ol>
<p>The policy search methods presented in this section are called <strong>policy gradient methods</strong>. As we are going to apply gradient ascent on the weights <span class="math inline">\(\theta\)</span> in order to maximize <span class="math inline">\(J(\theta)\)</span>, all we actually need is the gradient <span class="math inline">\(\nabla_\theta J(\theta)\)</span> of the objective function w.r.t the weights:</p>
<p><span class="math display">\[
    \nabla_\theta J(\theta) = \frac{\partial J(\theta)}{\partial \theta}
\]</span></p>
<p>Once a suitable estimation of this <strong>policy gradient</strong> is obtained, gradient ascent is straightforward:</p>
<p><span class="math display">\[
    \theta \leftarrow \theta + \eta \, \nabla_\theta J(\theta)
\]</span></p>
<p>The rest of this section basically presents methods allowing to estimate the policy gradient (REINFORCE, DPG) and to improve the sample complexity. See <a href="http://www.scholarpedia.org/article/Policy_gradient_methods" class="uri">http://www.scholarpedia.org/article/Policy_gradient_methods</a> for an more detailed overview of policy gradient methods, <a href="https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html" class="uri">https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html</a> and <a href="http://karpathy.github.io/2016/05/31/rl/" class="uri">http://karpathy.github.io/2016/05/31/rl/</a> for excellent tutorials from Lilian Weng and Andrej Karpathy. The article by <span class="citation" data-cites="Peters2008">Peters and Schaal (<a href="References.html#ref-Peters2008" role="doc-biblioref">2008</a>)</span> is also a good overview of policy gradient methods.</p>
<h2 id="sec:reinforce"><span class="header-section-number">4.1</span> REINFORCE</h2>
<h3 id="sec:estimating-the-policy-gradient"><span class="header-section-number">4.1.1</span> Estimating the policy gradient</h3>
<p><span class="citation" data-cites="Williams1992">Williams (<a href="References.html#ref-Williams1992" role="doc-biblioref">1992</a>)</span> proposed a useful estimate of the policy gradient. Considering that the return <span class="math inline">\(R(\tau)\)</span> of a trajectory does not depend on the parameters <span class="math inline">\(\theta\)</span>, one can simplify the policy gradient in the following way:</p>
<p><span class="math display">\[
    \nabla_\theta J(\theta) = \nabla_\theta \int_\tau \rho_\theta (\tau) \, R(\tau) \, d\tau =  \int_\tau (\nabla_\theta \rho_\theta (\tau)) \, R(\tau) \, d\tau
\]</span></p>
<p>We now use the <strong>log-trick</strong>, a simple identity based on the fact that:</p>
<p><span class="math display">\[
    \frac{d \log f(x)}{dx} = \frac{f&#39;(x)}{f(x)}
\]</span></p>
<p>to rewrite the policy gradient of a single trajectory:</p>
<p><span class="math display">\[
    \nabla_\theta \rho_\theta (\tau) = \rho_\theta (\tau) \, \nabla_\theta \log \rho_\theta (\tau)
\]</span></p>
<p>The policy gradient becomes:</p>
<p><span class="math display">\[
    \nabla_\theta J(\theta) =  \int_\tau \rho_\theta (\tau) \, \nabla_\theta \log \rho_\theta (\tau) \, R(\tau) \, d\tau
\]</span></p>
<p>which now has the form of a mathematical expectation:</p>
<p><span class="math display">\[
    \nabla_\theta J(\theta) =  \mathbb{E}_{\tau \sim \rho_\theta}[ \nabla_\theta \log \rho_\theta (\tau) \, R(\tau) ]
\]</span></p>
<p>This means that we can obtain an estimate of the policy gradient by simply sampling different trajectories <span class="math inline">\(\{\tau_i\}\)</span> and averaging <span class="math inline">\(\nabla_\theta \log \rho_\theta (\tau_i) \, R(\tau_i)\)</span> (Monte-Carlo sampling).</p>
<p>Let’s now look further at how the gradient of the log-likelihood of a trajectory <span class="math inline">\(\log \pi_\theta (\tau)\)</span> look like. Through its definition (Eq. <a href="#eq:likelihood_trajectory">6</a>), the log-likelihood of a trajectory is:</p>
<p><span id="eq:loglikelihood_trajectory"><span class="math display">\[
    \log \rho_\theta(\tau) = \log p_0 (s_0) + \sum_{t=0}^T \log \pi_\theta(s_t, a_t) + \sum_{t=0}^T \log p(s_{t+1} | s_t, a_t)
\qquad(7)\]</span></span></p>
<p><span class="math inline">\(\log p_0 (s_0)\)</span> and <span class="math inline">\(\log p(s_{t+1} | s_t, a_t)\)</span> do not depend on the parameters <span class="math inline">\(\theta\)</span> (they are defined by the MDP), so the gradient of the log-likelihood is simply:</p>
<p><span id="eq:gradloglikelihood_trajectory"><span class="math display">\[
    \nabla_\theta \log \rho_\theta(\tau) = \sum_{t=0}^T \nabla_\theta \log \pi_\theta(s_t, a_t)
\qquad(8)\]</span></span></p>
<p><span class="math inline">\(\nabla_\theta \log \pi_\theta(s_t, a_t)\)</span> is called the <strong>score function</strong>.</p>
<p>This is the main reason why policy gradient algorithms are used: the gradient is independent from the MDP dynamics, allowing <strong>model-free</strong> learning. The policy gradient is then given by:</p>
<p><span class="math display">\[
    \nabla_\theta J(\theta) =  \mathbb{E}_{\tau \sim \rho_\theta}[\sum_{t=0}^T \nabla_\theta \log \pi_\theta(s_t, a_t) \, R(\tau) ] =  \mathbb{E}_{\tau \sim \rho_\theta}[ \sum_{t=0}^T \nabla_\theta \log \pi_\theta(s_t, a_t) \, (\sum_{t=0}^T \gamma^t r_{t+1})]
\]</span></p>
<p>Estimating the policy gradient now becomes straightforward using Monte-Carlo sampling. The resulting algorithm is called the <strong>REINFORCE</strong> algorithm <span class="citation" data-cites="Williams1992">(Williams, <a href="References.html#ref-Williams1992" role="doc-biblioref">1992</a>)</span>:</p>
<hr />
<ul>
<li><p>while not converged:</p>
<ul>
<li><p>Sample <span class="math inline">\(N\)</span> trajectories <span class="math inline">\(\{\tau_i\}\)</span> using the current policy <span class="math inline">\(\pi_\theta\)</span> and observe the returns <span class="math inline">\(\{R(\tau_i)\}\)</span>.</p></li>
<li><p>Estimate the policy gradient as an average over the trajectories:</p></li>
</ul>
<p><span class="math display">\[
     \nabla_\theta J(\theta) \approx \frac{1}{N} \sum_{i=1}^N \sum_{t=0}^T \nabla_\theta \log \pi_\theta(s_t, a_t) \, R(\tau_i)
  \]</span></p>
<ul>
<li>Update the policy using gradient ascent:</li>
</ul>
<p><span class="math display">\[
      \theta \leftarrow \theta + \eta \, \nabla_\theta J(\theta)
  \]</span></p></li>
</ul>
<hr />
<p>While very simple, the REINFORCE algorithm does not work very well in practice:</p>
<ol type="1">
<li>The returns <span class="math inline">\(\{R(\tau_i)\}\)</span> have a very high variance (as the Q-values in value-based methods), which is problematic for NNs (see Section <a href="./PolicyGradient.html#sec:reducing-the-variance">4.1.2</a>).</li>
<li>It requires a lot of episodes to converge (sample inefficient).</li>
<li>It only works with <strong>online</strong> learning: trajectories must be frequently sampled and immediately used to update the policy.</li>
<li>The problem must be episodic (<span class="math inline">\(T\)</span> finite).</li>
</ol>
<p>However, it has two main advantages:</p>
<ol type="1">
<li>It is a <strong>model-free</strong> method, i.e. one does not need to know anything about the MDP.</li>
<li>It also works on <strong>partially observable</strong> problems (POMDP): as the return is computed over complete trajectories, it does not matter if the states are not Markovian.</li>
</ol>
<p>The methods presented in this section basically try to solve the limitations of REINFORCE (high variance, sample efficiency, online learning) to produce efficient policy gradient algorithms.</p>
<h3 id="sec:reducing-the-variance"><span class="header-section-number">4.1.2</span> Reducing the variance</h3>
<p>The main problem with the REINFORCE algorithm is the <strong>high variance</strong> of the policy gradient. This variance comes from the fact that we learn stochastic policies (it is often unlikely to generate twice the exact same trajectory) in stochastic environments (rewards are stochastic, the same action in the same state may receive). Two trajectories which are identical at the beginning will be associated with different returns depending on the stochasticity of the policy, the transition probabilities and the probabilistic rewards.</p>
<p>Consider playing a game like chess with always the same opening, and then following a random policy. You may end up winning (<span class="math inline">\(R=1\)</span>) or losing (<span class="math inline">\(R=-1\)</span>) with some probability. The initial actions of the opening will receive a policy gradient which is sometimes positive, sometimes negative: were these actions good or bad? Should they be reinforced? In supervised learning, this would mean that the same image of a cat will be randomly associated to the labels “cat” or “dog” during training: the NN will not like it.</p>
<p>In supervised learning, there is no problem of variance in the outputs, as training sets are fixed. This is in contrary very hard to ensure in deep RL and constitutes one of its main limitations. The only direct solution is to sample enough trajectories and hope that the average will be able to smooth the variance. The problem is even worse in the following conditions:</p>
<ul>
<li>High-dimensional action spaces: it becomes difficult to sample the environment densely enough if many actions are possible.</li>
<li>Long horizons: the longer the trajectory, the more likely it will be unique.</li>
<li>Finite samples: if we cannot sample enough trajectories, the high variance can introduce a bias in the gradient, leading to poor convergence.</li>
</ul>
<p>See <a href="https://medium.com/mlreview/making-sense-of-the-bias-variance-trade-off-in-deep-reinforcement-learning-79cf1e83d565" class="uri">https://medium.com/mlreview/making-sense-of-the-bias-variance-trade-off-in-deep-reinforcement-learning-79cf1e83d565</a> for a nice explanation of the bias/variance trade-off in deep RL.</p>
<p>Another related problem is that the REINFORCE gradient is sensitive to <strong>reward scaling</strong>. Let’s consider a simple MDP where only two trajectories <span class="math inline">\(\tau_1\)</span> and <span class="math inline">\(\tau_2\)</span> are possible. Depending on the choice of the reward function, the returns may be different:</p>
<ol type="1">
<li><span class="math inline">\(R(\tau_1) = 1\)</span> and <span class="math inline">\(R(\tau_2) = -1\)</span></li>
<li><span class="math inline">\(R(\tau_1) = 3\)</span> and <span class="math inline">\(R(\tau_2) = 1\)</span></li>
</ol>
<p>In both cases, the policy should select the trajectory <span class="math inline">\(\tau_1\)</span>. However, the policy gradient for <span class="math inline">\(\tau_2\)</span> will change its sign between the two cases, although the problem is the same! What we want to do is to maximize the returns, regardless the absolute value of the rewards, but the returns are unbounded. Because of the non-stationarity of the problem (the agent becomes better with training, so the returns of the sampled trajectories will increase), the policy gradients will increase over time, what is linked to the variance problem. Value-based methods addressed this problem by using <strong>target networks</strong>, but it is not a perfect solution (the gradients become biased).</p>
<p>A first simple but effective idea to solve both problems would be to subtract the mean of the sampled returns from the returns:</p>
<hr />
<ul>
<li><p>while not converged:</p>
<ul>
<li>Sample <span class="math inline">\(N\)</span> trajectories <span class="math inline">\(\{\tau_i\}\)</span> using the current policy <span class="math inline">\(\pi_\theta\)</span> and observe the returns <span class="math inline">\(\{R(\tau_i)\}\)</span>.</li>
<li>Compute the mean return: <span class="math display">\[
  \hat{R} = \frac{1}{N} \sum_{i=1}^N R(\tau_i)
  \]</span></li>
<li>Estimate the policy gradient as an average over the trajectories: <span class="math display">\[
 \nabla_\theta J(\theta) \approx \frac{1}{N} \sum_{i=1}^N \sum_{t=0}^T \nabla_\theta \log \pi_\theta(s_t, a_t) \, ( R(\tau_i) - \hat{R})
  \]</span></li>
<li>Update the policy using gradient ascent: <span class="math display">\[
  \theta \leftarrow \theta + \eta \, \nabla_\theta J(\theta)
  \]</span></li>
</ul></li>
</ul>
<hr />
<p>This obviously solves the reward scaling problem, and reduces the variance of the gradients. But are we allowed to do this (i.e. does it introduce a bias to the gradient)? <span class="citation" data-cites="Williams1992">Williams (<a href="References.html#ref-Williams1992" role="doc-biblioref">1992</a>)</span> showed that subtracting a constant <span class="math inline">\(b\)</span> from the returns still leads to an unbiased estimate of the gradient:</p>
<p><span class="math display">\[
    \nabla_\theta J(\theta) =  \mathbb{E}_{\tau \sim \rho_\theta}[\nabla_\theta \log \rho_\theta (\tau) \, (R(\tau) -b) ]
\]</span></p>
<p>The proof is actually quite simple:</p>
<p><span class="math display">\[
    \mathbb{E}_{\tau \sim \rho_\theta}[\nabla_\theta \log \rho_\theta (\tau) \, b ] = \int_\tau \rho_\theta (\tau) \nabla_\theta \log \rho_\theta (\tau) \, b \, d\tau = \int_\tau \nabla_\theta  \rho_\theta (\tau) \, b \, d\tau = b \, \nabla_\theta \int_\tau \rho_\theta (\tau) \, d\tau =  b \, \nabla_\theta 1 = 0
\]</span></p>
<p>As long as the constant <span class="math inline">\(b\)</span> does not depend on <span class="math inline">\(\theta\)</span>, the estimator is unbiased. The resulting algorithm is called <strong>REINFORCE with baseline</strong>. <span class="citation" data-cites="Williams1992">Williams (<a href="References.html#ref-Williams1992" role="doc-biblioref">1992</a>)</span> has actually showed that the best baseline (the one which also reduces the variance) is the mean return weighted by the square of the gradient of the log-likelihood:</p>
<p><span class="math display">\[
    b = \frac{\mathbb{E}_{\tau \sim \rho_\theta}[(\nabla_\theta \log \rho_\theta (\tau))^2 \, R(\tau)]}{\mathbb{E}_{\tau \sim \rho_\theta}[(\nabla_\theta \log \rho_\theta (\tau))^2]}
\]</span></p>
<p>but the mean reward actually work quite well. Advantage actor-critic methods (Section <a href="./ActorCritic.html#sec:advantage-actor-critic-methods">4.2</a>) replace the constant <span class="math inline">\(b\)</span> with an estimate of the value of each state <span class="math inline">\(\hat{V}(s_t)\)</span>.</p>
<h3 id="sec:policy-gradient-theorem"><span class="header-section-number">4.1.3</span> Policy Gradient theorem</h3>
<p>Let’s have another look at the REINFORCE estimate of the policy gradient after sampling:</p>
<p><span class="math display">\[
   \nabla_\theta J(\theta) \approx \frac{1}{N} \sum_{i=1}^N \sum_{t=0}^T \nabla_\theta \log \pi_\theta(s_t, a_t) \, R(\tau_i) = \frac{1}{N} \sum_{i=1}^N (\sum_{t=0}^T \nabla_\theta \log \pi_\theta(s_t, a_t) ) \, (\sum_{t&#39;=0}^T \gamma^{t&#39;} \, r(s_{t&#39;}, a_{t&#39;}, s_{t&#39;+1}) )
\]</span></p>
<p>For each transition <span class="math inline">\((s_t, a_t)\)</span>, the gradient of its log-likelihood (<em>score function</em>) <span class="math inline">\(\nabla_\theta \log \pi_\theta(s_t, a_t) )\)</span> is multiplied by the return of the whole episode <span class="math inline">\(R(\tau) = \sum_{t&#39;=0}^T \gamma^{t&#39;} \, r(s_{t&#39;}, a_{t&#39;}, s_{t&#39;+1})\)</span>. However, the <strong>causality principle</strong> dictates that the reward received at <span class="math inline">\(t=0\)</span> does not depend on actions taken in the future, so we can simplify the return for each transition:</p>
<p><span class="math display">\[
 \nabla_\theta J(\theta) \approx \frac{1}{N} \sum_{i=1}^N (\sum_{t=0}^T \nabla_\theta \log \pi_\theta(s_t, a_t)  \, \sum_{t&#39;=t}^T \gamma^{t&#39;-t} \, r(s_{t&#39;}, a_{t&#39;}, s_{t&#39;+1}) )
\]</span></p>
<p>The quantity <span class="math inline">\(\hat{Q}(s_t, a_t) = \sum_{t&#39;=t}^T \gamma^{t&#39;-t} \, r(s_{t&#39;}, a_{t&#39;}, s_{t&#39;+1})\)</span> is called the <strong>reward to-go</strong> from the transition <span class="math inline">\((s_t, a_t)\)</span>, i.e. the discounted sum of future rewards after that transition. Quite obviously, the Q-value of that action is the mathematical expectation of this reward to-go.</p>
<figure>
<img src="img/rewardtogo.png" id="fig:rewardtogo" style="width:30.0%" alt="" /><figcaption>Figure 20: The reward to-go is the sum of rewards gathered during a single trajectory after a transition <span class="math inline">\((s, a)\)</span>. The Q-value of the action <span class="math inline">\((s, a)\)</span> is the expectation of the reward to-go. Taken from S. Levine’s lecture <a href="http://rll.berkeley.edu/deeprlcourse/" class="uri">http://rll.berkeley.edu/deeprlcourse/</a>.</figcaption>
</figure>
<p><span class="math display">\[
 \nabla_\theta J(\theta) \approx \frac{1}{N} \sum_{i=1}^N \sum_{t=0}^T \nabla_\theta \log \pi_\theta(s_t, a_t) \, \hat{Q}(s_t, a_t)
\]</span></p>
<p><span class="citation" data-cites="Sutton1999">Sutton et al. (<a href="References.html#ref-Sutton1999" role="doc-biblioref">1999</a>)</span> showed that the policy gradient can be estimated by replacing the return of the sampled trajectory with the Q-value of each action, what leads to the <strong>policy gradient theorem</strong> (Eq. <a href="#eq:policygradienttheorem">9</a>):</p>
<p><span id="eq:policygradienttheorem"><span class="math display">\[
    \nabla_\theta J(\theta) =  \mathbb{E}_{s \sim \rho_\theta, a \sim \pi_\theta}[\nabla_\theta \log \pi_\theta(s, a) \, Q^{\pi_\theta}(s, a)]
\qquad(9)\]</span></span></p>
<p>where <span class="math inline">\(\rho_\theta\)</span> is the distribution of states reachable under the policy <span class="math inline">\(\pi_\theta\)</span>. Because the actual return <span class="math inline">\(R(\tau)\)</span> is replaced by its expectation <span class="math inline">\(Q^{\pi_\theta}(s, a)\)</span>, the policy gradient is now a mathematical expectation over <strong>single transitions</strong> instead of complete trajectories, allowing <strong>bootstrapping</strong> as in temporal difference methods (Section <a href="./BasicRL.html#sec:temporal-difference">2.1.5</a>).</p>
<p>One clearly sees that REINFORCE is actually a special case of the policy gradient theorem, where the Q-value of an action replaces the return obtained during the corresponding trajectory.</p>
<p>The problem is of course that the true Q-value of the actions is as unknown as the policy. However, <span class="citation" data-cites="Sutton1999">Sutton et al. (<a href="References.html#ref-Sutton1999" role="doc-biblioref">1999</a>)</span> showed that it is possible to estimate the Q-values with a function approximator <span class="math inline">\(Q_\varphi(s, a)\)</span> with parameters <span class="math inline">\(\varphi\)</span> and obtain an unbiased estimation:</p>
<p><span id="eq:policygradienttheoremapprox"><span class="math display">\[
    \nabla_\theta J(\theta) =  \mathbb{E}_{s \sim \rho_\theta, a \sim \pi_\theta}[\nabla_\theta \log \pi_\theta(s, a) \, Q_\varphi(s, a))]
\qquad(10)\]</span></span></p>
<p>Formally, the Q-value approximator must respect the Compatible Function Approximation Theorem, which states that the value approximator must be compatible with the policy (<span class="math inline">\(\nabla_\varphi Q_\varphi(s, a) = \nabla_\theta \log \pi_\theta(s, a)\)</span>) and minimize the mean-square error with the true Q-values <span class="math inline">\(\mathbb{E}_{s \sim \rho^\pi, a \sim \pi_\theta} [(Q^{\pi_\theta}(s, a) - Q_\varphi(s, a))^2]\)</span>. In the algorithms presented in this section, these conditions are either met or neglected.</p>
<p>The resulting algorithm belongs to the <strong>actor-critic</strong> class, in the sense that:</p>
<ul>
<li>The <strong>actor</strong> <span class="math inline">\(\pi_\theta(s, a)\)</span> learns to approximate the policy by maximizing Eq. <a href="#eq:policygradienttheoremapprox">10</a>.</li>
<li>The <strong>critic</strong> <span class="math inline">\(Q_\varphi(s, a)\)</span> learns to estimate the policy by minimizing the mse with the true Q-values.</li>
</ul>
<p>Fig. <a href="#fig:actorcriticpolicy">21</a> shows the architecture of the algorithm. The only problem left is to provide the critic with the true Q-values.</p>
<figure>
<img src="img/policygradient.png" id="fig:actorcriticpolicy" style="width:80.0%" alt="" /><figcaption>Figure 21: Architecture of the policy gradient (PG) method.</figcaption>
</figure>
<p>Most policy-gradient algorithms (A3C, DPPG, TRPO) are actor-critic architectures. Some remarks already:</p>
<ul>
<li>Trajectories now appear only implicitly in the policy gradient, one can even sample single transitions. It should therefore be possible (with modifications) to do <strong>off-policy learning</strong>, for example with using importance sampling (Section <a href="./ImportanceSampling.html#sec:importance-sampling">4.3.1</a>) or a replay buffer of stored transitions as in DQN (see ACER Section <a href="./NaturalGradient.html#sec:actor-critic-with-experience-replay-acer">4.5.5</a>). REINFORCE works strictly on-policy.</li>
<li>The policy gradient theorem suffers from the same <strong>high variance</strong> problem as REINFORCE. The different algorithms presented later are principally attempts to solve this problem and reduce the sample complexity: advantages, deterministic policies, natural gradients…</li>
<li>The actor and the critic can be completely separated, or share some parameters.</li>
</ul>

<br>
<div class="arrows">
<a href="Valuebased.html" class="previous">&laquo; Previous</a>
<a href="ActorCritic.html" class="next">Next &raquo;</a>
</div>

</article>

</body>
</html>
