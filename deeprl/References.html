<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <meta name="author" content="Julien Vitay">
  <title>Deep Reinforcement Learning</title>


  <link rel="stylesheet" href="assets/syntax.css">
  <link rel="stylesheet" href="assets/github.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->

</head>

<body class="markdown-body">

<header>
  <h1 class="title"><strong>Deep Reinforcement Learning</strong></h1>
  <p class="author">Julien Vitay - <a href="mailto:julien.vitay@informatik.tu-chemnitz.de">julien.vitay@informatik.tu-chemnitz.de</a></p>
</header>


<article>

<nav id="TOC" class ="toc">
  <h2><strong>Deep Reinforcement Learning</strong></h2>
  <p class="author">Julien Vitay</p>

<ul>
<li><a href="./Introduction.html#sec:introduction"><span class="toc-section-number">1</span> Introduction</a></li>
<li><a href="./BasicRL.html#sec:basics"><span class="toc-section-number">2</span> Basics</a><ul>
<li><a href="./BasicRL.html#sec:reinforcement-learning-and-markov-decision-process"><span class="toc-section-number">2.1</span> Reinforcement learning and Markov Decision Process</a><ul>
<li><a href="./BasicRL.html#sec:policy-and-value-functions"><span class="toc-section-number">2.1.1</span> Policy and value functions</a></li>
<li><a href="./BasicRL.html#sec:bellman-equations"><span class="toc-section-number">2.1.2</span> Bellman equations</a></li>
<li><a href="./BasicRL.html#sec:dynamic-programming"><span class="toc-section-number">2.1.3</span> Dynamic programming</a></li>
<li><a href="./BasicRL.html#sec:monte-carlo-sampling"><span class="toc-section-number">2.1.4</span> Monte-Carlo sampling</a></li>
<li><a href="./BasicRL.html#sec:temporal-difference"><span class="toc-section-number">2.1.5</span> Temporal Difference</a></li>
<li><a href="./BasicRL.html#sec:eligibility-traces"><span class="toc-section-number">2.1.6</span> Eligibility traces</a></li>
<li><a href="./BasicRL.html#sec:actor-critic-architectures"><span class="toc-section-number">2.1.7</span> Actor-critic architectures</a></li>
<li><a href="./BasicRL.html#sec:function-approximation"><span class="toc-section-number">2.1.8</span> Function approximation</a></li>
</ul></li>
<li><a href="./DeepLearning.html#sec:deep-learning"><span class="toc-section-number">2.2</span> Deep learning</a><ul>
<li><a href="./DeepLearning.html#sec:deep-neural-networks"><span class="toc-section-number">2.2.1</span> Deep neural networks</a></li>
<li><a href="./DeepLearning.html#sec:convolutional-networks"><span class="toc-section-number">2.2.2</span> Convolutional networks</a></li>
<li><a href="./DeepLearning.html#sec:recurrent-neural-networks"><span class="toc-section-number">2.2.3</span> Recurrent neural networks</a></li>
</ul></li>
</ul></li>
<li><a href="./Valuebased.html#sec:value-based-methods"><span class="toc-section-number">3</span> Value-based methods</a><ul>
<li><a href="./Valuebased.html#sec:limitations-of-deep-neural-networks-for-function-approximation"><span class="toc-section-number">3.1</span> Limitations of deep neural networks for function approximation</a></li>
<li><a href="./Valuebased.html#sec:deep-q-network-dqn"><span class="toc-section-number">3.2</span> Deep Q-Network (DQN)</a></li>
<li><a href="./Valuebased.html#sec:double-dqn"><span class="toc-section-number">3.3</span> Double DQN</a></li>
<li><a href="./Valuebased.html#sec:prioritized-experience-replay"><span class="toc-section-number">3.4</span> Prioritized experience replay</a></li>
<li><a href="./Valuebased.html#sec:duelling-network"><span class="toc-section-number">3.5</span> Duelling network</a></li>
<li><a href="./Valuebased.html#sec:distributed-dqn-gorila"><span class="toc-section-number">3.6</span> Distributed DQN (GORILA)</a></li>
<li><a href="./Valuebased.html#sec:deep-recurrent-q-learning-drqn"><span class="toc-section-number">3.7</span> Deep Recurrent Q-learning (DRQN)</a></li>
<li><a href="./Valuebased.html#sec:other-variants-of-dqn"><span class="toc-section-number">3.8</span> Other variants of DQN</a></li>
</ul></li>
<li><a href="./PolicyGradient.html#sec:policy-gradient-methods"><span class="toc-section-number">4</span> Policy Gradient methods</a><ul>
<li><a href="./PolicyGradient.html#sec:reinforce"><span class="toc-section-number">4.1</span> REINFORCE</a><ul>
<li><a href="./PolicyGradient.html#sec:estimating-the-policy-gradient"><span class="toc-section-number">4.1.1</span> Estimating the policy gradient</a></li>
<li><a href="./PolicyGradient.html#sec:reducing-the-variance"><span class="toc-section-number">4.1.2</span> Reducing the variance</a></li>
<li><a href="./PolicyGradient.html#sec:policy-gradient-theorem"><span class="toc-section-number">4.1.3</span> Policy Gradient theorem</a></li>
</ul></li>
<li><a href="./ActorCritic.html#sec:advantage-actor-critic-methods"><span class="toc-section-number">4.2</span> Advantage Actor-Critic methods</a><ul>
<li><a href="./ActorCritic.html#sec:advantage-actor-critic-a2c"><span class="toc-section-number">4.2.1</span> Advantage Actor-Critic (A2C)</a></li>
<li><a href="./ActorCritic.html#sec:asynchronous-advantage-actor-critic-a3c"><span class="toc-section-number">4.2.2</span> Asynchronous Advantage Actor-Critic (A3C)</a></li>
<li><a href="./ActorCritic.html#sec:generalized-advantage-estimation-gae"><span class="toc-section-number">4.2.3</span> Generalized Advantage Estimation (GAE)</a></li>
<li><a href="./ActorCritic.html#sec:stochastic-actor-critic-for-continuous-action-spaces"><span class="toc-section-number">4.2.4</span> Stochastic actor-critic for continuous action spaces</a></li>
</ul></li>
<li><a href="./ImportanceSampling.html#sec:off-policy-actor-critic"><span class="toc-section-number">4.3</span> Off-policy Actor-Critic</a><ul>
<li><a href="./ImportanceSampling.html#sec:importance-sampling"><span class="toc-section-number">4.3.1</span> Importance sampling</a></li>
<li><a href="./ImportanceSampling.html#sec:linear-off-policy-actor-critic-off-pac"><span class="toc-section-number">4.3.2</span> Linear Off-Policy Actor-Critic (Off-PAC)</a></li>
<li><a href="./ImportanceSampling.html#sec:retrace"><span class="toc-section-number">4.3.3</span> Retrace</a></li>
<li><a href="./ImportanceSampling.html#sec:self-imitation-learning-sil"><span class="toc-section-number">4.3.4</span> Self-Imitation Learning (SIL)</a></li>
</ul></li>
<li><a href="./DPG.html#sec:deterministic-policy-gradient-dpg"><span class="toc-section-number">4.4</span> Deterministic Policy Gradient (DPG)</a><ul>
<li><a href="./DPG.html#sec:deterministic-policy-gradient-theorem"><span class="toc-section-number">4.4.1</span> Deterministic policy gradient theorem</a></li>
<li><a href="./DPG.html#sec:deep-deterministic-policy-gradient-ddpg"><span class="toc-section-number">4.4.2</span> Deep Deterministic Policy Gradient (DDPG)</a></li>
<li><a href="./DPG.html#sec:distributed-distributional-ddpg-d4pg"><span class="toc-section-number">4.4.3</span> Distributed Distributional DDPG (D4PG)</a></li>
</ul></li>
<li><a href="./NaturalGradient.html#sec:natural-gradients"><span class="toc-section-number">4.5</span> Natural Gradients</a><ul>
<li><a href="./NaturalGradient.html#sec:principle-of-natural-gradients"><span class="toc-section-number">4.5.1</span> Principle of natural gradients</a></li>
<li><a href="./NaturalGradient.html#sec:natural-policy-gradient-and-natural-actor-critic-nac"><span class="toc-section-number">4.5.2</span> Natural policy gradient and Natural Actor Critic (NAC)</a></li>
<li><a href="./NaturalGradient.html#sec:trust-region-policy-optimization-trpo"><span class="toc-section-number">4.5.3</span> Trust Region Policy Optimization (TRPO)</a></li>
<li><a href="./NaturalGradient.html#sec:proximal-policy-optimization-ppo"><span class="toc-section-number">4.5.4</span> Proximal Policy Optimization (PPO)</a></li>
<li><a href="./NaturalGradient.html#sec:actor-critic-with-experience-replay-acer"><span class="toc-section-number">4.5.5</span> Actor-Critic with Experience Replay (ACER)</a></li>
</ul></li>
<li><a href="./OtherPolicyGradient.html#sec:maximum-entropy-rl"><span class="toc-section-number">4.6</span> Maximum Entropy RL</a><ul>
<li><a href="#sec:entropy-regularization-1"><span class="toc-section-number">4.6.1</span> Entropy regularization</a></li>
<li><a href="./OtherPolicyGradient.html#sec:soft-actor-critic-sac"><span class="toc-section-number">4.6.2</span> Soft Actor-Critic (SAC)</a></li>
</ul></li>
<li><a href="./OtherPolicyGradient.html#sec:distributional-learning"><span class="toc-section-number">4.7</span> Distributional learning</a><ul>
<li><a href="./OtherPolicyGradient.html#sec:the-reactor"><span class="toc-section-number">4.7.1</span> The Reactor</a></li>
</ul></li>
<li><a href="./OtherPolicyGradient.html#sec:other-policy-search-methods"><span class="toc-section-number">4.8</span> Other policy search methods</a><ul>
<li><a href="./OtherPolicyGradient.html#sec:stochastic-value-gradient-svg"><span class="toc-section-number">4.8.1</span> Stochastic Value Gradient (SVG)</a></li>
<li><a href="./OtherPolicyGradient.html#sec:q-prop"><span class="toc-section-number">4.8.2</span> Q-Prop</a></li>
<li><a href="./OtherPolicyGradient.html#sec:normalized-advantage-function-naf"><span class="toc-section-number">4.8.3</span> Normalized Advantage Function (NAF)</a></li>
<li><a href="./OtherPolicyGradient.html#sec:fictitious-self-play-fsp"><span class="toc-section-number">4.8.4</span> Fictitious Self-Play (FSP)</a></li>
</ul></li>
<li><a href="./OtherPolicyGradient.html#sec:comparison-between-value-based-and-policy-gradient-methods"><span class="toc-section-number">4.9</span> Comparison between value-based and policy gradient methods</a></li>
<li><a href="./OtherPolicyGradient.html#sec:gradient-free-policy-search"><span class="toc-section-number">4.10</span> Gradient-free policy search</a><ul>
<li><a href="./OtherPolicyGradient.html#sec:cross-entropy-method-cem"><span class="toc-section-number">4.10.1</span> Cross-entropy Method (CEM)</a></li>
<li><a href="./OtherPolicyGradient.html#sec:evolutionary-search-es"><span class="toc-section-number">4.10.2</span> Evolutionary Search (ES)</a></li>
</ul></li>
</ul></li>
<li><a href="./Practice.html#sec:deep-rl-in-practice"><span class="toc-section-number">5</span> Deep RL in practice</a><ul>
<li><a href="./Practice.html#sec:limitations"><span class="toc-section-number">5.1</span> Limitations</a></li>
<li><a href="./Practice.html#sec:reward-shaping"><span class="toc-section-number">5.2</span> Reward shaping</a></li>
<li><a href="./Practice.html#sec:simulation-environments"><span class="toc-section-number">5.3</span> Simulation environments</a></li>
<li><a href="./Practice.html#sec:algorithm-implementations"><span class="toc-section-number">5.4</span> Algorithm implementations</a></li>
</ul></li>
<li><a href="./References.html#sec:references">References</a></li>
</ul>


</nav>



<h1 id="sec:references" class="unnumbered">References</h1>
<div id="refs" class="references">
<div id="ref-Amari1998">
<p>Amari, S.-I. 1998. “Natural Gradient Works Efficiently in Learning.” <em>Neural Computation</em> 10 (2): 251–76.</p>
</div>
<div id="ref-Andrychowicz2017">
<p>Andrychowicz, Marcin, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Josh Tobin, Pieter Abbeel, and Wojciech Zaremba. 2017. “Hindsight Experience Replay,” July. <a href="http://arxiv.org/abs/1707.01495">http://arxiv.org/abs/1707.01495</a>.</p>
</div>
<div id="ref-Anschel2016">
<p>Anschel, Oron, Nir Baram, and Nahum Shimkin. 2016. “Averaged-DQN: Variance Reduction and Stabilization for Deep Reinforcement Learning,” November. <a href="http://arxiv.org/abs/1611.01929">http://arxiv.org/abs/1611.01929</a>.</p>
</div>
<div id="ref-Arjovsky2017">
<p>Arjovsky, Martin, Soumith Chintala, and Léon Bottou. 2017. “Wasserstein GAN,” January. <a href="http://arxiv.org/abs/1701.07875">http://arxiv.org/abs/1701.07875</a>.</p>
</div>
<div id="ref-Arulkumaran2017">
<p>Arulkumaran, Kai, Marc Peter Deisenroth, Miles Brundage, and Anil Anthony Bharath. 2017. “A Brief Survey of Deep Reinforcement Learning.” <a href="https://arxiv.org/pdf/1708.05866.pdf">https://arxiv.org/pdf/1708.05866.pdf</a>.</p>
</div>
<div id="ref-Baird1993">
<p>Baird, L.C. 1993. “Advantage Updating.” Technical Report WL-TR-93-1146. Wright-Patterson Air Force Base. <a href="http://leemon.com/papers/1993b.pdf">http://leemon.com/papers/1993b.pdf</a>.</p>
</div>
<div id="ref-Bakker2001">
<p>Bakker, Bram. 2001. “Reinforcement Learning with Long Short-Term Memory.” In <em>Advances in Neural Information Processing Systems 14 (NIPS 2001)</em>, 1475–82. <a href="https://papers.nips.cc/paper/1953-reinforcement-learning-with-long-short-term-memory">https://papers.nips.cc/paper/1953-reinforcement-learning-with-long-short-term-memory</a>.</p>
</div>
<div id="ref-Barth-Maron2018">
<p>Barth-Maron, Gabriel, Matthew W. Hoffman, David Budden, Will Dabney, Dan Horgan, Dhruva TB, Alistair Muldal, Nicolas Heess, and Timothy Lillicrap. 2018. “Distributed Distributional Deterministic Policy Gradients,” April. <a href="http://arxiv.org/abs/1804.08617">http://arxiv.org/abs/1804.08617</a>.</p>
</div>
<div id="ref-Bellemare2017">
<p>Bellemare, Marc G., Will Dabney, and Rémi Munos. 2017. “A Distributional Perspective on Reinforcement Learning,” July. <a href="http://arxiv.org/abs/1707.06887">http://arxiv.org/abs/1707.06887</a>.</p>
</div>
<div id="ref-Cho2014">
<p>Cho, Kyunghyun, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. “Learning Phrase Representations Using RNN Encoder-Decoder for Statistical Machine Translation,” June. <a href="http://arxiv.org/abs/1406.1078">http://arxiv.org/abs/1406.1078</a>.</p>
</div>
<div id="ref-Chou2017">
<p>Chou, Po-Wei, Daniel Maturana, and Sebastian Scherer. 2017. “Improving Stochastic Policy Gradients in Continuous Control with Deep Reinforcement Learning Using the Beta Distribution.” In <em>International Conference on Machine Learning</em>. <a href="http://proceedings.mlr.press/v70/chou17a/chou17a.pdf">http://proceedings.mlr.press/v70/chou17a/chou17a.pdf</a>.</p>
</div>
<div id="ref-Degris2012">
<p>Degris, Thomas, Martha White, and Richard S. Sutton. 2012. “Linear Off-Policy Actor-Critic.” In <em>Proceedings of the 2012 International Conference on Machine Learning</em>. <a href="http://arxiv.org/abs/1205.4839">http://arxiv.org/abs/1205.4839</a>.</p>
</div>
<div id="ref-Duan2016">
<p>Duan, Yan, Xi Chen, Rein Houthooft, John Schulman, and Pieter Abbeel. 2016. “Benchmarking Deep Reinforcement Learning for Continuous Control,” April. <a href="http://arxiv.org/abs/1604.06778">http://arxiv.org/abs/1604.06778</a>.</p>
</div>
<div id="ref-Gers2001">
<p>Gers, Felix. 2001. “Long Short-Term Memory in Recurrent Neural Networks.” PhD thesis. <a href="http://www.felixgers.de/papers/phd.pdf">http://www.felixgers.de/papers/phd.pdf</a>.</p>
</div>
<div id="ref-Goodfellow2016">
<p>Goodfellow, Ian, Yoshua Bengio, and Aaron Courville. 2016. <em>Deep Learning</em>. MIT Press. <a href="http://www.deeplearningbook.org">http://www.deeplearningbook.org</a>.</p>
</div>
<div id="ref-Goodfellow2014">
<p>Goodfellow, Ian J., Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. “Generative Adversarial Networks,” June. <a href="http://arxiv.org/abs/1406.2661">http://arxiv.org/abs/1406.2661</a>.</p>
</div>
<div id="ref-Gruslys2017">
<p>Gruslys, Audrunas, Will Dabney, Mohammad Gheshlaghi Azar, Bilal Piot, Marc Bellemare, and Remi Munos. 2017. “The Reactor: A Fast and Sample-Efficient Actor-Critic Agent for Reinforcement Learning,” April. <a href="http://arxiv.org/abs/1704.04651">http://arxiv.org/abs/1704.04651</a>.</p>
</div>
<div id="ref-Gu2017">
<p>Gu, Shixiang, Ethan Holly, Timothy Lillicrap, and Sergey Levine. 2017. “Deep Reinforcement Learning for Robotic Manipulation with Asynchronous Off-Policy Updates.” In <em>Proc. ICRA</em>. <a href="http://arxiv.org/abs/1610.00633">http://arxiv.org/abs/1610.00633</a>.</p>
</div>
<div id="ref-Gu2016">
<p>Gu, Shixiang, Timothy Lillicrap, Zoubin Ghahramani, Richard E. Turner, and Sergey Levine. 2016. “Q-Prop: Sample-Efficient Policy Gradient with an Off-Policy Critic,” November. <a href="http://arxiv.org/abs/1611.02247">http://arxiv.org/abs/1611.02247</a>.</p>
</div>
<div id="ref-Gu2016a">
<p>Gu, Shixiang, Timothy Lillicrap, Ilya Sutskever, and Sergey Levine. 2016. “Continuous Deep Q-Learning with Model-Based Acceleration,” March. <a href="http://arxiv.org/abs/1603.00748">http://arxiv.org/abs/1603.00748</a>.</p>
</div>
<div id="ref-Haarnoja2017">
<p>Haarnoja, Tuomas, Haoran Tang, Pieter Abbeel, and Sergey Levine. 2017. “Reinforcement Learning with Deep Energy-Based Policies.” <em>arXiv:1702.08165 [Cs]</em>, February. <a href="http://arxiv.org/abs/1702.08165">http://arxiv.org/abs/1702.08165</a>.</p>
</div>
<div id="ref-Haarnoja2018a">
<p>Haarnoja, Tuomas, Aurick Zhou, Pieter Abbeel, and Sergey Levine. 2018. “Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor,” January. <a href="http://arxiv.org/abs/1801.01290">http://arxiv.org/abs/1801.01290</a>.</p>
</div>
<div id="ref-Hafner2011">
<p>Hafner, Roland, and Martin Riedmiller. 2011. “Reinforcement Learning in Feedback Control.” <em>Machine Learning</em> 84 (1-2): 137–69. <a href="https://doi.org/10.1007/s10994-011-5235-x">https://doi.org/10.1007/s10994-011-5235-x</a>.</p>
</div>
<div id="ref-Harutyunyan2016">
<p>Harutyunyan, A., M. G. Bellemare, T. Stepleton, and R. Munos. 2016. “Q(<span class="math inline">\(\lambda\)</span>) with Off-Policy Corrections.” <a href="http://arxiv.org/abs/1602.04951">http://arxiv.org/abs/1602.04951</a>.</p>
</div>
<div id="ref-Hausknecht2015">
<p>Hausknecht, Matthew, and Peter Stone. 2015. “Deep Recurrent Q-Learning for Partially Observable MDPs,” July. <a href="http://arxiv.org/abs/1507.06527">http://arxiv.org/abs/1507.06527</a>.</p>
</div>
<div id="ref-He2016">
<p>He, Frank S., Yang Liu, Alexander G. Schwing, and Jian Peng. 2016. “Learning to Play in a Day: Faster Deep Reinforcement Learning by Optimality Tightening,” November. <a href="http://arxiv.org/abs/1611.01606">http://arxiv.org/abs/1611.01606</a>.</p>
</div>
<div id="ref-He2015">
<p>He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015. “Deep Residual Learning for Image Recognition,” December. <a href="http://arxiv.org/abs/1512.03385">http://arxiv.org/abs/1512.03385</a>.</p>
</div>
<div id="ref-Heess2015">
<p>Heess, Nicolas, Greg Wayne, David Silver, Timothy Lillicrap, Yuval Tassa, and Tom Erez. 2015. “Learning Continuous Control Policies by Stochastic Value Gradients.” <em>Proc. International Conference on Neural Information Processing Systems</em>, 2944–52. <a href="http://dl.acm.org/citation.cfm?id=2969569">http://dl.acm.org/citation.cfm?id=2969569</a>.</p>
</div>
<div id="ref-Heinrich2015">
<p>Heinrich, Johannes, Marc Lanctot, and David Silver. 2015. “Fictitious Self-Play in Extensive-Form Games,” June, 805–13. <a href="http://proceedings.mlr.press/v37/heinrich15.html">http://proceedings.mlr.press/v37/heinrich15.html</a>.</p>
</div>
<div id="ref-Heinrich2016">
<p>Heinrich, Johannes, and David Silver. 2016. “Deep Reinforcement Learning from Self-Play in Imperfect-Information Games,” March. <a href="http://arxiv.org/abs/1603.01121">http://arxiv.org/abs/1603.01121</a>.</p>
</div>
<div id="ref-Hessel2017">
<p>Hessel, Matteo, Joseph Modayil, Hado van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan Horgan, Bilal Piot, Mohammad Azar, and David Silver. 2017. “Rainbow: Combining Improvements in Deep Reinforcement Learning,” October. <a href="http://arxiv.org/abs/1710.02298">http://arxiv.org/abs/1710.02298</a>.</p>
</div>
<div id="ref-Hochreiter1991">
<p>Hochreiter, Sepp. 1991. “Untersuchungen Zu Dynamischen Neuronalen Netzen.” PhD thesis. <a href="http://people.idsia.ch/~juergen/SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf">http://people.idsia.ch/~juergen/SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf</a>.</p>
</div>
<div id="ref-Hochreiter1997">
<p>Hochreiter, S, and J Schmidhuber. 1997. “Long Short-Term Memory.” <em>Neural Computation</em> 9 (8): 1735–80. <a href="http://www.ncbi.nlm.nih.gov/pubmed/9377276">http://www.ncbi.nlm.nih.gov/pubmed/9377276</a>.</p>
</div>
<div id="ref-Ioffe2015">
<p>Ioffe, Sergey, and Christian Szegedy. 2015. “Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift,” February. <a href="http://arxiv.org/abs/1502.03167">http://arxiv.org/abs/1502.03167</a>.</p>
</div>
<div id="ref-Kakade2001">
<p>Kakade, Sham. 2001. “A Natural Policy Gradient.” In <em>Advances in Neural Information Processing Systems 14</em>. <a href="https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf">https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf</a>.</p>
</div>
<div id="ref-Kakade2002">
<p>Kakade, Sham, and John Langford. 2002. “Approximately Optimal Approximate Reinforcement Learning.” <em>Proc. 19th International Conference on Machine Learning</em>, 267–74. <a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.7.7601">http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.7.7601</a>.</p>
</div>
<div id="ref-Kingma2013">
<p>Kingma, Diederik P, and Max Welling. 2013. “Auto-Encoding Variational Bayes,” December. <a href="http://arxiv.org/abs/1312.6114">http://arxiv.org/abs/1312.6114</a>.</p>
</div>
<div id="ref-Knight2018">
<p>Knight, Ethan, and Osher Lerner. 2018. “Natural Gradient Deep Q-Learning,” March. <a href="http://arxiv.org/abs/1803.07482">http://arxiv.org/abs/1803.07482</a>.</p>
</div>
<div id="ref-Krizhevsky2012">
<p>Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E Hinton. 2012. “ImageNet Classification with Deep Convolutional Neural Networks.” In <em>Advances in Neural Information Processing Systems (NIPS)</em>. <a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf</a>.</p>
</div>
<div id="ref-Levine2013">
<p>Levine, Sergey, and Vladlen Koltun. 2013. “Guided Policy Search.” In <em>Proceedings of Machine Learning Research</em>, 1–9. <a href="http://proceedings.mlr.press/v28/levine13.html">http://proceedings.mlr.press/v28/levine13.html</a>.</p>
</div>
<div id="ref-Li2017">
<p>Li, Yuxi. 2017. “Deep Reinforcement Learning: An Overview,” January. <a href="http://arxiv.org/abs/1701.07274">http://arxiv.org/abs/1701.07274</a>.</p>
</div>
<div id="ref-Lillicrap2015">
<p>Lillicrap, Timothy P., Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. 2015. “Continuous Control with Deep Reinforcement Learning.” <em>CoRR</em>. <a href="http://arxiv.org/abs/1509.02971">http://arxiv.org/abs/1509.02971</a>.</p>
</div>
<div id="ref-Lotzsch2017">
<p>Lötzsch, Winfried, Julien Vitay, and Fred H. Hamker. 2017. “Training a Deep Policy Gradient-Based Neural Network with Asynchronous Learners on a Simulated Robotic Problem.” In <em>INFORMATIK 2017. Gesellschaft Für Informatik</em>, edited by Maximilian Eibl and Martin Gaedke, 2143–54. Gesellschaft für Informatik, Bonn. <a href="https://dl.gi.de/handle/20.500.12116/3986">https://dl.gi.de/handle/20.500.12116/3986</a>.</p>
</div>
<div id="ref-Meuleau2000">
<p>Meuleau, Nicolas, Leonid Peshkin, Leslie P. Kaelbling, and Kee-eung Kim. 2000. “Off-Policy Policy Search.” <a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.28.894">http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.28.894</a>.</p>
</div>
<div id="ref-Mirowski2016">
<p>Mirowski, Piotr, Razvan Pascanu, Fabio Viola, Hubert Soyer, Andrew J. Ballard, Andrea Banino, Misha Denil, et al. 2016. “Learning to Navigate in Complex Environments,” November. <a href="http://arxiv.org/abs/1611.03673">http://arxiv.org/abs/1611.03673</a>.</p>
</div>
<div id="ref-Mnih2016">
<p>Mnih, Volodymyr, Adrià Puigdomènech Badia, Mehdi Mirza, Alex Graves, Timothy P. Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. 2016. “Asynchronous Methods for Deep Reinforcement Learning.” In <em>Proc. ICML</em>. <a href="http://arxiv.org/abs/1602.01783">http://arxiv.org/abs/1602.01783</a>.</p>
</div>
<div id="ref-Mnih2013">
<p>Mnih, Volodymyr, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. 2013. “Playing Atari with Deep Reinforcement Learning,” December. <a href="http://arxiv.org/abs/1312.5602">http://arxiv.org/abs/1312.5602</a>.</p>
</div>
<div id="ref-Mnih2015">
<p>Mnih, Volodymyr, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, et al. 2015. “Human-Level Control Through Deep Reinforcement Learning.” <em>Nature</em> 518 (7540): 529–33. <a href="https://doi.org/10.1038/nature14236">https://doi.org/10.1038/nature14236</a>.</p>
</div>
<div id="ref-Mousavi2018">
<p>Mousavi, Seyed Sajad, Michael Schukat, and Enda Howley. 2018. “Deep Reinforcement Learning: An Overview.” In, 426–40. Springer, Cham. <a href="https://doi.org/10.1007/978-3-319-56991-8_32">https://doi.org/10.1007/978-3-319-56991-8_32</a>.</p>
</div>
<div id="ref-Munos2016">
<p>Munos, Rémi, Tom Stepleton, Anna Harutyunyan, and Marc G. Bellemare. 2016. “Safe and Efficient Off-Policy Reinforcement Learning,” June. <a href="http://arxiv.org/abs/1606.02647">http://arxiv.org/abs/1606.02647</a>.</p>
</div>
<div id="ref-Nair2015">
<p>Nair, Arun, Praveen Srinivasan, Sam Blackwell, Cagdas Alcicek, Rory Fearon, Alessandro De Maria, Vedavyas Panneershelvam, et al. 2015. “Massively Parallel Methods for Deep Reinforcement Learning.” <a href="https://arxiv.org/pdf/1507.04296.pdf">https://arxiv.org/pdf/1507.04296.pdf</a>.</p>
</div>
<div id="ref-Nielsen2015">
<p>Nielsen, Michael A. 2015. <em>Neural Networks and Deep Learning</em>. Determination Press. <a href="http://neuralnetworksanddeeplearning.com/">http://neuralnetworksanddeeplearning.com/</a>.</p>
</div>
<div id="ref-Niu2011">
<p>Niu, Feng, Benjamin Recht, Christopher Re, and Stephen J. Wright. 2011. “HOGWILD!: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent.” In <em>Proc. Advances in Neural Information Processing Systems</em>, 21–21. <a href="http://arxiv.org/abs/1106.5730">http://arxiv.org/abs/1106.5730</a>.</p>
</div>
<div id="ref-ODonoghue2016">
<p>O’Donoghue, Brendan, Remi Munos, Koray Kavukcuoglu, and Volodymyr Mnih. 2016. “Combining Policy Gradient and Q-Learning.” <em>arXiv:1611.01626 [Cs, Math, Stat]</em>, November. <a href="http://arxiv.org/abs/1611.01626">http://arxiv.org/abs/1611.01626</a>.</p>
</div>
<div id="ref-Oh2018">
<p>Oh, Junhyuk, Yijie Guo, Satinder Singh, and Honglak Lee. 2018. “Self-Imitation Learning,” June. <a href="http://arxiv.org/abs/1806.05635">http://arxiv.org/abs/1806.05635</a>.</p>
</div>
<div id="ref-Pascanu2013">
<p>Pascanu, Razvan, and Yoshua Bengio. 2013. “Revisiting Natural Gradient for Deep Networks,” January. <a href="https://arxiv.org/abs/1301.3584">https://arxiv.org/abs/1301.3584</a>.</p>
</div>
<div id="ref-Peshkin2002">
<p>Peshkin, Leonid, and Christian R. Shelton. 2002. “Learning from Scarce Experience,” April. <a href="http://arxiv.org/abs/cs/0204043">http://arxiv.org/abs/cs/0204043</a>.</p>
</div>
<div id="ref-Peters2008">
<p>Peters, Jan, and Stefan Schaal. 2008. “Reinforcement Learning of Motor Skills with Policy Gradients.” <em>Neural Networks</em> 21 (4): 682–97. <a href="https://doi.org/10.1016/j.neunet.2008.02.003">https://doi.org/10.1016/j.neunet.2008.02.003</a>.</p>
</div>
<div id="ref-Pong2018">
<p>Pong, Vitchyr, Shixiang Gu, Murtaza Dalal, and Sergey Levine. 2018. “Temporal Difference Models: Model-Free Deep RL for Model-Based Control,” February. <a href="http://arxiv.org/abs/1802.09081">http://arxiv.org/abs/1802.09081</a>.</p>
</div>
<div id="ref-Popov2017">
<p>Popov, Ivaylo, Nicolas Heess, Timothy Lillicrap, Roland Hafner, Gabriel Barth-Maron, Matej Vecerik, Thomas Lampe, Yuval Tassa, Tom Erez, and Martin Riedmiller. 2017. “Data-Efficient Deep Reinforcement Learning for Dexterous Manipulation,” April. <a href="http://arxiv.org/abs/1704.03073">http://arxiv.org/abs/1704.03073</a>.</p>
</div>
<div id="ref-Precup2000">
<p>Precup, D., R. S Sutton, and S. Singh. 2000. “Eligibility Traces for Off-Policy Policy Evaluation.” In <em>Proceedings of the Seventeenth International Conference on Machine Learning.</em></p>
</div>
<div id="ref-Ruder2016">
<p>Ruder, Sebastian. 2016. “An Overview of Gradient Descent Optimization Algorithms,” September. <a href="http://arxiv.org/abs/1609.04747">http://arxiv.org/abs/1609.04747</a>.</p>
</div>
<div id="ref-Salimans2017">
<p>Salimans, Tim, Jonathan Ho, Xi Chen, Szymon Sidor, and Ilya Sutskever. 2017. “Evolution Strategies as a Scalable Alternative to Reinforcement Learning,” March. <a href="http://arxiv.org/abs/1703.03864">http://arxiv.org/abs/1703.03864</a>.</p>
</div>
<div id="ref-Schaul2015">
<p>Schaul, Tom, John Quan, Ioannis Antonoglou, and David Silver. 2015. “Prioritized Experience Replay,” November. <a href="http://arxiv.org/abs/1511.05952">http://arxiv.org/abs/1511.05952</a>.</p>
</div>
<div id="ref-Schulman2015">
<p>Schulman, John, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. 2015. “Trust Region Policy Optimization,” June, 1889–97. <a href="http://proceedings.mlr.press/v37/schulman15.html">http://proceedings.mlr.press/v37/schulman15.html</a>.</p>
</div>
<div id="ref-Schulman2015a">
<p>Schulman, John, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. 2015. “High-Dimensional Continuous Control Using Generalized Advantage Estimation,” June. <a href="http://arxiv.org/abs/1506.02438">http://arxiv.org/abs/1506.02438</a>.</p>
</div>
<div id="ref-Schulman2017">
<p>Schulman, John, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. “Proximal Policy Optimization Algorithms,” July. <a href="http://arxiv.org/abs/1707.06347">http://arxiv.org/abs/1707.06347</a>.</p>
</div>
<div id="ref-Silver2016">
<p>Silver, David, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, et al. 2016. “Mastering the Game of Go with Deep Neural Networks and Tree Search.” <em>Nature</em>. <a href="https://doi.org/10.1038/nature16961">https://doi.org/10.1038/nature16961</a>.</p>
</div>
<div id="ref-Silver2014">
<p>Silver, David, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller. 2014. “Deterministic Policy Gradient Algorithms.” In <em>Proc. ICML</em>, edited by Eric P Xing and Tony Jebara, 32:387–95. Proceedings of Machine Learning Research. PMLR. <a href="http://proceedings.mlr.press/v32/silver14.html">http://proceedings.mlr.press/v32/silver14.html</a>.</p>
</div>
<div id="ref-Simonyan2015">
<p>Simonyan, Karen, and Andrew Zisserman. 2015. “Very Deep Convolutional Networks for Large-Scale Image Recognition.” <em>International Conference on Learning Representations (ICRL)</em>, 1–14. <a href="https://doi.org/10.1016/j.infsof.2008.09.005">https://doi.org/10.1016/j.infsof.2008.09.005</a>.</p>
</div>
<div id="ref-Sutton1999">
<p>Sutton, Richard S., David McAllester, Satinder Singh, and Yishay Mansour. 1999. “Policy Gradient Methods for Reinforcement Learning with Function Approximation.” In <em>Proceedings of the 12th International Conference on Neural Information Processing Systems</em>, 1057–63. MIT Press. <a href="https://dl.acm.org/citation.cfm?id=3009806">https://dl.acm.org/citation.cfm?id=3009806</a>.</p>
</div>
<div id="ref-Sutton1998">
<p>Sutton, R. S., and A. G. Barto. 1998. <em>Reinforcement Learning: An Introduction</em>. Cambridge, MA: MIT press.</p>
</div>
<div id="ref-Sutton2017">
<p>———. 2017. <em>Reinforcement Learning: An Introduction</em>. 2nd ed. Cambridge, MA: MIT Press. <a href="http://incompleteideas.net/book/the-book-2nd.html">http://incompleteideas.net/book/the-book-2nd.html</a>.</p>
</div>
<div id="ref-Szita2006">
<p>Szita, István, and András Lörincz. 2006. “Learning Tetris Using the Noisy Cross-Entropy Method.” <em>Neural Computation</em> 18 (12): 2936–41. <a href="https://doi.org/10.1162/neco.2006.18.12.2936">https://doi.org/10.1162/neco.2006.18.12.2936</a>.</p>
</div>
<div id="ref-Tang2010">
<p>Tang, Jie, and Pieter Abbeel. 2010. “On a Connection Between Importance Sampling and the Likelihood Ratio Policy Gradient.” In <em>Adv. Neural Inf. Process. Syst.</em> <a href="http://rll.berkeley.edu/~jietang/pubs/nips10_Tang.pdf">http://rll.berkeley.edu/~jietang/pubs/nips10_Tang.pdf</a>.</p>
</div>
<div id="ref-Todorov2008">
<p>Todorov, E. 2008. “General Duality Between Optimal Control and Estimation.” In <em>2008 47th IEEE Conference on Decision and Control</em>, 4286–92. <a href="https://doi.org/10.1109/CDC.2008.4739438">https://doi.org/10.1109/CDC.2008.4739438</a>.</p>
</div>
<div id="ref-Toussaint2009">
<p>Toussaint, Marc. 2009. “Robot Trajectory Optimization Using Approximate Inference.” In <em>Proceedings of the 26th Annual International Conference on Machine Learning</em>, 1049–56. ICML ’09. New York, NY, USA: ACM. <a href="https://doi.org/10.1145/1553374.1553508">https://doi.org/10.1145/1553374.1553508</a>.</p>
</div>
<div id="ref-Uhlenbeck1930">
<p>Uhlenbeck, G. E., and L.S Ornstein. 1930. “On the Theory of the Brownian Motion.” <em>Physical Review</em> 36. <a href="https://doi.org/10.1103/PhysRev.36.823">https://doi.org/10.1103/PhysRev.36.823</a>.</p>
</div>
<div id="ref-vanHasselt2010">
<p>van Hasselt, Hado. 2010. “Double Q-Learning.” In <em>Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2</em>, 2613–21. Curran Associates Inc. <a href="https://dl.acm.org/citation.cfm?id=2997187">https://dl.acm.org/citation.cfm?id=2997187</a>.</p>
</div>
<div id="ref-vanHasselt2015">
<p>van Hasselt, Hado, Arthur Guez, and David Silver. 2015. “Deep Reinforcement Learning with Double Q-Learning,” September. <a href="http://arxiv.org/abs/1509.06461">http://arxiv.org/abs/1509.06461</a>.</p>
</div>
<div id="ref-Wang2017">
<p>Wang, Ziyu, Victor Bapst, Nicolas Heess, Volodymyr Mnih, Remi Munos, Koray Kavukcuoglu, and Nando de Freitas. 2017. “Sample Efficient Actor-Critic with Experience Replay,” November. <a href="http://arxiv.org/abs/1611.01224">http://arxiv.org/abs/1611.01224</a>.</p>
</div>
<div id="ref-Wang2016">
<p>Wang, Ziyu, Tom Schaul, Matteo Hessel, Hado van Hasselt, Marc Lanctot, and Nando de Freitas. 2016. “Dueling Network Architectures for Deep Reinforcement Learning.” In <em>Proc. ICML</em>. <a href="http://arxiv.org/abs/1511.06581">http://arxiv.org/abs/1511.06581</a>.</p>
</div>
<div id="ref-Watkins1989">
<p>Watkins, Christopher JCH. 1989. “Learning from Delayed Rewards.” PhD thesis.</p>
</div>
<div id="ref-Wierstra2007">
<p>Wierstra, Daan, Alexander Foerster, Jan Peters, and Jürgen Schmidhuber. 2007. “Solving Deep Memory POMDPs with Recurrent Policy Gradients.” In, 697–706. Springer, Berlin, Heidelberg. <a href="https://doi.org/10.1007/978-3-540-74690-4_71">https://doi.org/10.1007/978-3-540-74690-4_71</a>.</p>
</div>
<div id="ref-Williams1992">
<p>Williams, R. J. 1992. “Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning.” <em>Machine Learning</em> 8: 229–56.</p>
</div>
<div id="ref-Williams1991">
<p>Williams, Ronald J, and Jing Peng. 1991. “Function Optimization Using Connectionist Reinforcement Learning Algorithms.” <em>Connection Science</em> 3 (3): 241–68.</p>
</div>
</div>

<br>
<div class="arrows">
<a href="Practice.html" class="previous">&laquo; Previous</a>
<a href="#" class="next">Next &raquo;</a>
</div>


</article>
</body>
</html>
