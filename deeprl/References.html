<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <meta name="author" content="Julien Vitay">
  <title>Deep Reinforcement Learning</title>


  <link rel="stylesheet" href="assets/syntax.css">
  <link rel="stylesheet" href="assets/github.css">
  <script src="/usr/share/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->

</head>

<body class="markdown-body">

<header>
  <h1 class="title"><strong>Deep Reinforcement Learning</strong></h1>
  <p class="author">Julien Vitay - <a href="mailto:julien.vitay@informatik.tu-chemnitz.de">julien.vitay@informatik.tu-chemnitz.de</a></p>
</header>


<article>

<nav id="TOC" class ="toc">
  <h2><strong>Deep Reinforcement Learning</strong></h2>
  <p class="author">Julien Vitay</p>

<ul>
<li><a href="./Introduction.html#sec:introduction"><span class="toc-section-number">1</span> Introduction</a></li>
<li><a href="./BasicRL.html#sec:basics"><span class="toc-section-number">2</span> Basics</a><ul>
<li><a href="./BasicRL.html#sec:reinforcement-learning-and-markov-decision-process"><span class="toc-section-number">2.1</span> Reinforcement learning and Markov Decision Process</a><ul>
<li><a href="./BasicRL.html#sec:policy-and-value-functions"><span class="toc-section-number">2.1.1</span> Policy and value functions</a></li>
<li><a href="./BasicRL.html#sec:bellman-equations"><span class="toc-section-number">2.1.2</span> Bellman equations</a></li>
<li><a href="./BasicRL.html#sec:dynamic-programming"><span class="toc-section-number">2.1.3</span> Dynamic programming</a></li>
<li><a href="./BasicRL.html#sec:monte-carlo-sampling"><span class="toc-section-number">2.1.4</span> Monte-Carlo sampling</a></li>
<li><a href="./BasicRL.html#sec:temporal-difference"><span class="toc-section-number">2.1.5</span> Temporal Difference</a></li>
<li><a href="./BasicRL.html#sec:eligibility-traces"><span class="toc-section-number">2.1.6</span> Eligibility traces</a></li>
<li><a href="./BasicRL.html#sec:actor-critic-architectures"><span class="toc-section-number">2.1.7</span> Actor-critic architectures</a></li>
<li><a href="./BasicRL.html#sec:function-approximation"><span class="toc-section-number">2.1.8</span> Function approximation</a></li>
</ul></li>
<li><a href="./DeepLearning.html#sec:deep-learning"><span class="toc-section-number">2.2</span> Deep learning</a><ul>
<li><a href="./DeepLearning.html#sec:deep-neural-networks"><span class="toc-section-number">2.2.1</span> Deep neural networks</a></li>
<li><a href="./DeepLearning.html#sec:convolutional-networks"><span class="toc-section-number">2.2.2</span> Convolutional networks</a></li>
<li><a href="./DeepLearning.html#sec:recurrent-neural-networks"><span class="toc-section-number">2.2.3</span> Recurrent neural networks</a></li>
</ul></li>
</ul></li>
<li><a href="./Valuebased.html#sec:value-based-methods"><span class="toc-section-number">3</span> Value-based methods</a><ul>
<li><a href="./Valuebased.html#sec:limitations-of-deep-neural-networks-for-function-approximation"><span class="toc-section-number">3.1</span> Limitations of deep neural networks for function approximation</a></li>
<li><a href="./Valuebased.html#sec:deep-q-network-dqn"><span class="toc-section-number">3.2</span> Deep Q-Network (DQN)</a></li>
<li><a href="./Valuebased.html#sec:double-dqn"><span class="toc-section-number">3.3</span> Double DQN</a></li>
<li><a href="./Valuebased.html#sec:prioritized-experience-replay"><span class="toc-section-number">3.4</span> Prioritized experience replay</a></li>
<li><a href="./Valuebased.html#sec:duelling-network"><span class="toc-section-number">3.5</span> Duelling network</a></li>
<li><a href="./Valuebased.html#sec:distributed-dqn-gorila"><span class="toc-section-number">3.6</span> Distributed DQN (GORILA)</a></li>
<li><a href="./Valuebased.html#sec:deep-recurrent-q-learning-drqn"><span class="toc-section-number">3.7</span> Deep Recurrent Q-learning (DRQN)</a></li>
<li><a href="./Valuebased.html#sec:other-variants-of-dqn"><span class="toc-section-number">3.8</span> Other variants of DQN</a></li>
</ul></li>
<li><a href="./PolicyGradient.html#sec:policy-gradient-methods"><span class="toc-section-number">4</span> Policy Gradient methods</a><ul>
<li><a href="./PolicyGradient.html#sec:reinforce"><span class="toc-section-number">4.1</span> REINFORCE</a><ul>
<li><a href="./PolicyGradient.html#sec:estimating-the-policy-gradient"><span class="toc-section-number">4.1.1</span> Estimating the policy gradient</a></li>
<li><a href="./PolicyGradient.html#sec:reducing-the-variance"><span class="toc-section-number">4.1.2</span> Reducing the variance</a></li>
<li><a href="./PolicyGradient.html#sec:policy-gradient-theorem"><span class="toc-section-number">4.1.3</span> Policy Gradient theorem</a></li>
</ul></li>
<li><a href="./ActorCritic.html#sec:advantage-actor-critic-methods"><span class="toc-section-number">4.2</span> Advantage Actor-Critic methods</a><ul>
<li><a href="./ActorCritic.html#sec:advantage-actor-critic-a2c"><span class="toc-section-number">4.2.1</span> Advantage Actor-Critic (A2C)</a></li>
<li><a href="./ActorCritic.html#sec:asynchronous-advantage-actor-critic-a3c"><span class="toc-section-number">4.2.2</span> Asynchronous Advantage Actor-Critic (A3C)</a></li>
<li><a href="./ActorCritic.html#sec:generalized-advantage-estimation-gae"><span class="toc-section-number">4.2.3</span> Generalized Advantage Estimation (GAE)</a></li>
<li><a href="./ActorCritic.html#sec:stochastic-actor-critic-for-continuous-action-spaces"><span class="toc-section-number">4.2.4</span> Stochastic actor-critic for continuous action spaces</a></li>
</ul></li>
<li><a href="./ImportanceSampling.html#sec:off-policy-actor-critic"><span class="toc-section-number">4.3</span> Off-policy Actor-Critic</a><ul>
<li><a href="./ImportanceSampling.html#sec:importance-sampling"><span class="toc-section-number">4.3.1</span> Importance sampling</a></li>
<li><a href="./ImportanceSampling.html#sec:linear-off-policy-actor-critic-off-pac"><span class="toc-section-number">4.3.2</span> Linear Off-Policy Actor-Critic (Off-PAC)</a></li>
<li><a href="./ImportanceSampling.html#sec:retrace"><span class="toc-section-number">4.3.3</span> Retrace</a></li>
<li><a href="./ImportanceSampling.html#sec:self-imitation-learning-sil"><span class="toc-section-number">4.3.4</span> Self-Imitation Learning (SIL)</a></li>
</ul></li>
<li><a href="./DPG.html#sec:deterministic-policy-gradient-dpg"><span class="toc-section-number">4.4</span> Deterministic Policy Gradient (DPG)</a><ul>
<li><a href="./DPG.html#sec:deterministic-policy-gradient-theorem"><span class="toc-section-number">4.4.1</span> Deterministic policy gradient theorem</a></li>
<li><a href="./DPG.html#sec:deep-deterministic-policy-gradient-ddpg"><span class="toc-section-number">4.4.2</span> Deep Deterministic Policy Gradient (DDPG)</a></li>
<li><a href="./DPG.html#sec:distributed-distributional-ddpg-d4pg"><span class="toc-section-number">4.4.3</span> Distributed Distributional DDPG (D4PG)</a></li>
</ul></li>
<li><a href="./NaturalGradient.html#sec:natural-gradients"><span class="toc-section-number">4.5</span> Natural Gradients</a><ul>
<li><a href="./NaturalGradient.html#sec:principle-of-natural-gradients"><span class="toc-section-number">4.5.1</span> Principle of natural gradients</a></li>
<li><a href="./NaturalGradient.html#sec:natural-policy-gradient-and-natural-actor-critic-nac"><span class="toc-section-number">4.5.2</span> Natural policy gradient and Natural Actor Critic (NAC)</a></li>
<li><a href="./NaturalGradient.html#sec:trust-region-policy-optimization-trpo"><span class="toc-section-number">4.5.3</span> Trust Region Policy Optimization (TRPO)</a></li>
<li><a href="./NaturalGradient.html#sec:proximal-policy-optimization-ppo"><span class="toc-section-number">4.5.4</span> Proximal Policy Optimization (PPO)</a></li>
<li><a href="./NaturalGradient.html#sec:actor-critic-with-experience-replay-acer"><span class="toc-section-number">4.5.5</span> Actor-Critic with Experience Replay (ACER)</a></li>
</ul></li>
<li><a href="./EntropyRL.html#sec:maximum-entropy-rl"><span class="toc-section-number">4.6</span> Maximum Entropy RL</a><ul>
<li><a href="#sec:entropy-regularization-1"><span class="toc-section-number">4.6.1</span> Entropy regularization</a></li>
<li><a href="./EntropyRL.html#sec:soft-q-learning"><span class="toc-section-number">4.6.2</span> Soft Q-learning</a></li>
<li><a href="./EntropyRL.html#sec:soft-actor-critic-sac"><span class="toc-section-number">4.6.3</span> Soft Actor-Critic (SAC)</a></li>
</ul></li>
<li><a href="./DistributionalRL.html#sec:distributional-learning"><span class="toc-section-number">4.7</span> Distributional learning</a><ul>
<li><a href="./DistributionalRL.html#sec:categorical-dqn"><span class="toc-section-number">4.7.1</span> Categorical DQN</a></li>
<li><a href="./DistributionalRL.html#sec:the-reactor"><span class="toc-section-number">4.7.2</span> The Reactor</a></li>
</ul></li>
<li><a href="./OtherPolicyGradient.html#sec:miscellaneous-model-free-algorithm"><span class="toc-section-number">4.8</span> Miscellaneous model-free algorithm</a><ul>
<li><a href="./OtherPolicyGradient.html#sec:stochastic-value-gradient-svg"><span class="toc-section-number">4.8.1</span> Stochastic Value Gradient (SVG)</a></li>
<li><a href="./OtherPolicyGradient.html#sec:q-prop"><span class="toc-section-number">4.8.2</span> Q-Prop</a></li>
<li><a href="./OtherPolicyGradient.html#sec:normalized-advantage-function-naf"><span class="toc-section-number">4.8.3</span> Normalized Advantage Function (NAF)</a></li>
<li><a href="./OtherPolicyGradient.html#sec:fictitious-self-play-fsp"><span class="toc-section-number">4.8.4</span> Fictitious Self-Play (FSP)</a></li>
</ul></li>
<li><a href="./OtherPolicyGradient.html#sec:comparison-between-value-based-and-policy-gradient-methods"><span class="toc-section-number">4.9</span> Comparison between value-based and policy gradient methods</a></li>
<li><a href="./OtherPolicyGradient.html#sec:gradient-free-policy-search"><span class="toc-section-number">4.10</span> Gradient-free policy search</a><ul>
<li><a href="./OtherPolicyGradient.html#sec:cross-entropy-method-cem"><span class="toc-section-number">4.10.1</span> Cross-entropy Method (CEM)</a></li>
<li><a href="./OtherPolicyGradient.html#sec:evolutionary-search-es"><span class="toc-section-number">4.10.2</span> Evolutionary Search (ES)</a></li>
</ul></li>
</ul></li>
<li><a href="./ModelBased.html#sec:model-based-rl"><span class="toc-section-number">5</span> Model-based RL</a><ul>
<li><a href="./ModelBased.html#sec:dyna-q"><span class="toc-section-number">5.1</span> Dyna-Q</a></li>
<li><a href="./ModelBased.html#sec:unsorted-references"><span class="toc-section-number">5.2</span> Unsorted references</a></li>
</ul></li>
<li><a href="./Practice.html#sec:deep-rl-in-practice"><span class="toc-section-number">6</span> Deep RL in practice</a><ul>
<li><a href="./Practice.html#sec:limitations"><span class="toc-section-number">6.1</span> Limitations</a></li>
<li><a href="./Practice.html#sec:simulation-environments"><span class="toc-section-number">6.2</span> Simulation environments</a></li>
<li><a href="./Practice.html#sec:algorithm-implementations"><span class="toc-section-number">6.3</span> Algorithm implementations</a></li>
</ul></li>
<li><a href="./References.html#sec:references">References</a></li>
</ul>


</nav>



<h1 id="sec:references" class="unnumbered">References</h1>
<div id="refs" class="references" role="doc-bibliography">
<div id="ref-Amari1998">
<p>Amari, S.-I. (1998). Natural gradient works efficiently in learning. <em>Neural Computation</em> 10, 251–276.</p>
</div>
<div id="ref-Anschel2016">
<p>Anschel, O., Baram, N., and Shimkin, N. (2016). Averaged-DQN: Variance Reduction and Stabilization for Deep Reinforcement Learning. Available at: <a href="http://arxiv.org/abs/1611.01929">http://arxiv.org/abs/1611.01929</a>.</p>
</div>
<div id="ref-Arjovsky2017">
<p>Arjovsky, M., Chintala, S., and Bottou, L. (2017). Wasserstein GAN. Available at: <a href="http://arxiv.org/abs/1701.07875">http://arxiv.org/abs/1701.07875</a>.</p>
</div>
<div id="ref-Arulkumaran2017">
<p>Arulkumaran, K., Deisenroth, M. P., Brundage, M., and Bharath, A. A. (2017). A Brief Survey of Deep Reinforcement Learning. Available at: <a href="https://arxiv.org/pdf/1708.05866.pdf">https://arxiv.org/pdf/1708.05866.pdf</a>.</p>
</div>
<div id="ref-Baird1993">
<p>Baird, L. C. (1993). Advantage updating. Wright-Patterson Air Force Base Available at: <a href="http://leemon.com/papers/1993b.pdf">http://leemon.com/papers/1993b.pdf</a>.</p>
</div>
<div id="ref-Bakker2001">
<p>Bakker, B. (2001). Reinforcement Learning with Long Short-Term Memory. in <em>Advances in Neural Information Processing Systems 14 (NIPS 2001)</em>, 1475–1482. Available at: <a href="https://papers.nips.cc/paper/1953-reinforcement-learning-with-long-short-term-memory">https://papers.nips.cc/paper/1953-reinforcement-learning-with-long-short-term-memory</a>.</p>
</div>
<div id="ref-Barth-Maron2018">
<p>Barth-Maron, G., Hoffman, M. W., Budden, D., Dabney, W., Horgan, D., TB, D., et al. (2018). Distributed Distributional Deterministic Policy Gradients. Available at: <a href="http://arxiv.org/abs/1804.08617">http://arxiv.org/abs/1804.08617</a>.</p>
</div>
<div id="ref-Bellemare2017">
<p>Bellemare, M. G., Dabney, W., and Munos, R. (2017). A Distributional Perspective on Reinforcement Learning. Available at: <a href="http://arxiv.org/abs/1707.06887">http://arxiv.org/abs/1707.06887</a>.</p>
</div>
<div id="ref-Cho2014">
<p>Cho, K., van Merrienboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., et al. (2014). Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation. Available at: <a href="http://arxiv.org/abs/1406.1078">http://arxiv.org/abs/1406.1078</a>.</p>
</div>
<div id="ref-Chou2017">
<p>Chou, P.-W., Maturana, D., and Scherer, S. (2017). Improving Stochastic Policy Gradients in Continuous Control with Deep Reinforcement Learning using the Beta Distribution. in <em>International Conference on Machine Learning</em> Available at: <a href="http://proceedings.mlr.press/v70/chou17a/chou17a.pdf">http://proceedings.mlr.press/v70/chou17a/chou17a.pdf</a>.</p>
</div>
<div id="ref-Clavera2018">
<p>Clavera, I., Nagabandi, A., Fearing, R. S., Abbeel, P., Levine, S., and Finn, C. (2018). Learning to Adapt: Meta-Learning for Model-Based Control. Available at: <a href="http://arxiv.org/abs/1803.11347">http://arxiv.org/abs/1803.11347</a>.</p>
</div>
<div id="ref-Corneil2018">
<p>Corneil, D., Gerstner, W., and Brea, J. (2018). Efficient Model-Based Deep Reinforcement Learning with Variational State Tabulation. Available at: <a href="http://arxiv.org/abs/1802.04325">http://arxiv.org/abs/1802.04325</a>.</p>
</div>
<div id="ref-Dabney2017">
<p>Dabney, W., Rowland, M., Bellemare, M. G., and Munos, R. (2017). Distributional Reinforcement Learning with Quantile Regression. <em>arXiv:1710.10044 [cs, stat]</em>. Available at: <a href="http://arxiv.org/abs/1710.10044">http://arxiv.org/abs/1710.10044</a> [Accessed June 28, 2019].</p>
</div>
<div id="ref-Degris2012">
<p>Degris, T., White, M., and Sutton, R. S. (2012). Linear Off-Policy Actor-Critic. in <em>Proceedings of the 2012 International Conference on Machine Learning</em> Available at: <a href="http://arxiv.org/abs/1205.4839">http://arxiv.org/abs/1205.4839</a>.</p>
</div>
<div id="ref-Duan2016">
<p>Duan, Y., Chen, X., Houthooft, R., Schulman, J., and Abbeel, P. (2016). Benchmarking Deep Reinforcement Learning for Continuous Control. Available at: <a href="http://arxiv.org/abs/1604.06778">http://arxiv.org/abs/1604.06778</a>.</p>
</div>
<div id="ref-Feinberg2018">
<p>Feinberg, V., Wan, A., Stoica, I., Jordan, M. I., Gonzalez, J. E., and Levine, S. (2018). Model-Based Value Estimation for Efficient Model-Free Reinforcement Learning. Available at: <a href="http://arxiv.org/abs/1803.00101">http://arxiv.org/abs/1803.00101</a>.</p>
</div>
<div id="ref-Gers2001">
<p>Gers, F. (2001). Long Short-Term Memory in Recurrent Neural Networks. Available at: <a href="http://www.felixgers.de/papers/phd.pdf">http://www.felixgers.de/papers/phd.pdf</a>.</p>
</div>
<div id="ref-Goodfellow2016">
<p>Goodfellow, I., Bengio, Y., and Courville, A. (2016). <em>Deep Learning</em>. MIT Press Available at: <a href="http://www.deeplearningbook.org">http://www.deeplearningbook.org</a>.</p>
</div>
<div id="ref-Goodfellow2014">
<p>Goodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., et al. (2014). Generative Adversarial Networks. Available at: <a href="http://arxiv.org/abs/1406.2661">http://arxiv.org/abs/1406.2661</a>.</p>
</div>
<div id="ref-Goyal2018">
<p>Goyal, A., Brakel, P., Fedus, W., Lillicrap, T., Levine, S., Larochelle, H., et al. (2018). Recall Traces: Backtracking Models for Efficient Reinforcement Learning. Available at: <a href="http://arxiv.org/abs/1804.00379">http://arxiv.org/abs/1804.00379</a>.</p>
</div>
<div id="ref-Gruslys2017">
<p>Gruslys, A., Dabney, W., Azar, M. G., Piot, B., Bellemare, M., and Munos, R. (2017). The Reactor: A fast and sample-efficient Actor-Critic agent for Reinforcement Learning. Available at: <a href="http://arxiv.org/abs/1704.04651">http://arxiv.org/abs/1704.04651</a>.</p>
</div>
<div id="ref-Gu2017">
<p>Gu, S., Holly, E., Lillicrap, T., and Levine, S. (2017). Deep Reinforcement Learning for Robotic Manipulation with Asynchronous Off-Policy Updates. in <em>Proc. ICRA</em> Available at: <a href="http://arxiv.org/abs/1610.00633">http://arxiv.org/abs/1610.00633</a>.</p>
</div>
<div id="ref-Gu2016">
<p>Gu, S., Lillicrap, T., Ghahramani, Z., Turner, R. E., and Levine, S. (2016a). Q-Prop: Sample-Efficient Policy Gradient with An Off-Policy Critic. Available at: <a href="http://arxiv.org/abs/1611.02247">http://arxiv.org/abs/1611.02247</a>.</p>
</div>
<div id="ref-Gu2016a">
<p>Gu, S., Lillicrap, T., Sutskever, I., and Levine, S. (2016b). Continuous Deep Q-Learning with Model-based Acceleration. Available at: <a href="http://arxiv.org/abs/1603.00748">http://arxiv.org/abs/1603.00748</a>.</p>
</div>
<div id="ref-Ha2018">
<p>Ha, D., and Schmidhuber, J. (2018). World Models. doi:<a href="https://doi.org/10.5281/zenodo.1207631">10.5281/zenodo.1207631</a>.</p>
</div>
<div id="ref-Haarnoja2017">
<p>Haarnoja, T., Tang, H., Abbeel, P., and Levine, S. (2017). Reinforcement Learning with Deep Energy-Based Policies. <em>arXiv:1702.08165 [cs]</em>. Available at: <a href="http://arxiv.org/abs/1702.08165">http://arxiv.org/abs/1702.08165</a> [Accessed February 13, 2019].</p>
</div>
<div id="ref-Haarnoja2018a">
<p>Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S. (2018). Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor. Available at: <a href="http://arxiv.org/abs/1801.01290">http://arxiv.org/abs/1801.01290</a>.</p>
</div>
<div id="ref-Hafner2011">
<p>Hafner, R., and Riedmiller, M. (2011). Reinforcement learning in feedback control. <em>Machine Learning</em> 84, 137–169. doi:<a href="https://doi.org/10.1007/s10994-011-5235-x">10.1007/s10994-011-5235-x</a>.</p>
</div>
<div id="ref-Harutyunyan2016">
<p>Harutyunyan, A., Bellemare, M. G., Stepleton, T., and Munos, R. (2016). Q(<span class="math inline">\(\lambda\)</span>) with off-policy corrections. Available at: <a href="http://arxiv.org/abs/1602.04951">http://arxiv.org/abs/1602.04951</a>.</p>
</div>
<div id="ref-Hausknecht2015">
<p>Hausknecht, M., and Stone, P. (2015). Deep Recurrent Q-Learning for Partially Observable MDPs. Available at: <a href="http://arxiv.org/abs/1507.06527">http://arxiv.org/abs/1507.06527</a>.</p>
</div>
<div id="ref-He2016">
<p>He, F. S., Liu, Y., Schwing, A. G., and Peng, J. (2016). Learning to Play in a Day: Faster Deep Reinforcement Learning by Optimality Tightening. Available at: <a href="http://arxiv.org/abs/1611.01606">http://arxiv.org/abs/1611.01606</a>.</p>
</div>
<div id="ref-He2015">
<p>He, K., Zhang, X., Ren, S., and Sun, J. (2015). Deep Residual Learning for Image Recognition. Available at: <a href="http://arxiv.org/abs/1512.03385">http://arxiv.org/abs/1512.03385</a>.</p>
</div>
<div id="ref-Heess2015">
<p>Heess, N., Wayne, G., Silver, D., Lillicrap, T., Tassa, Y., and Erez, T. (2015). Learning continuous control policies by stochastic value gradients. <em>Proc. International Conference on Neural Information Processing Systems</em>, 2944–2952. Available at: <a href="http://dl.acm.org/citation.cfm?id=2969569">http://dl.acm.org/citation.cfm?id=2969569</a>.</p>
</div>
<div id="ref-Heinrich2015">
<p>Heinrich, J., Lanctot, M., and Silver, D. (2015). Fictitious Self-Play in Extensive-Form Games. 805–813. Available at: <a href="http://proceedings.mlr.press/v37/heinrich15.html">http://proceedings.mlr.press/v37/heinrich15.html</a>.</p>
</div>
<div id="ref-Heinrich2016">
<p>Heinrich, J., and Silver, D. (2016). Deep Reinforcement Learning from Self-Play in Imperfect-Information Games. Available at: <a href="http://arxiv.org/abs/1603.01121">http://arxiv.org/abs/1603.01121</a>.</p>
</div>
<div id="ref-Henaff2017">
<p>Henaff, M., Whitney, W. F., and LeCun, Y. (2017). Model-Based Planning with Discrete and Continuous Actions. Available at: <a href="http://arxiv.org/abs/1705.07177">http://arxiv.org/abs/1705.07177</a>.</p>
</div>
<div id="ref-Hessel2017">
<p>Hessel, M., Modayil, J., van Hasselt, H., Schaul, T., Ostrovski, G., Dabney, W., et al. (2017). Rainbow: Combining Improvements in Deep Reinforcement Learning. Available at: <a href="http://arxiv.org/abs/1710.02298">http://arxiv.org/abs/1710.02298</a>.</p>
</div>
<div id="ref-Hochreiter1991">
<p>Hochreiter, S. (1991). Untersuchungen zu dynamischen neuronalen Netzen. Available at: <a href="http://people.idsia.ch/~juergen/SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf">http://people.idsia.ch/~juergen/SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf</a>.</p>
</div>
<div id="ref-Hochreiter1997">
<p>Hochreiter, S., and Schmidhuber, J. (1997). Long short-term memory. <em>Neural computation</em> 9, 1735–80. Available at: <a href="http://www.ncbi.nlm.nih.gov/pubmed/9377276">http://www.ncbi.nlm.nih.gov/pubmed/9377276</a>.</p>
</div>
<div id="ref-Ioffe2015">
<p>Ioffe, S., and Szegedy, C. (2015). Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. Available at: <a href="http://arxiv.org/abs/1502.03167">http://arxiv.org/abs/1502.03167</a>.</p>
</div>
<div id="ref-Kakade2001">
<p>Kakade, S. (2001). A Natural Policy Gradient. in <em>Advances in Neural Information Processing Systems 14</em> Available at: <a href="https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf">https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf</a>.</p>
</div>
<div id="ref-Kakade2002">
<p>Kakade, S., and Langford, J. (2002). Approximately Optimal Approximate Reinforcement Learning. <em>Proc. 19th International Conference on Machine Learning</em>, 267–274. Available at: <a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.7.7601">http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.7.7601</a>.</p>
</div>
<div id="ref-Kansky2017">
<p>Kansky, K., Silver, T., Mély, D. A., Eldawy, M., Lázaro-Gredilla, M., Lou, X., et al. (2017). Schema Networks: Zero-shot Transfer with a Generative Causal Model of Intuitive Physics. <em>arXiv:1706.04317 [cs]</em>. Available at: <a href="http://arxiv.org/abs/1706.04317">http://arxiv.org/abs/1706.04317</a> [Accessed January 10, 2019].</p>
</div>
<div id="ref-Kingma2013">
<p>Kingma, D. P., and Welling, M. (2013). Auto-Encoding Variational Bayes. Available at: <a href="http://arxiv.org/abs/1312.6114">http://arxiv.org/abs/1312.6114</a>.</p>
</div>
<div id="ref-Knight2018">
<p>Knight, E., and Lerner, O. (2018). Natural Gradient Deep Q-learning. Available at: <a href="http://arxiv.org/abs/1803.07482">http://arxiv.org/abs/1803.07482</a>.</p>
</div>
<div id="ref-Krizhevsky2012">
<p>Krizhevsky, A., Sutskever, I., and Hinton, G. E. (2012). ImageNet Classification with Deep Convolutional Neural Networks. in <em>Advances in Neural Information Processing Systems (NIPS)</em> Available at: <a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf</a>.</p>
</div>
<div id="ref-Levine2013">
<p>Levine, S., and Koltun, V. (2013). Guided Policy Search. in <em>Proceedings of Machine Learning Research</em>, 1–9. Available at: <a href="http://proceedings.mlr.press/v28/levine13.html">http://proceedings.mlr.press/v28/levine13.html</a>.</p>
</div>
<div id="ref-Li2017">
<p>Li, Y. (2017). Deep Reinforcement Learning: An Overview. Available at: <a href="http://arxiv.org/abs/1701.07274">http://arxiv.org/abs/1701.07274</a>.</p>
</div>
<div id="ref-Lillicrap2015">
<p>Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., et al. (2015). Continuous control with deep reinforcement learning. <em>CoRR</em>. Available at: <a href="http://arxiv.org/abs/1509.02971">http://arxiv.org/abs/1509.02971</a>.</p>
</div>
<div id="ref-Lotzsch2017">
<p>Lötzsch, W., Vitay, J., and Hamker, F. H. (2017). Training a deep policy gradient-based neural network with asynchronous learners on a simulated robotic problem. in <em>INFORMATIK 2017. Gesellschaft für Informatik</em>, eds. M. Eibl and M. Gaedke (Gesellschaft für Informatik, Bonn), 2143–2154. Available at: <a href="https://dl.gi.de/handle/20.500.12116/3986">https://dl.gi.de/handle/20.500.12116/3986</a>.</p>
</div>
<div id="ref-Machado2018">
<p>Machado, M. C., Bellemare, M. G., and Bowling, M. (2018). Count-Based Exploration with the Successor Representation. <em>arXiv:1807.11622 [cs, stat]</em>. Available at: <a href="http://arxiv.org/abs/1807.11622">http://arxiv.org/abs/1807.11622</a> [Accessed February 23, 2019].</p>
</div>
<div id="ref-Meuleau2000">
<p>Meuleau, N., Peshkin, L., Kaelbling, L. P., and Kim, K.-e. (2000). Off-Policy Policy Search. Available at: <a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.28.894">http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.28.894</a>.</p>
</div>
<div id="ref-Mirowski2016">
<p>Mirowski, P., Pascanu, R., Viola, F., Soyer, H., Ballard, A. J., Banino, A., et al. (2016). Learning to Navigate in Complex Environments. Available at: <a href="http://arxiv.org/abs/1611.03673">http://arxiv.org/abs/1611.03673</a>.</p>
</div>
<div id="ref-Mnih2016">
<p>Mnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap, T. P., Harley, T., et al. (2016). Asynchronous Methods for Deep Reinforcement Learning. in <em>Proc. ICML</em> Available at: <a href="http://arxiv.org/abs/1602.01783">http://arxiv.org/abs/1602.01783</a>.</p>
</div>
<div id="ref-Mnih2013">
<p>Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., et al. (2013). Playing Atari with Deep Reinforcement Learning. Available at: <a href="http://arxiv.org/abs/1312.5602">http://arxiv.org/abs/1312.5602</a>.</p>
</div>
<div id="ref-Mnih2015">
<p>Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., et al. (2015). Human-level control through deep reinforcement learning. <em>Nature</em> 518, 529–533. doi:<a href="https://doi.org/10.1038/nature14236">10.1038/nature14236</a>.</p>
</div>
<div id="ref-Mousavi2018">
<p>Mousavi, S. S., Schukat, M., and Howley, E. (2018). “Deep Reinforcement Learning: An Overview,” in (Springer, Cham), 426–440. doi:<a href="https://doi.org/10.1007/978-3-319-56991-8_32">10.1007/978-3-319-56991-8_32</a>.</p>
</div>
<div id="ref-Munos2016">
<p>Munos, R., Stepleton, T., Harutyunyan, A., and Bellemare, M. G. (2016). Safe and Efficient Off-Policy Reinforcement Learning. Available at: <a href="http://arxiv.org/abs/1606.02647">http://arxiv.org/abs/1606.02647</a>.</p>
</div>
<div id="ref-Nachum2017">
<p>Nachum, O., Norouzi, M., Xu, K., and Schuurmans, D. (2017). Bridging the Gap Between Value and Policy Based Reinforcement Learning. <em>arXiv:1702.08892 [cs, stat]</em>. Available at: <a href="http://arxiv.org/abs/1702.08892">http://arxiv.org/abs/1702.08892</a> [Accessed June 12, 2019].</p>
</div>
<div id="ref-Nair2015">
<p>Nair, A., Srinivasan, P., Blackwell, S., Alcicek, C., Fearon, R., De Maria, A., et al. (2015). Massively Parallel Methods for Deep Reinforcement Learning. Available at: <a href="https://arxiv.org/pdf/1507.04296.pdf">https://arxiv.org/pdf/1507.04296.pdf</a>.</p>
</div>
<div id="ref-Nielsen2015">
<p>Nielsen, M. A. (2015). <em>Neural Networks and Deep Learning</em>. Determination Press Available at: <a href="http://neuralnetworksanddeeplearning.com/">http://neuralnetworksanddeeplearning.com/</a>.</p>
</div>
<div id="ref-Niu2011">
<p>Niu, F., Recht, B., Re, C., and Wright, S. J. (2011). HOGWILD!: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent. in <em>Proc. Advances in Neural Information Processing Systems</em>, 21–21. Available at: <a href="http://arxiv.org/abs/1106.5730">http://arxiv.org/abs/1106.5730</a>.</p>
</div>
<div id="ref-ODonoghue2016">
<p>O’Donoghue, B., Munos, R., Kavukcuoglu, K., and Mnih, V. (2016). Combining policy gradient and Q-learning. <em>arXiv:1611.01626 [cs, math, stat]</em>. Available at: <a href="http://arxiv.org/abs/1611.01626">http://arxiv.org/abs/1611.01626</a> [Accessed February 13, 2019].</p>
</div>
<div id="ref-Oh2018">
<p>Oh, J., Guo, Y., Singh, S., and Lee, H. (2018). Self-Imitation Learning. Available at: <a href="http://arxiv.org/abs/1806.05635">http://arxiv.org/abs/1806.05635</a>.</p>
</div>
<div id="ref-Pardo2018">
<p>Pardo, F., Levdik, V., and Kormushev, P. (2018). Q-map: A Convolutional Approach for Goal-Oriented Reinforcement Learning. Available at: <a href="http://arxiv.org/abs/1810.02927">http://arxiv.org/abs/1810.02927</a>.</p>
</div>
<div id="ref-Pascanu2013">
<p>Pascanu, R., and Bengio, Y. (2013). Revisiting Natural Gradient for Deep Networks. Available at: <a href="https://arxiv.org/abs/1301.3584">https://arxiv.org/abs/1301.3584</a>.</p>
</div>
<div id="ref-Peng2018">
<p>Peng, B., Li, X., Gao, J., Liu, J., Wong, K.-F., and Su, S.-Y. (2018). Deep Dyna-Q: Integrating Planning for Task-Completion Dialogue Policy Learning. Available at: <a href="http://arxiv.org/abs/1801.06176">http://arxiv.org/abs/1801.06176</a>.</p>
</div>
<div id="ref-Peshkin2002">
<p>Peshkin, L., and Shelton, C. R. (2002). Learning from Scarce Experience. Available at: <a href="http://arxiv.org/abs/cs/0204043">http://arxiv.org/abs/cs/0204043</a>.</p>
</div>
<div id="ref-Peters2008">
<p>Peters, J., and Schaal, S. (2008). Reinforcement learning of motor skills with policy gradients. <em>Neural Networks</em> 21, 682–697. doi:<a href="https://doi.org/10.1016/j.neunet.2008.02.003">10.1016/j.neunet.2008.02.003</a>.</p>
</div>
<div id="ref-Pong2018">
<p>Pong, V., Gu, S., Dalal, M., and Levine, S. (2018). Temporal Difference Models: Model-Free Deep RL for Model-Based Control. Available at: <a href="http://arxiv.org/abs/1802.09081">http://arxiv.org/abs/1802.09081</a>.</p>
</div>
<div id="ref-Popov2017">
<p>Popov, I., Heess, N., Lillicrap, T., Hafner, R., Barth-Maron, G., Vecerik, M., et al. (2017). Data-efficient Deep Reinforcement Learning for Dexterous Manipulation. Available at: <a href="http://arxiv.org/abs/1704.03073">http://arxiv.org/abs/1704.03073</a>.</p>
</div>
<div id="ref-Precup2000">
<p>Precup, D., Sutton, R. S., and Singh, S. (2000). Eligibility traces for off-policy policy evaluation. in <em>Proceedings of the Seventeenth International Conference on Machine Learning.</em></p>
</div>
<div id="ref-Ruder2016">
<p>Ruder, S. (2016). An overview of gradient descent optimization algorithms. Available at: <a href="http://arxiv.org/abs/1609.04747">http://arxiv.org/abs/1609.04747</a>.</p>
</div>
<div id="ref-Salimans2017">
<p>Salimans, T., Ho, J., Chen, X., Sidor, S., and Sutskever, I. (2017). Evolution Strategies as a Scalable Alternative to Reinforcement Learning. Available at: <a href="http://arxiv.org/abs/1703.03864">http://arxiv.org/abs/1703.03864</a>.</p>
</div>
<div id="ref-Schaul2015">
<p>Schaul, T., Quan, J., Antonoglou, I., and Silver, D. (2015). Prioritized Experience Replay. Available at: <a href="http://arxiv.org/abs/1511.05952">http://arxiv.org/abs/1511.05952</a>.</p>
</div>
<div id="ref-Schulman2017a">
<p>Schulman, J., Chen, X., and Abbeel, P. (2017a). Equivalence Between Policy Gradients and Soft Q-Learning. <em>arXiv:1704.06440 [cs]</em>. Available at: <a href="http://arxiv.org/abs/1704.06440">http://arxiv.org/abs/1704.06440</a> [Accessed June 12, 2019].</p>
</div>
<div id="ref-Schulman2015">
<p>Schulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, P. (2015a). Trust Region Policy Optimization. 1889–1897. Available at: <a href="http://proceedings.mlr.press/v37/schulman15.html">http://proceedings.mlr.press/v37/schulman15.html</a>.</p>
</div>
<div id="ref-Schulman2015a">
<p>Schulman, J., Moritz, P., Levine, S., Jordan, M., and Abbeel, P. (2015b). High-Dimensional Continuous Control Using Generalized Advantage Estimation. Available at: <a href="http://arxiv.org/abs/1506.02438">http://arxiv.org/abs/1506.02438</a>.</p>
</div>
<div id="ref-Schulman2017">
<p>Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. (2017b). Proximal Policy Optimization Algorithms. Available at: <a href="http://arxiv.org/abs/1707.06347">http://arxiv.org/abs/1707.06347</a>.</p>
</div>
<div id="ref-Silver2016">
<p>Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., Van Den Driessche, G., et al. (2016a). Mastering the game of Go with deep neural networks and tree search. <em>Nature</em>. doi:<a href="https://doi.org/10.1038/nature16961">10.1038/nature16961</a>.</p>
</div>
<div id="ref-Silver2014">
<p>Silver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., and Riedmiller, M. (2014). Deterministic Policy Gradient Algorithms. in <em>Proc. ICML</em> Proceedings of Machine Learning Research., eds. E. P. Xing and T. Jebara (PMLR), 387–395. Available at: <a href="http://proceedings.mlr.press/v32/silver14.html">http://proceedings.mlr.press/v32/silver14.html</a>.</p>
</div>
<div id="ref-Silver2016a">
<p>Silver, D., van Hasselt, H., Hessel, M., Schaul, T., Guez, A., Harley, T., et al. (2016b). The Predictron: End-To-End Learning and Planning. Available at: <a href="http://arxiv.org/abs/1612.08810">http://arxiv.org/abs/1612.08810</a>.</p>
</div>
<div id="ref-Simonyan2015">
<p>Simonyan, K., and Zisserman, A. (2015). Very Deep Convolutional Networks for Large-Scale Image Recognition. <em>International Conference on Learning Representations (ICRL)</em>, 1–14. doi:<a href="https://doi.org/10.1016/j.infsof.2008.09.005">10.1016/j.infsof.2008.09.005</a>.</p>
</div>
<div id="ref-Srinivas2018">
<p>Srinivas, A., Jabri, A., Abbeel, P., Levine, S., and Finn, C. (2018). Universal Planning Networks. Available at: <a href="http://arxiv.org/abs/1804.00645">http://arxiv.org/abs/1804.00645</a>.</p>
</div>
<div id="ref-Sutton1990a">
<p>Sutton, R. S. (1990). Integrated Architectures for Learning, Planning, and Reacting Based on Approximating Dynamic Programming. <em>Machine Learning Proceedings 1990</em>, 216–224. doi:<a href="https://doi.org/10.1016/B978-1-55860-141-3.50030-4">10.1016/B978-1-55860-141-3.50030-4</a>.</p>
</div>
<div id="ref-Sutton1998">
<p>Sutton, R. S., and Barto, A. G. (1998). <em>Reinforcement Learning: An introduction</em>. Cambridge, MA: MIT press.</p>
</div>
<div id="ref-Sutton2017">
<p>Sutton, R. S., and Barto, A. G. (2017). <em>Reinforcement Learning: An Introduction</em>. 2nd ed. Cambridge, MA: MIT Press Available at: <a href="http://incompleteideas.net/book/the-book-2nd.html">http://incompleteideas.net/book/the-book-2nd.html</a>.</p>
</div>
<div id="ref-Sutton1999">
<p>Sutton, R. S., McAllester, D., Singh, S., and Mansour, Y. (1999). Policy gradient methods for reinforcement learning with function approximation. in <em>Proceedings of the 12th International Conference on Neural Information Processing Systems</em> (MIT Press), 1057–1063. Available at: <a href="https://dl.acm.org/citation.cfm?id=3009806">https://dl.acm.org/citation.cfm?id=3009806</a>.</p>
</div>
<div id="ref-Szita2006">
<p>Szita, I., and Lörincz, A. (2006). Learning Tetris Using the Noisy Cross-Entropy Method. <em>Neural Computation</em> 18, 2936–2941. doi:<a href="https://doi.org/10.1162/neco.2006.18.12.2936">10.1162/neco.2006.18.12.2936</a>.</p>
</div>
<div id="ref-Tang2010">
<p>Tang, J., and Abbeel, P. (2010). On a Connection between Importance Sampling and the Likelihood Ratio Policy Gradient. in <em>Adv. Neural inf. Process. Syst.</em> Available at: <a href="http://rll.berkeley.edu/~jietang/pubs/nips10_Tang.pdf">http://rll.berkeley.edu/~jietang/pubs/nips10_Tang.pdf</a>.</p>
</div>
<div id="ref-Todorov2008">
<p>Todorov, E. (2008). General duality between optimal control and estimation. in <em>2008 47th IEEE Conference on Decision and Control</em>, 4286–4292. doi:<a href="https://doi.org/10.1109/CDC.2008.4739438">10.1109/CDC.2008.4739438</a>.</p>
</div>
<div id="ref-Toussaint2009">
<p>Toussaint, M. (2009). Robot Trajectory Optimization Using Approximate Inference. in <em>Proceedings of the 26th Annual International Conference on Machine Learning</em> ICML ’09. (New York, NY, USA: ACM), 1049–1056. doi:<a href="https://doi.org/10.1145/1553374.1553508">10.1145/1553374.1553508</a>.</p>
</div>
<div id="ref-Uhlenbeck1930">
<p>Uhlenbeck, G. E., and Ornstein, L. S. (1930). On the Theory of the Brownian Motion. <em>Physical Review</em> 36. doi:<a href="https://doi.org/10.1103/PhysRev.36.823">10.1103/PhysRev.36.823</a>.</p>
</div>
<div id="ref-vanHasselt2010">
<p>van Hasselt, H. (2010). Double Q-learning. in <em>Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2</em> (Curran Associates Inc.), 2613–2621. Available at: <a href="https://dl.acm.org/citation.cfm?id=2997187">https://dl.acm.org/citation.cfm?id=2997187</a>.</p>
</div>
<div id="ref-vanHasselt2015">
<p>van Hasselt, H., Guez, A., and Silver, D. (2015). Deep Reinforcement Learning with Double Q-learning. Available at: <a href="http://arxiv.org/abs/1509.06461">http://arxiv.org/abs/1509.06461</a>.</p>
</div>
<div id="ref-Wang2017">
<p>Wang, Z., Bapst, V., Heess, N., Mnih, V., Munos, R., Kavukcuoglu, K., et al. (2017). Sample Efficient Actor-Critic with Experience Replay. Available at: <a href="http://arxiv.org/abs/1611.01224">http://arxiv.org/abs/1611.01224</a>.</p>
</div>
<div id="ref-Wang2016">
<p>Wang, Z., Schaul, T., Hessel, M., van Hasselt, H., Lanctot, M., and de Freitas, N. (2016). Dueling Network Architectures for Deep Reinforcement Learning. in <em>Proc. ICML</em> Available at: <a href="http://arxiv.org/abs/1511.06581">http://arxiv.org/abs/1511.06581</a>.</p>
</div>
<div id="ref-Watkins1989">
<p>Watkins, C. J. (1989). Learning from delayed rewards.</p>
</div>
<div id="ref-Watter2015">
<p>Watter, M., Springenberg, J. T., Boedecker, J., and Riedmiller, M. (2015). Embed to Control: A Locally Linear Latent Dynamics Model for Control from Raw Images. Available at: <a href="https://arxiv.org/pdf/1506.07365.pdf">https://arxiv.org/pdf/1506.07365.pdf</a>.</p>
</div>
<div id="ref-Weber2017">
<p>Weber, T., Racanière, S., Reichert, D. P., Buesing, L., Guez, A., Rezende, D. J., et al. (2017). Imagination-Augmented Agents for Deep Reinforcement Learning. Available at: <a href="http://arxiv.org/abs/1707.06203">http://arxiv.org/abs/1707.06203</a>.</p>
</div>
<div id="ref-Wierstra2007">
<p>Wierstra, D., Foerster, A., Peters, J., and Schmidhuber, J. (2007). “Solving Deep Memory POMDPs with Recurrent Policy Gradients,” in (Springer, Berlin, Heidelberg), 697–706. doi:<a href="https://doi.org/10.1007/978-3-540-74690-4_71">10.1007/978-3-540-74690-4_71</a>.</p>
</div>
<div id="ref-Williams1992">
<p>Williams, R. J. (1992). Simple statistical gradient-following algorithms for connectionist reinforcement learning. <em>Machine Learning</em> 8, 229–256.</p>
</div>
<div id="ref-Williams1991">
<p>Williams, R. J., and Peng, J. (1991). Function optimization using connectionist reinforcement learning algorithms. <em>Connection Science</em> 3, 241–268.</p>
</div>
<div id="ref-Ziebart2008">
<p>Ziebart, B. D., Maas, A., Bagnell, J. A., and Dey, A. K. (2008). Maximum Entropy Inverse Reinforcement Learning. in <em>AAAI Conference on Artificial Intelligence</em>, 6.</p>
</div>
</div>

<br>
<div class="arrows">
<a href="Practice.html" class="previous">&laquo; Previous</a>
<a href="#" class="next">Next &raquo;</a>
</div>


</article>
</body>
</html>
