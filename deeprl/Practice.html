<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <meta name="author" content="Julien Vitay">
  <title>Deep Reinforcement Learning</title>


  <link rel="stylesheet" href="assets/syntax.css">
  <link rel="stylesheet" href="assets/github.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->

</head>

<body class="markdown-body">

<header>
  <h1 class="title"><strong>Deep Reinforcement Learning</strong></h1>
  <p class="author">Julien Vitay - <a href="mailto:julien.vitay@informatik.tu-chemnitz.de">julien.vitay@informatik.tu-chemnitz.de</a></p>
</header>


<article>

<nav id="TOC" class ="toc">
  <h2><strong>Deep Reinforcement Learning</strong></h2>
  <p class="author">Julien Vitay</p>

<ul>
<li><a href="./Introduction.html#sec:introduction"><span class="toc-section-number">1</span> Introduction</a></li>
<li><a href="./BasicRL.html#sec:basics"><span class="toc-section-number">2</span> Basics</a><ul>
<li><a href="./BasicRL.html#sec:reinforcement-learning-and-markov-decision-process"><span class="toc-section-number">2.1</span> Reinforcement learning and Markov Decision Process</a><ul>
<li><a href="./BasicRL.html#sec:policy-and-value-functions"><span class="toc-section-number">2.1.1</span> Policy and value functions</a></li>
<li><a href="./BasicRL.html#sec:bellman-equations"><span class="toc-section-number">2.1.2</span> Bellman equations</a></li>
<li><a href="./BasicRL.html#sec:dynamic-programming"><span class="toc-section-number">2.1.3</span> Dynamic programming</a></li>
<li><a href="./BasicRL.html#sec:monte-carlo-sampling"><span class="toc-section-number">2.1.4</span> Monte-Carlo sampling</a></li>
<li><a href="./BasicRL.html#sec:temporal-difference"><span class="toc-section-number">2.1.5</span> Temporal Difference</a></li>
<li><a href="./BasicRL.html#sec:eligibility-traces"><span class="toc-section-number">2.1.6</span> Eligibility traces</a></li>
<li><a href="./BasicRL.html#sec:actor-critic-architectures"><span class="toc-section-number">2.1.7</span> Actor-critic architectures</a></li>
<li><a href="./BasicRL.html#sec:function-approximation"><span class="toc-section-number">2.1.8</span> Function approximation</a></li>
</ul></li>
<li><a href="./DeepLearning.html#sec:deep-learning"><span class="toc-section-number">2.2</span> Deep learning</a><ul>
<li><a href="./DeepLearning.html#sec:deep-neural-networks"><span class="toc-section-number">2.2.1</span> Deep neural networks</a></li>
<li><a href="./DeepLearning.html#sec:convolutional-networks"><span class="toc-section-number">2.2.2</span> Convolutional networks</a></li>
<li><a href="./DeepLearning.html#sec:recurrent-neural-networks"><span class="toc-section-number">2.2.3</span> Recurrent neural networks</a></li>
</ul></li>
</ul></li>
<li><a href="./Valuebased.html#sec:value-based-methods"><span class="toc-section-number">3</span> Value-based methods</a><ul>
<li><a href="./Valuebased.html#sec:limitations-of-deep-neural-networks-for-function-approximation"><span class="toc-section-number">3.1</span> Limitations of deep neural networks for function approximation</a></li>
<li><a href="./Valuebased.html#sec:deep-q-network-dqn"><span class="toc-section-number">3.2</span> Deep Q-Network (DQN)</a></li>
<li><a href="./Valuebased.html#sec:double-dqn"><span class="toc-section-number">3.3</span> Double DQN</a></li>
<li><a href="./Valuebased.html#sec:prioritized-experience-replay"><span class="toc-section-number">3.4</span> Prioritized experience replay</a></li>
<li><a href="./Valuebased.html#sec:duelling-network"><span class="toc-section-number">3.5</span> Duelling network</a></li>
<li><a href="./Valuebased.html#sec:distributed-dqn-gorila"><span class="toc-section-number">3.6</span> Distributed DQN (GORILA)</a></li>
<li><a href="./Valuebased.html#sec:deep-recurrent-q-learning-drqn"><span class="toc-section-number">3.7</span> Deep Recurrent Q-learning (DRQN)</a></li>
<li><a href="./Valuebased.html#sec:other-variants-of-dqn"><span class="toc-section-number">3.8</span> Other variants of DQN</a></li>
</ul></li>
<li><a href="./PolicyGradient.html#sec:policy-gradient-methods"><span class="toc-section-number">4</span> Policy Gradient methods</a><ul>
<li><a href="./PolicyGradient.html#sec:reinforce"><span class="toc-section-number">4.1</span> REINFORCE</a><ul>
<li><a href="./PolicyGradient.html#sec:estimating-the-policy-gradient"><span class="toc-section-number">4.1.1</span> Estimating the policy gradient</a></li>
<li><a href="./PolicyGradient.html#sec:reducing-the-variance"><span class="toc-section-number">4.1.2</span> Reducing the variance</a></li>
<li><a href="./PolicyGradient.html#sec:policy-gradient-theorem"><span class="toc-section-number">4.1.3</span> Policy Gradient theorem</a></li>
</ul></li>
<li><a href="./ActorCritic.html#sec:advantage-actor-critic-methods"><span class="toc-section-number">4.2</span> Advantage Actor-Critic methods</a><ul>
<li><a href="./ActorCritic.html#sec:advantage-actor-critic-a2c"><span class="toc-section-number">4.2.1</span> Advantage Actor-Critic (A2C)</a></li>
<li><a href="./ActorCritic.html#sec:asynchronous-advantage-actor-critic-a3c"><span class="toc-section-number">4.2.2</span> Asynchronous Advantage Actor-Critic (A3C)</a></li>
<li><a href="./ActorCritic.html#sec:generalized-advantage-estimation-gae"><span class="toc-section-number">4.2.3</span> Generalized Advantage Estimation (GAE)</a></li>
<li><a href="./ActorCritic.html#sec:stochastic-actor-critic-for-continuous-action-spaces"><span class="toc-section-number">4.2.4</span> Stochastic actor-critic for continuous action spaces</a></li>
</ul></li>
<li><a href="./ImportanceSampling.html#sec:off-policy-actor-critic"><span class="toc-section-number">4.3</span> Off-policy Actor-Critic</a><ul>
<li><a href="./ImportanceSampling.html#sec:importance-sampling"><span class="toc-section-number">4.3.1</span> Importance sampling</a></li>
<li><a href="./ImportanceSampling.html#sec:linear-off-policy-actor-critic-off-pac"><span class="toc-section-number">4.3.2</span> Linear Off-Policy Actor-Critic (Off-PAC)</a></li>
<li><a href="./ImportanceSampling.html#sec:retrace"><span class="toc-section-number">4.3.3</span> Retrace</a></li>
<li><a href="./ImportanceSampling.html#sec:self-imitation-learning-sil"><span class="toc-section-number">4.3.4</span> Self-Imitation Learning (SIL)</a></li>
</ul></li>
<li><a href="./DPG.html#sec:deterministic-policy-gradient-dpg"><span class="toc-section-number">4.4</span> Deterministic Policy Gradient (DPG)</a><ul>
<li><a href="./DPG.html#sec:deterministic-policy-gradient-theorem"><span class="toc-section-number">4.4.1</span> Deterministic policy gradient theorem</a></li>
<li><a href="./DPG.html#sec:deep-deterministic-policy-gradient-ddpg"><span class="toc-section-number">4.4.2</span> Deep Deterministic Policy Gradient (DDPG)</a></li>
<li><a href="./DPG.html#sec:distributed-distributional-ddpg-d4pg"><span class="toc-section-number">4.4.3</span> Distributed Distributional DDPG (D4PG)</a></li>
</ul></li>
<li><a href="./NaturalGradient.html#sec:natural-gradients"><span class="toc-section-number">4.5</span> Natural Gradients</a><ul>
<li><a href="./NaturalGradient.html#sec:principle-of-natural-gradients"><span class="toc-section-number">4.5.1</span> Principle of natural gradients</a></li>
<li><a href="./NaturalGradient.html#sec:natural-policy-gradient-and-natural-actor-critic-nac"><span class="toc-section-number">4.5.2</span> Natural policy gradient and Natural Actor Critic (NAC)</a></li>
<li><a href="./NaturalGradient.html#sec:trust-region-policy-optimization-trpo"><span class="toc-section-number">4.5.3</span> Trust Region Policy Optimization (TRPO)</a></li>
<li><a href="./NaturalGradient.html#sec:proximal-policy-optimization-ppo"><span class="toc-section-number">4.5.4</span> Proximal Policy Optimization (PPO)</a></li>
<li><a href="./NaturalGradient.html#sec:actor-critic-with-experience-replay-acer"><span class="toc-section-number">4.5.5</span> Actor-Critic with Experience Replay (ACER)</a></li>
</ul></li>
<li><a href="./OtherPolicyGradient.html#sec:maximum-entropy-rl"><span class="toc-section-number">4.6</span> Maximum Entropy RL</a><ul>
<li><a href="#sec:entropy-regularization-1"><span class="toc-section-number">4.6.1</span> Entropy regularization</a></li>
<li><a href="./OtherPolicyGradient.html#sec:soft-actor-critic-sac"><span class="toc-section-number">4.6.2</span> Soft Actor-Critic (SAC)</a></li>
</ul></li>
<li><a href="./OtherPolicyGradient.html#sec:distributional-learning"><span class="toc-section-number">4.7</span> Distributional learning</a><ul>
<li><a href="./OtherPolicyGradient.html#sec:the-reactor"><span class="toc-section-number">4.7.1</span> The Reactor</a></li>
</ul></li>
<li><a href="./OtherPolicyGradient.html#sec:other-policy-search-methods"><span class="toc-section-number">4.8</span> Other policy search methods</a><ul>
<li><a href="./OtherPolicyGradient.html#sec:stochastic-value-gradient-svg"><span class="toc-section-number">4.8.1</span> Stochastic Value Gradient (SVG)</a></li>
<li><a href="./OtherPolicyGradient.html#sec:q-prop"><span class="toc-section-number">4.8.2</span> Q-Prop</a></li>
<li><a href="./OtherPolicyGradient.html#sec:normalized-advantage-function-naf"><span class="toc-section-number">4.8.3</span> Normalized Advantage Function (NAF)</a></li>
<li><a href="./OtherPolicyGradient.html#sec:fictitious-self-play-fsp"><span class="toc-section-number">4.8.4</span> Fictitious Self-Play (FSP)</a></li>
</ul></li>
<li><a href="./OtherPolicyGradient.html#sec:comparison-between-value-based-and-policy-gradient-methods"><span class="toc-section-number">4.9</span> Comparison between value-based and policy gradient methods</a></li>
<li><a href="./OtherPolicyGradient.html#sec:gradient-free-policy-search"><span class="toc-section-number">4.10</span> Gradient-free policy search</a><ul>
<li><a href="./OtherPolicyGradient.html#sec:cross-entropy-method-cem"><span class="toc-section-number">4.10.1</span> Cross-entropy Method (CEM)</a></li>
<li><a href="./OtherPolicyGradient.html#sec:evolutionary-search-es"><span class="toc-section-number">4.10.2</span> Evolutionary Search (ES)</a></li>
</ul></li>
</ul></li>
<li><a href="./Practice.html#sec:deep-rl-in-practice"><span class="toc-section-number">5</span> Deep RL in practice</a><ul>
<li><a href="./Practice.html#sec:limitations"><span class="toc-section-number">5.1</span> Limitations</a></li>
<li><a href="./Practice.html#sec:reward-shaping"><span class="toc-section-number">5.2</span> Reward shaping</a></li>
<li><a href="./Practice.html#sec:simulation-environments"><span class="toc-section-number">5.3</span> Simulation environments</a></li>
<li><a href="./Practice.html#sec:algorithm-implementations"><span class="toc-section-number">5.4</span> Algorithm implementations</a></li>
</ul></li>
<li><a href="./References.html#sec:references">References</a></li>
</ul>


</nav>



<h1 id="sec:deep-rl-in-practice"><span class="header-section-number">5</span> Deep RL in practice</h1>
<h2 id="sec:limitations"><span class="header-section-number">5.1</span> Limitations</h2>
<p>Excellent blog post from Alex Irpan on the limitations of deep RL: <a href="https://www.alexirpan.com/2018/02/14/rl-hard.html" class="uri">https://www.alexirpan.com/2018/02/14/rl-hard.html</a></p>
<p>Another documented critic on deep RL: <a href="https://thegradient.pub/why-rl-is-flawed/" class="uri">https://thegradient.pub/why-rl-is-flawed/</a></p>
<h2 id="sec:reward-shaping"><span class="header-section-number">5.2</span> Reward shaping</h2>
<p>Hindsight experience replay: <span class="citation" data-cites="Andrychowicz2017">Andrychowicz et al. (<a href="References.html#ref-Andrychowicz2017">2017</a>)</span></p>
<h2 id="sec:simulation-environments"><span class="header-section-number">5.3</span> Simulation environments</h2>
<p>Standard RL environments are needed to better compare the performance of RL algoritms. Below is a list of the most popular ones.</p>
<ul>
<li>OpenAI Gym <a href="https://gym.openai.com" class="uri">https://gym.openai.com</a>: a standard toolkit for comparing RL algorithms provided by the OpenAI fundation. It provides many environments, from the classical toy problems in RL (GridWorld, pole-balancing) to more advanced problems (Mujoco simulated robots, Atari games, Minecraft…). The main advantage is the simplicity of the interface: the user only needs to select which task he wants to solve, and a simple for loop allows to perform actions and observe their consequences:</li>
</ul>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb1-1" title="1"><span class="im">import</span> gym</a>
<a class="sourceLine" id="cb1-2" title="2">env <span class="op">=</span> gym.make(<span class="st">&quot;Taxi-v1&quot;</span>)</a>
<a class="sourceLine" id="cb1-3" title="3">observation <span class="op">=</span> env.reset()</a>
<a class="sourceLine" id="cb1-4" title="4"><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1000</span>):</a>
<a class="sourceLine" id="cb1-5" title="5">    env.render()</a>
<a class="sourceLine" id="cb1-6" title="6">    action <span class="op">=</span> env.action_space.sample()</a>
<a class="sourceLine" id="cb1-7" title="7">    observation, reward, done, info <span class="op">=</span> env.step(action)</a></code></pre></div>
<ul>
<li><p>OpenAI Universe <a href="https://universe.openai.com" class="uri">https://universe.openai.com</a>: a similar framework from OpenAI, but to control realistic video games (GTA V, etc).</p></li>
<li><p>Darts environment <a href="https://github.com/DartEnv/dart-env" class="uri">https://github.com/DartEnv/dart-env</a>: a fork of gym to use the Darts simulator instead of Mujoco.</p></li>
<li><p>Roboschool <a href="https://github.com/openai/roboschool" class="uri">https://github.com/openai/roboschool</a>: another alternative to Mujoco for continuous robotic control, this time from openAI.</p></li>
<li><p>NIPS 2017 musculoskettal challenge <a href="https://github.com/stanfordnmbl/osim-rl" class="uri">https://github.com/stanfordnmbl/osim-rl</a></p></li>
</ul>
<h2 id="sec:algorithm-implementations"><span class="header-section-number">5.4</span> Algorithm implementations</h2>
<p>State-of-the-art algorithms in deep RL are already implemented and freely available on the internet. Below is a preliminary list of the most popular ones. Most of them rely on tensorflow or keras for training the neural networks and interact directly with gym-like interfaces.</p>
<ul>
<li><p><code>rl-code</code> <a href="https://github.com/rlcode/reinforcement-learning" class="uri">https://github.com/rlcode/reinforcement-learning</a>: many code samples for simple RL problems (GridWorld, Cartpole, Atari Games). The code samples are mostly for educational purpose (Policy Iteration, Value Iteration, Monte-Carlo, SARSA, Q-learning, REINFORCE, DQN, A2C, A3C).</p></li>
<li><p><code>keras-rl</code> <a href="https://github.com/matthiasplappert/keras-rl" class="uri">https://github.com/matthiasplappert/keras-rl</a>: many deep RL algorithms implemented directly in keras: DQN, DDQN, DDPG, Continuous DQN (CDQN or NAF), Cross-Entropy Method (CEM), Dueling DQN, Deep SARSA…</p></li>
<li><p><code>Coach</code> <a href="https://github.com/NervanaSystems/coach" class="uri">https://github.com/NervanaSystems/coach</a> from Intel Nervana also provides many state-of-the-art algorithms: DQN, DDQN, Dueling DQN, Mixed Monte Carlo (MMC), Persistent Advantage Learning (PAL), Distributional Deep Q Network, Bootstrapped Deep Q Network, N-Step Q Learning, Neural Episodic Control (NEC), Normalized Advantage Functions (NAF), Policy Gradients (PG), A3C, DDPG, Proximal Policy Optimization (PPO), Clipped Proximal Policy Optimization, Direct Future Prediction (DFP)…</p></li>
<li><p><code>OpenAI Baselines</code> <a href="https://github.com/openai/baselines" class="uri">https://github.com/openai/baselines</a> from OpenAI too: A2C, ACER, ACKTR, DDPG, DQN, PPO, TRPO…</p></li>
<li><p><code>rlkit</code> <a href="https://github.com/vitchyr/rlkit" class="uri">https://github.com/vitchyr/rlkit</a> from Vitchyr Pong (PhD student at Berkeley) with in particular model-based algorithms (TDM <span class="citation" data-cites="Pong2018">Pong et al. (<a href="References.html#ref-Pong2018">2018</a>)</span>).</p></li>
<li><p><code>chainer-rl</code> <a href="https://github.com/chainer/chainerrl" class="uri">https://github.com/chainer/chainerrl</a> implemented in Chainer (an alternative to tensorflow): A3C, ACER, Categorical DQN; DQN (including Double DQN, Persistent Advantage Learning (PAL), Double PAL, Dynamic Policy Programming (DPP)), DDPG, , PGT (Policy Gradient Theorem), PCL (Path Consistency Learning), PPO, TRPO.</p></li>
</ul>

<br>
<div class="arrows">
<a href="OtherPolicyGradient.html" class="previous">&laquo; Previous</a>
<a href="References.html" class="next">Next &raquo;</a>
</div>


</article>
</body>
</html>
