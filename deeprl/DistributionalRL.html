<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <meta name="author" content="Julien Vitay">
  <title>Deep Reinforcement Learning</title>


  <link rel="stylesheet" href="assets/syntax.css">
  <link rel="stylesheet" href="assets/menu.css">
  <link rel="stylesheet" href="assets/github.css">
  <script src="assets/menu.js" type="text/javascript"></script>
  <script src="/usr/share/mathjax2/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->

</head>

<body class="markdown-body">

<div class="navbar">
  <div class="dropdown" id="dropdown">
    <button class="dropbtn">
      <div class="dropicon-first"></div>
      <div class="dropicon"></div>
      <div class="dropicon"></div>
      <i class="fa fa-caret-down"></i>
    </button> 
    <span class="navbar-title">Deep Reinforcement Learning - Julien Vitay</span>
    <div class="dropdown-content">

<ul>
<li><a href="./Introduction.html#sec:introduction"><span class="toc-section-number">1</span> Introduction</a></li>
<li><a href="./BasicRL.html#sec:basics"><span class="toc-section-number">2</span> Basics</a><ul>
<li><a href="./BasicRL.html#sec:reinforcement-learning-and-markov-decision-process"><span class="toc-section-number">2.1</span> Reinforcement learning and Markov Decision Process</a><ul>
<li><a href="./BasicRL.html#sec:policy-and-value-functions"><span class="toc-section-number">2.1.1</span> Policy and value functions</a></li>
<li><a href="./BasicRL.html#sec:bellman-equations"><span class="toc-section-number">2.1.2</span> Bellman equations</a></li>
<li><a href="./BasicRL.html#sec:dynamic-programming"><span class="toc-section-number">2.1.3</span> Dynamic programming</a></li>
<li><a href="./BasicRL.html#sec:monte-carlo-sampling"><span class="toc-section-number">2.1.4</span> Monte-Carlo sampling</a></li>
<li><a href="./BasicRL.html#sec:temporal-difference"><span class="toc-section-number">2.1.5</span> Temporal Difference</a></li>
<li><a href="./BasicRL.html#sec:eligibility-traces"><span class="toc-section-number">2.1.6</span> Eligibility traces</a></li>
<li><a href="./BasicRL.html#sec:actor-critic-architectures"><span class="toc-section-number">2.1.7</span> Actor-critic architectures</a></li>
<li><a href="./BasicRL.html#sec:function-approximation"><span class="toc-section-number">2.1.8</span> Function approximation</a></li>
</ul></li>
<li><a href="./DeepLearning.html#sec:deep-learning"><span class="toc-section-number">2.2</span> Deep learning</a><ul>
<li><a href="./DeepLearning.html#sec:deep-neural-networks"><span class="toc-section-number">2.2.1</span> Deep neural networks</a></li>
<li><a href="./DeepLearning.html#sec:convolutional-networks"><span class="toc-section-number">2.2.2</span> Convolutional networks</a></li>
<li><a href="./DeepLearning.html#sec:recurrent-neural-networks"><span class="toc-section-number">2.2.3</span> Recurrent neural networks</a></li>
</ul></li>
</ul></li>
<li><a href="./Valuebased.html#sec:value-based-methods"><span class="toc-section-number">3</span> Value-based methods</a><ul>
<li><a href="./Valuebased.html#sec:limitations-of-deep-neural-networks-for-function-approximation"><span class="toc-section-number">3.1</span> Limitations of deep neural networks for function approximation</a></li>
<li><a href="./Valuebased.html#sec:deep-q-network-dqn"><span class="toc-section-number">3.2</span> Deep Q-Network (DQN)</a></li>
<li><a href="./Valuebased.html#sec:double-dqn"><span class="toc-section-number">3.3</span> Double DQN</a></li>
<li><a href="./Valuebased.html#sec:prioritized-experience-replay"><span class="toc-section-number">3.4</span> Prioritized experience replay</a></li>
<li><a href="./Valuebased.html#sec:duelling-network"><span class="toc-section-number">3.5</span> Duelling network</a></li>
<li><a href="./Valuebased.html#sec:distributed-dqn-gorila"><span class="toc-section-number">3.6</span> Distributed DQN (GORILA)</a></li>
<li><a href="./Valuebased.html#sec:deep-recurrent-q-learning-drqn"><span class="toc-section-number">3.7</span> Deep Recurrent Q-learning (DRQN)</a></li>
<li><a href="./Valuebased.html#sec:other-variants-of-dqn"><span class="toc-section-number">3.8</span> Other variants of DQN</a></li>
</ul></li>
<li><a href="./PolicyGradient.html#sec:policy-gradient-methods"><span class="toc-section-number">4</span> Policy Gradient methods</a><ul>
<li><a href="./PolicyGradient.html#sec:reinforce"><span class="toc-section-number">4.1</span> REINFORCE</a><ul>
<li><a href="./PolicyGradient.html#sec:estimating-the-policy-gradient"><span class="toc-section-number">4.1.1</span> Estimating the policy gradient</a></li>
<li><a href="./PolicyGradient.html#sec:reducing-the-variance"><span class="toc-section-number">4.1.2</span> Reducing the variance</a></li>
<li><a href="./PolicyGradient.html#sec:policy-gradient-theorem"><span class="toc-section-number">4.1.3</span> Policy Gradient theorem</a></li>
</ul></li>
<li><a href="./ActorCritic.html#sec:advantage-actor-critic-methods"><span class="toc-section-number">4.2</span> Advantage Actor-Critic methods</a><ul>
<li><a href="./ActorCritic.html#sec:advantage-actor-critic-a2c"><span class="toc-section-number">4.2.1</span> Advantage Actor-Critic (A2C)</a></li>
<li><a href="./ActorCritic.html#sec:asynchronous-advantage-actor-critic-a3c"><span class="toc-section-number">4.2.2</span> Asynchronous Advantage Actor-Critic (A3C)</a></li>
<li><a href="./ActorCritic.html#sec:generalized-advantage-estimation-gae"><span class="toc-section-number">4.2.3</span> Generalized Advantage Estimation (GAE)</a></li>
<li><a href="./ActorCritic.html#sec:stochastic-actor-critic-for-continuous-action-spaces"><span class="toc-section-number">4.2.4</span> Stochastic actor-critic for continuous action spaces</a></li>
</ul></li>
<li><a href="./ImportanceSampling.html#sec:off-policy-actor-critic"><span class="toc-section-number">4.3</span> Off-policy Actor-Critic</a><ul>
<li><a href="./ImportanceSampling.html#sec:importance-sampling"><span class="toc-section-number">4.3.1</span> Importance sampling</a></li>
<li><a href="./ImportanceSampling.html#sec:linear-off-policy-actor-critic-off-pac"><span class="toc-section-number">4.3.2</span> Linear Off-Policy Actor-Critic (Off-PAC)</a></li>
<li><a href="./ImportanceSampling.html#sec:retrace"><span class="toc-section-number">4.3.3</span> Retrace</a></li>
<li><a href="./ImportanceSampling.html#sec:self-imitation-learning-sil"><span class="toc-section-number">4.3.4</span> Self-Imitation Learning (SIL)</a></li>
</ul></li>
<li><a href="./DPG.html#sec:deterministic-policy-gradient-dpg"><span class="toc-section-number">4.4</span> Deterministic Policy Gradient (DPG)</a><ul>
<li><a href="./DPG.html#sec:deterministic-policy-gradient-theorem"><span class="toc-section-number">4.4.1</span> Deterministic policy gradient theorem</a></li>
<li><a href="./DPG.html#sec:deep-deterministic-policy-gradient-ddpg"><span class="toc-section-number">4.4.2</span> Deep Deterministic Policy Gradient (DDPG)</a></li>
<li><a href="./DPG.html#sec:distributed-distributional-ddpg-d4pg"><span class="toc-section-number">4.4.3</span> Distributed Distributional DDPG (D4PG)</a></li>
</ul></li>
<li><a href="./NaturalGradient.html#sec:natural-gradients"><span class="toc-section-number">4.5</span> Natural Gradients</a><ul>
<li><a href="./NaturalGradient.html#sec:principle-of-natural-gradients"><span class="toc-section-number">4.5.1</span> Principle of natural gradients</a></li>
<li><a href="./NaturalGradient.html#sec:natural-policy-gradient-and-natural-actor-critic-nac"><span class="toc-section-number">4.5.2</span> Natural policy gradient and Natural Actor Critic (NAC)</a></li>
<li><a href="./NaturalGradient.html#sec:trust-region-policy-optimization-trpo"><span class="toc-section-number">4.5.3</span> Trust Region Policy Optimization (TRPO)</a></li>
<li><a href="./NaturalGradient.html#sec:proximal-policy-optimization-ppo"><span class="toc-section-number">4.5.4</span> Proximal Policy Optimization (PPO)</a></li>
<li><a href="./NaturalGradient.html#sec:actor-critic-with-experience-replay-acer"><span class="toc-section-number">4.5.5</span> Actor-Critic with Experience Replay (ACER)</a></li>
</ul></li>
<li><a href="./EntropyRL.html#sec:maximum-entropy-rl"><span class="toc-section-number">4.6</span> Maximum Entropy RL</a><ul>
<li><a href="#sec:entropy-regularization-1"><span class="toc-section-number">4.6.1</span> Entropy regularization</a></li>
<li><a href="./EntropyRL.html#sec:soft-q-learning"><span class="toc-section-number">4.6.2</span> Soft Q-learning</a></li>
<li><a href="./EntropyRL.html#sec:soft-actor-critic-sac"><span class="toc-section-number">4.6.3</span> Soft Actor-Critic (SAC)</a></li>
</ul></li>
<li><a href="./DistributionalRL.html#sec:distributional-learning"><span class="toc-section-number">4.7</span> Distributional learning</a><ul>
<li><a href="./DistributionalRL.html#sec:categorical-dqn"><span class="toc-section-number">4.7.1</span> Categorical DQN</a></li>
<li><a href="./DistributionalRL.html#sec:the-reactor"><span class="toc-section-number">4.7.2</span> The Reactor</a></li>
</ul></li>
<li><a href="./OtherPolicyGradient.html#sec:miscellaneous-model-free-algorithm"><span class="toc-section-number">4.8</span> Miscellaneous model-free algorithm</a><ul>
<li><a href="./OtherPolicyGradient.html#sec:stochastic-value-gradient-svg"><span class="toc-section-number">4.8.1</span> Stochastic Value Gradient (SVG)</a></li>
<li><a href="./OtherPolicyGradient.html#sec:q-prop"><span class="toc-section-number">4.8.2</span> Q-Prop</a></li>
<li><a href="./OtherPolicyGradient.html#sec:normalized-advantage-function-naf"><span class="toc-section-number">4.8.3</span> Normalized Advantage Function (NAF)</a></li>
<li><a href="./OtherPolicyGradient.html#sec:fictitious-self-play-fsp"><span class="toc-section-number">4.8.4</span> Fictitious Self-Play (FSP)</a></li>
</ul></li>
<li><a href="./OtherPolicyGradient.html#sec:comparison-between-value-based-and-policy-gradient-methods"><span class="toc-section-number">4.9</span> Comparison between value-based and policy gradient methods</a></li>
<li><a href="./OtherPolicyGradient.html#sec:gradient-free-policy-search"><span class="toc-section-number">4.10</span> Gradient-free policy search</a><ul>
<li><a href="./OtherPolicyGradient.html#sec:cross-entropy-method-cem"><span class="toc-section-number">4.10.1</span> Cross-entropy Method (CEM)</a></li>
<li><a href="./OtherPolicyGradient.html#sec:evolutionary-search-es"><span class="toc-section-number">4.10.2</span> Evolutionary Search (ES)</a></li>
</ul></li>
</ul></li>
<li><a href="./ModelBased.html#sec:model-based-rl"><span class="toc-section-number">5</span> Model-based RL</a><ul>
<li><a href="./ModelBased.html#sec:dyna-q"><span class="toc-section-number">5.1</span> Dyna-Q</a></li>
<li><a href="./ModelBased.html#sec:unsorted-references"><span class="toc-section-number">5.2</span> Unsorted references</a></li>
</ul></li>
<li><a href="./Practice.html#sec:deep-rl-in-practice"><span class="toc-section-number">6</span> Deep RL in practice</a><ul>
<li><a href="./Practice.html#sec:limitations"><span class="toc-section-number">6.1</span> Limitations</a></li>
<li><a href="./Practice.html#sec:simulation-environments"><span class="toc-section-number">6.2</span> Simulation environments</a></li>
<li><a href="./Practice.html#sec:algorithm-implementations"><span class="toc-section-number">6.3</span> Algorithm implementations</a></li>
</ul></li>
<li><a href="./References.html#sec:references">References</a></li>
</ul>


    </div>
  </div>
</div>

<article>

<h2 id="sec:distributional-learning"><span class="header-section-number">4.7</span> Distributional learning</h2>
<p>All RL methods based on the Bellman equations use the expectation operator to average returns and compute the values of states and actions:</p>
<p><span class="math display">\[
    Q^\pi(s, a) = \mathbb{E}_{s, a \sim \pi}[R(s, a)]
\]</span></p>
<p>The variance of the returns is not considered in the action selection scheme, and most methods actually try to reduce this variance as it impairs the convergence of neural networks. Decision theory states that only the mean should matter on the long-term, but one can imagine tasks where the variance is an important factor for the decision. Imagine you are in a game where you have two actions available: the first one brings returns of 10 and 20, with a probability of 0.5 each (to simplify), while the second one brings returns of -10 and +40 with probability 0.5 too. Both actions have the same Q-value of 15 (a return which is actually never experienced), so one can theoretically pick whatever action, both are optimal in the Bellman’s sense.</p>
<p>However, this is only true when playing <strong>long enough</strong>. If, after learning, one is only allowed one try on that game, it is obviously safer (but less fun) to choose the first action, as one wins at worse 10, while it is -10 with the second action. Knowing the distribution of the returns can allow to distinguish risky choices from safe ones more easily and adapt the behavior. Another advantage would be that by learning the distribution of the returns instead of just their mean, one actually gathers more information about the environment dynamics: it can only help the convergence of the algorithm towards the optimal policy.</p>
<h3 id="sec:categorical-dqn"><span class="header-section-number">4.7.1</span> Categorical DQN</h3>
<p><span class="citation" data-cites="Bellemare2017">Bellemare et al. (<a href="References.html#ref-Bellemare2017" role="doc-biblioref">2017</a>)</span> proposed to learn the <strong>value distribution</strong> (the probability distribution of the returns) through a modification of the Bellman equation. They show that learning the complete distribution of rewards instead of their mean leads to performance improvements on Atari games over modern variants of DQN.</p>
<p>Their proposed <strong>categorical DQN</strong> (also called C51) has an architecture based on DQN, but where the output layer predicts the distribution of the returns for each action <span class="math inline">\(a\)</span> in state <span class="math inline">\(s\)</span>, instead of its mean <span class="math inline">\(Q^\pi(s, a)\)</span>. In practice, each action <span class="math inline">\(a\)</span> is represented by <span class="math inline">\(N\)</span> output neurons, who encode the support of the distribution of returns. If the returns take values between <span class="math inline">\(V_\text{min}\)</span> and <span class="math inline">\(V_\text{max}\)</span>, one can represent their distribution <span class="math inline">\(\mathcal{Z}\)</span> by taking <span class="math inline">\(N\)</span> discrete “bins” (called <em>atoms</em> in the paper) in that range. Fig. <a href="#fig:distributionallearning">34</a> shows how the distribution of returns between -10 and 10 can be represented using 21 atoms.</p>
<figure>
<img src="img/distributionallearning.png" id="fig:distributionallearning" style="width:80.0%" alt="" /><figcaption>Figure 34: Example of a value distribution using 21 atoms between -10 and 10. The average return is 3, but its variance is explicitly represented.</figcaption>
</figure>
<p>Of course, the main problem is to know in advance the range of returns <span class="math inline">\([V_\text{min}, V_\text{max}]\)</span> (it depends largely on the choice of the discount rate <span class="math inline">\(\gamma\)</span>), but you can infer it from training another algorithm such as DQN beforehand. <span class="citation" data-cites="Dabney2017">Dabney et al. (<a href="References.html#ref-Dabney2017" role="doc-biblioref">2017</a>)</span> got rid of this problem with quantile regression. In the paper, the authors found out experimentally that 51 is the most efficient number of atoms (hence the name C51).</p>
<p>Let’s note <span class="math inline">\(z_i\)</span> these atoms with <span class="math inline">\(1 \leq i &lt; N\)</span>. The atom probability that the return associated to a state-action pair <span class="math inline">\((s, a)\)</span> lies within the bin associated to the atom <span class="math inline">\(z_i\)</span> is noted <span class="math inline">\(p_i(s, a)\)</span>. These probabilities can be predicted by a neural network, typically by using a softmax function over outputs <span class="math inline">\(f_i(s, a; \theta)\)</span>:</p>
<p><span class="math display">\[
    p_i(s, a; \theta) = \frac{\exp f_i(s, a; \theta)}{\sum_{j=1}^{N} \exp f_j(s, a; \theta)}
\]</span></p>
<p>The distribution of the returns <span class="math inline">\(\mathcal{Z}\)</span> is simply a sum over the atoms (represented by the Dirac distribution <span class="math inline">\(\delta_{z_i}\)</span>):</p>
<p><span class="math display">\[
    \mathcal{Z}_\theta(s, a) = \sum_{i=1}^{N} p_i(s, a; \theta) \, \delta_{z_i}
\]</span></p>
<p>If these probabilities are correctly estimated, the Q-value is easy to compute as the mean of the distribution:</p>
<p><span class="math display">\[
    Q_\theta(s, a) = \mathbb{E} [\mathcal{Z}_\theta(s, a)] = \sum_{i=1}^{N} p_i(s, a; \theta) \, z_i
\]</span></p>
<p>These Q-values can then be used for action selection as in the regular DQN. The problem is now to learn the value distribution <span class="math inline">\(\mathcal{Z}_\theta\)</span>, i.e. to find a learning rule / loss function for the <span class="math inline">\(p_i(s, a; \theta)\)</span>. Let’s consider a single transition <span class="math inline">\((s, a, r, s&#39;)\)</span> and select the greedy action <span class="math inline">\(a&#39;\)</span> in <span class="math inline">\(s&#39;\)</span> using the current policy <span class="math inline">\(\pi_\theta\)</span>. The value distribution <span class="math inline">\(\mathcal{Z}_\theta\)</span> can be evaluated by applying recursively the Bellman operator <span class="math inline">\(\mathcal{T}\)</span>:</p>
<p><span class="math display">\[
    \mathcal{T} \, \mathcal{Z}_\theta(s, a) = \mathcal{R}(s, a) + \gamma \, \mathcal{Z}_\theta(s&#39;, a&#39;)
\]</span></p>
<p>where <span class="math inline">\(\mathcal{R}(s, a)\)</span> is the distribution of immediate rewards after <span class="math inline">\((s, a)\)</span>. This use of the Bellman operator is the same as in Q-learning:</p>
<p><span class="math display">\[
    \mathcal{T} \, \mathcal{Q}_\theta(s, a) = \mathbb{E}[r(s, a)] + \gamma \, \mathcal{Q}_\theta(s&#39;, a&#39;)
\]</span></p>
<p>In Q-learning, one minimizes the difference (mse) between <span class="math inline">\(\mathcal{T} \, \mathcal{Q}_\theta(s, a)\)</span> and <span class="math inline">\(\mathcal{Q}_\theta(s, a)\)</span>, which are expectations (so we only manipulate scalars). Here, we will minimize the statistical distance between the distributions <span class="math inline">\(\mathcal{T} \, \mathcal{Z}_\theta(s, a)\)</span> and <span class="math inline">\(\mathcal{Z}_\theta(s, a)\)</span> themselves, using for example the KL divergence, Wasserstein metric, total variation or whatnot.</p>
<p>The problem is mostly that the distributions <span class="math inline">\(\mathcal{T} \, \mathcal{Z}_\theta(s, a)\)</span> and <span class="math inline">\(\mathcal{Z}_\theta(s, a)\)</span> do not have the same support: for a particular atom <span class="math inline">\(z_i\)</span>, <span class="math inline">\(\mathcal{T} \, \mathcal{Z}_\theta(s, a)\)</span> can have a non-zero probability <span class="math inline">\(p_i(s, a)\)</span>, while <span class="math inline">\(\mathcal{Z}_\theta(s, a)\)</span> has a zero probability. Besides, the probabilities must sum to 1, so one cannot update the <span class="math inline">\(z_i\)</span> independently from one another.</p>
<p>The proposed method consists of three steps:</p>
<ol type="1">
<li>Computation of the Bellman update <span class="math inline">\(\mathcal{T} \, \mathcal{Z}_\theta(s, a)\)</span>. They simply compute translated values for each <span class="math inline">\(z_i\)</span> according to:</li>
</ol>
<p><span class="math display">\[
    \mathcal{T} \, z_i = r + \gamma \, z_i
\]</span></p>
<p>and clip the obtained value to <span class="math inline">\([V_\text{min}, V_\text{max}]\)</span>. The reward <span class="math inline">\(r\)</span> translates the distribution of atoms, while the discount rate <span class="math inline">\(\gamma\)</span> scales it. Fig. <a href="#fig:distributionallearning2">35</a> shows the distribution of <span class="math inline">\(\mathcal{T} \, \mathcal{Z}_\theta(s, a)\)</span> compared to <span class="math inline">\(\mathcal{Z}_\theta(s, a)\)</span>. Note that the atoms of the two distributions are not aligned.</p>
<figure>
<img src="img/distributionallearning2.png" id="fig:distributionallearning2" style="width:80.0%" alt="" /><figcaption>Figure 35: Computation of the Bellman update <span class="math inline">\(\mathcal{T} \, \mathcal{Z}_\theta(s, a)\)</span>. The atoms of the two distributions are not aligned.</figcaption>
</figure>
<ol start="2" type="1">
<li>Distribution of the probabilities of <span class="math inline">\(\mathcal{T} \, \mathcal{Z}_\theta(s, a)\)</span> on the support of <span class="math inline">\(\mathcal{Z}_\theta(s, a)\)</span>. The projected atom <span class="math inline">\(\mathcal{T} \, z_i\)</span> lie between two “real” atoms <span class="math inline">\(z_l\)</span> and <span class="math inline">\(z_u\)</span>, with a non-integer index <span class="math inline">\(b\)</span> (for example <span class="math inline">\(b = 3.4\)</span>, <span class="math inline">\(l = 3\)</span> and <span class="math inline">\(u=4\)</span>). The corresponding probability <span class="math inline">\(p_{b}(s&#39;, a&#39;; \theta)\)</span> of the next greedy action <span class="math inline">\((s&#39;, a&#39;)\)</span> is “spread” to its neighbors through a local interpolation depending on the distances between <span class="math inline">\(b\)</span>, <span class="math inline">\(l\)</span> and <span class="math inline">\(u\)</span>:</li>
</ol>
<p><span class="math display">\[
    \Delta p_{l}(s&#39;, a&#39;; \theta) = p_{b}(s&#39;, a&#39;; \theta) \, (b - u)
\]</span> <span class="math display">\[
    \Delta p_{u}(s&#39;, a&#39;; \theta) = p_{b}(s&#39;, a&#39;; \theta) \, (l - b)
\]</span></p>
<p>Fig. <a href="#fig:distributionallearning3">36</a> shows how the projected update distribution <span class="math inline">\(\Phi \, \mathcal{T} \, \mathcal{Z}_\theta(s, a)\)</span> now matches the support of <span class="math inline">\(\mathcal{Z}_\theta(s, a)\)</span></p>
<figure>
<img src="img/distributionallearning3.png" id="fig:distributionallearning3" style="width:80.0%" alt="" /><figcaption>Figure 36: Projected update <span class="math inline">\(\Phi \, \mathcal{T} \, \mathcal{Z}_\theta(s, a)\)</span> on the support of <span class="math inline">\(\mathcal{Z}_\theta(s, a)\)</span>. The atoms are now aligned, the statistical distance between the two distributions can be minimized.</figcaption>
</figure>
<p>The projection of the Bellman update onto an atom <span class="math inline">\(z_i\)</span> can be summarized by the following equation:</p>
<p><span class="math display">\[
    (\Phi \, \mathcal{T} \, \mathcal{Z}_\theta(s, a))_i = \sum_{j=1}^N \big [1 - \frac{| [\mathcal{T}\, z_j]_{V_\text{min}}^{V_\text{max}} - z_i|}{\Delta z} \big ]_0^1 \, p_j (s&#39;, a&#39;; \theta)
\]</span></p>
<p>where <span class="math inline">\([\cdot]_a^b\)</span> bounds its argument in <span class="math inline">\([a, b]\)</span> and <span class="math inline">\(\Delta z\)</span> is the step size between two atoms.</p>
<ol start="3" type="1">
<li>Minimizing the statistical distance between <span class="math inline">\(\Phi \, \mathcal{T} \, \mathcal{Z}_\theta(s, a)\)</span> and <span class="math inline">\(\mathcal{Z}_\theta(s, a)\)</span>. Now that the Bellman update has the same support as the value distribution, we can minimize the KL divergence between the two for a single transition:</li>
</ol>
<p><span class="math display">\[
    \mathcal{L}(\theta) = D_\text{KL} (\Phi \, \mathcal{T} \, \mathcal{Z}_{\theta&#39;}(s, a) | \mathcal{Z}_\theta(s, a))
\]</span></p>
<p>using a target network <span class="math inline">\(\theta&#39;\)</span> for the target. It is to be noted that minimizing the KL divergence is the same as minimizing the cross-entropy between the two, as in classification tasks:</p>
<p><span class="math display">\[
    \mathcal{L}(\theta) =  - \sum_i (\Phi \, \mathcal{T} \, \mathcal{Z}_{\theta&#39;}(s, a))_i \log p_i (s, a; \theta)
\]</span></p>
<p>The projected Bellman update plays the role of the one-hot encoded target vector in classification (except that it is not one-hot encoded). DQN performs a regression on the Q-values (mse loss), while categorical DQN performs a classification (cross-entropy loss). Apart from the way the target is computed, categorical DQN is very similar to DQN: architecture, experience replay memory, target networks, etc.</p>
<p>Fig. <a href="#fig:categoricaldqn">37</a> illustrates how the predicted value distribution changes when playing Space invaders (also have a look at the Youtube video at <a href="https://www.youtube.com/watch?v=yFBwyPuO2Vg" class="uri">https://www.youtube.com/watch?v=yFBwyPuO2Vg</a>). C51 outperforms DQN on most Atari games, both in terms of the achieved performance and the sample complexity.</p>
<figure>
<img src="img/categoricaldqn.gif" id="fig:categoricaldqn" style="width:100.0%" alt="" /><figcaption>Figure 37: Evolution of the value distribution for the categorical DQN playing Space Invaders. Animation taken from <a href="https://deepmind.com/blog/going-beyond-average-reinforcement-learning/" class="uri">https://deepmind.com/blog/going-beyond-average-reinforcement-learning/</a></figcaption>
</figure>
<iframe width="600" height="300" src="https://www.youtube.com/embed/yFBwyPuO2Vg" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
</iframe>
<p><strong>Additional resources:</strong></p>
<ul>
<li><a href="https://deepmind.com/blog/going-beyond-average-reinforcement-learning" class="uri">https://deepmind.com/blog/going-beyond-average-reinforcement-learning</a></li>
<li><a href="https://physai.sciencesconf.org/data/pages/distributional_RL_Remi_Munos.pdf" class="uri">https://physai.sciencesconf.org/data/pages/distributional_RL_Remi_Munos.pdf</a></li>
<li><a href="https://flyyufelix.github.io/2017/10/24/distributional-bellman.html" class="uri">https://flyyufelix.github.io/2017/10/24/distributional-bellman.html</a>, with keras code for C51.</li>
</ul>
<h3 id="sec:the-reactor"><span class="header-section-number">4.7.2</span> The Reactor</h3>
<p>The <strong>Reactor</strong> (Retrace Actor) of <span class="citation" data-cites="Gruslys2017">Gruslys et al. (<a href="References.html#ref-Gruslys2017" role="doc-biblioref">2017</a>)</span> combines many architectural and algorithmic contributions seen until now in order to provide an algorithm that is both sample efficient and with a good run-time performance. A3C has for example a better run-time performance (smaller wall-clock time for the training) than DQN or categorical DQN thanks to the use of multiple actor-learners in parallel, but its sample complexity is actually higher (as it is on-policy).</p>
<p>The Reactor combines and improves on:</p>
<ul>
<li>An actor-critic architecture using policy gradient with importance sampling (Section <a href="./ImportanceSampling.html#sec:importance-sampling">4.3.1</a>),</li>
<li>Off-policy corrected returns computed by the Retrace algorithm (Section <a href="./ImportanceSampling.html#sec:retrace">4.3.3</a>),</li>
<li>Distributional learning of the Q-values in the critic (Section <a href="./DistributionalRL.html#sec:categorical-dqn">4.7.1</a>),</li>
<li>Prioritized experience replay for sequences (Section <a href="./Valuebased.html#sec:prioritized-experience-replay">3.4</a>).</li>
</ul>
<p>One could consider REACTOR as the distributional version of ACER (Section <a href="./NaturalGradient.html#sec:actor-critic-with-experience-replay-acer">4.5.5</a>, <span class="citation" data-cites="Wang2017">Wang et al. (<a href="References.html#ref-Wang2017" role="doc-biblioref">2017</a>)</span>). We will not go into all the details here, but simply outline the main novelties.</p>
<p>The Reactor is composed of an actor <span class="math inline">\(\pi_\theta(s, a)\)</span> and a critic <span class="math inline">\(Q_\varphi(s, a)\)</span>. The actor is trained using policy gradient with importance sampling, as in Off-PAC (Section <a href="./ImportanceSampling.html#sec:linear-off-policy-actor-critic-off-pac">4.3.2</a>). For a single state <span class="math inline">\(s\)</span> and an action <span class="math inline">\(\hat{a}\)</span> sampled by the behavior policy <span class="math inline">\(b\)</span>, the gradient of the objective is defined as:</p>
<p><span class="math display">\[
\begin{aligned}
    \nabla_\theta J(\theta) = \frac{\pi_\theta(s, \hat{a})}{b(s, \hat{a})} &amp; \, (R(s, \hat{a}) - Q_\varphi(s, \hat{a})) \, \nabla_\theta \log \pi_\theta(s, \hat{a}) \\
    &amp; + \sum_a Q_\varphi(s, a) \, \nabla_\theta \pi_\theta(s, a) \\
\end{aligned}
\]</span></p>
<p>The first term comes from Off-PAC and only concerns the chosen action <span class="math inline">\(\hat{a}\)</span> from the behavior policy. The actual return <span class="math inline">\(R(s, a)\)</span> is compared to its estimate <span class="math inline">\(Q_\varphi(s, \hat{a})\)</span> in order to reduce its variance. The second term <span class="math inline">\(\sum_a Q_\varphi(s, a) \, \nabla_\theta \pi_\theta(s, a)\)</span> depends on all available actions in <span class="math inline">\(s\)</span>. Its role is to reduce the bias of the first term, without adding any variance as it is only based on estimates. As the value of the state is defined by <span class="math inline">\(V^\pi(s) = \sum_a \pi(s, a) \, Q^\pi(s, a)\)</span>, maximizing this term maximizes the value of the state, i.e. the associated returns. This rule is called <strong>leave-one-out</strong> (LOO), as one action is left out from the sum and estimated from actual returns instead of other estimates.</p>
<p>For a better control on the variance, the behavior probability <span class="math inline">\(b(s, a)\)</span> is replaced by a parameter <span class="math inline">\(\beta\)</span>:</p>
<p><span class="math display">\[
    \nabla_\theta J(\theta) = \beta \, (R(s, \hat{a}) - Q_\varphi(s, \hat{a})) \, \nabla_\theta \pi_\theta(s, \hat{a}) + \sum_a Q_\varphi(s, a) \, \nabla_\theta \pi_\theta(s, a)
\]</span></p>
<p><span class="math inline">\(\beta\)</span> is defined as <span class="math inline">\(\min (c, \frac{1}{b(s, \hat{a})})\)</span>, where <span class="math inline">\(c&gt;1\)</span> is a constant. This truncated term is similar to what was used in ACER. The rule is now called <strong><span class="math inline">\(\beta\)</span>-LOO</strong> and is a novel proposition of the Reactor.</p>
<p>The second importance contribution of the Reactor is how to combine the Retrace algorithm (Section <a href="./ImportanceSampling.html#sec:retrace">4.3.3</a>, <span class="citation" data-cites="Munos2016">Munos et al. (<a href="References.html#ref-Munos2016" role="doc-biblioref">2016</a>)</span>) for estimating the return <span class="math inline">\(R(s, \hat{a})\)</span> on multiple steps, with the distributional learning method of Categorical DQN. As Retrace uses n-steps returns iteratively, the n-step distributional Bellman target can updated using the <span class="math inline">\(n\)</span> future rewards:</p>
<p><span class="math display">\[
    z_i^n = \mathcal{T}^n \, z_i = \sum_{k=t}^{t+n} \gamma^{k-t} r_k + \gamma^n \, z_i
\]</span></p>
<p>We leave out the details on how Retrace is combined with these distributional Bellman updates: the notation is complicated but the idea is simple. The last importance contribution of the paper is the use of <strong>prioritized sequence replay</strong>. Prioritized experience replay allows to select in priority transitions from the replay buffer which are the most surprising, i.e. where the TD error is the highest. These transitions are the ones carrying the most information. A similar principle can be applied to sequences of transitions, which are needed by the n-step updates. They devised a specific sampling algorithm in order to achieve this and reduce the variance of the samples.</p>
<p>The last particularities of the Reactor is that it uses a LSTM layer to make the problem Markovian (instead of stacking four frames as in DQN) and train multiple actor-learners as in A3C. The algorithm is trained on CPU, with 10 or 20 actor-learners. The Reactor outperforms DQN and its variants, A3C and ACER on Atari games. Importantly, Reactor only needs one day of training on CPU, compared to the 8 days of GPU training needed by DQN.</p>

<br>
<div class="arrows">
<a href="EntropyRL.html" class="previous">&laquo; Previous</a>
<a href="OtherPolicyGradient.html" class="next">Next &raquo;</a>
</div>

</article>

</body>
</html>
