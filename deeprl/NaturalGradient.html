<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <meta name="author" content="Julien Vitay">
  <title>Deep Reinforcement Learning</title>


  <link rel="stylesheet" href="assets/syntax.css">
  <link rel="stylesheet" href="assets/github.css">
  <script src="/usr/share/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->

</head>

<body class="markdown-body">

<header>
  <h1 class="title"><strong>Deep Reinforcement Learning</strong></h1>
  <p class="author">Julien Vitay - <a href="mailto:julien.vitay@informatik.tu-chemnitz.de">julien.vitay@informatik.tu-chemnitz.de</a></p>
</header>


<article>

<nav id="TOC" class ="toc">
  <h2><strong>Deep Reinforcement Learning</strong></h2>
  <p class="author">Julien Vitay</p>

<ul>
<li><a href="./Introduction.html#sec:introduction"><span class="toc-section-number">1</span> Introduction</a></li>
<li><a href="./BasicRL.html#sec:basics"><span class="toc-section-number">2</span> Basics</a><ul>
<li><a href="./BasicRL.html#sec:reinforcement-learning-and-markov-decision-process"><span class="toc-section-number">2.1</span> Reinforcement learning and Markov Decision Process</a><ul>
<li><a href="./BasicRL.html#sec:policy-and-value-functions"><span class="toc-section-number">2.1.1</span> Policy and value functions</a></li>
<li><a href="./BasicRL.html#sec:bellman-equations"><span class="toc-section-number">2.1.2</span> Bellman equations</a></li>
<li><a href="./BasicRL.html#sec:dynamic-programming"><span class="toc-section-number">2.1.3</span> Dynamic programming</a></li>
<li><a href="./BasicRL.html#sec:monte-carlo-sampling"><span class="toc-section-number">2.1.4</span> Monte-Carlo sampling</a></li>
<li><a href="./BasicRL.html#sec:temporal-difference"><span class="toc-section-number">2.1.5</span> Temporal Difference</a></li>
<li><a href="./BasicRL.html#sec:eligibility-traces"><span class="toc-section-number">2.1.6</span> Eligibility traces</a></li>
<li><a href="./BasicRL.html#sec:actor-critic-architectures"><span class="toc-section-number">2.1.7</span> Actor-critic architectures</a></li>
<li><a href="./BasicRL.html#sec:function-approximation"><span class="toc-section-number">2.1.8</span> Function approximation</a></li>
</ul></li>
<li><a href="./DeepLearning.html#sec:deep-learning"><span class="toc-section-number">2.2</span> Deep learning</a><ul>
<li><a href="./DeepLearning.html#sec:deep-neural-networks"><span class="toc-section-number">2.2.1</span> Deep neural networks</a></li>
<li><a href="./DeepLearning.html#sec:convolutional-networks"><span class="toc-section-number">2.2.2</span> Convolutional networks</a></li>
<li><a href="./DeepLearning.html#sec:recurrent-neural-networks"><span class="toc-section-number">2.2.3</span> Recurrent neural networks</a></li>
</ul></li>
</ul></li>
<li><a href="./Valuebased.html#sec:value-based-methods"><span class="toc-section-number">3</span> Value-based methods</a><ul>
<li><a href="./Valuebased.html#sec:limitations-of-deep-neural-networks-for-function-approximation"><span class="toc-section-number">3.1</span> Limitations of deep neural networks for function approximation</a></li>
<li><a href="./Valuebased.html#sec:deep-q-network-dqn"><span class="toc-section-number">3.2</span> Deep Q-Network (DQN)</a></li>
<li><a href="./Valuebased.html#sec:double-dqn"><span class="toc-section-number">3.3</span> Double DQN</a></li>
<li><a href="./Valuebased.html#sec:prioritized-experience-replay"><span class="toc-section-number">3.4</span> Prioritized experience replay</a></li>
<li><a href="./Valuebased.html#sec:duelling-network"><span class="toc-section-number">3.5</span> Duelling network</a></li>
<li><a href="./Valuebased.html#sec:distributed-dqn-gorila"><span class="toc-section-number">3.6</span> Distributed DQN (GORILA)</a></li>
<li><a href="./Valuebased.html#sec:deep-recurrent-q-learning-drqn"><span class="toc-section-number">3.7</span> Deep Recurrent Q-learning (DRQN)</a></li>
<li><a href="./Valuebased.html#sec:other-variants-of-dqn"><span class="toc-section-number">3.8</span> Other variants of DQN</a></li>
</ul></li>
<li><a href="./PolicyGradient.html#sec:policy-gradient-methods"><span class="toc-section-number">4</span> Policy Gradient methods</a><ul>
<li><a href="./PolicyGradient.html#sec:reinforce"><span class="toc-section-number">4.1</span> REINFORCE</a><ul>
<li><a href="./PolicyGradient.html#sec:estimating-the-policy-gradient"><span class="toc-section-number">4.1.1</span> Estimating the policy gradient</a></li>
<li><a href="./PolicyGradient.html#sec:reducing-the-variance"><span class="toc-section-number">4.1.2</span> Reducing the variance</a></li>
<li><a href="./PolicyGradient.html#sec:policy-gradient-theorem"><span class="toc-section-number">4.1.3</span> Policy Gradient theorem</a></li>
</ul></li>
<li><a href="./ActorCritic.html#sec:advantage-actor-critic-methods"><span class="toc-section-number">4.2</span> Advantage Actor-Critic methods</a><ul>
<li><a href="./ActorCritic.html#sec:advantage-actor-critic-a2c"><span class="toc-section-number">4.2.1</span> Advantage Actor-Critic (A2C)</a></li>
<li><a href="./ActorCritic.html#sec:asynchronous-advantage-actor-critic-a3c"><span class="toc-section-number">4.2.2</span> Asynchronous Advantage Actor-Critic (A3C)</a></li>
<li><a href="./ActorCritic.html#sec:generalized-advantage-estimation-gae"><span class="toc-section-number">4.2.3</span> Generalized Advantage Estimation (GAE)</a></li>
<li><a href="./ActorCritic.html#sec:stochastic-actor-critic-for-continuous-action-spaces"><span class="toc-section-number">4.2.4</span> Stochastic actor-critic for continuous action spaces</a></li>
</ul></li>
<li><a href="./ImportanceSampling.html#sec:off-policy-actor-critic"><span class="toc-section-number">4.3</span> Off-policy Actor-Critic</a><ul>
<li><a href="./ImportanceSampling.html#sec:importance-sampling"><span class="toc-section-number">4.3.1</span> Importance sampling</a></li>
<li><a href="./ImportanceSampling.html#sec:linear-off-policy-actor-critic-off-pac"><span class="toc-section-number">4.3.2</span> Linear Off-Policy Actor-Critic (Off-PAC)</a></li>
<li><a href="./ImportanceSampling.html#sec:retrace"><span class="toc-section-number">4.3.3</span> Retrace</a></li>
<li><a href="./ImportanceSampling.html#sec:self-imitation-learning-sil"><span class="toc-section-number">4.3.4</span> Self-Imitation Learning (SIL)</a></li>
</ul></li>
<li><a href="./DPG.html#sec:deterministic-policy-gradient-dpg"><span class="toc-section-number">4.4</span> Deterministic Policy Gradient (DPG)</a><ul>
<li><a href="./DPG.html#sec:deterministic-policy-gradient-theorem"><span class="toc-section-number">4.4.1</span> Deterministic policy gradient theorem</a></li>
<li><a href="./DPG.html#sec:deep-deterministic-policy-gradient-ddpg"><span class="toc-section-number">4.4.2</span> Deep Deterministic Policy Gradient (DDPG)</a></li>
<li><a href="./DPG.html#sec:distributed-distributional-ddpg-d4pg"><span class="toc-section-number">4.4.3</span> Distributed Distributional DDPG (D4PG)</a></li>
</ul></li>
<li><a href="./NaturalGradient.html#sec:natural-gradients"><span class="toc-section-number">4.5</span> Natural Gradients</a><ul>
<li><a href="./NaturalGradient.html#sec:principle-of-natural-gradients"><span class="toc-section-number">4.5.1</span> Principle of natural gradients</a></li>
<li><a href="./NaturalGradient.html#sec:natural-policy-gradient-and-natural-actor-critic-nac"><span class="toc-section-number">4.5.2</span> Natural policy gradient and Natural Actor Critic (NAC)</a></li>
<li><a href="./NaturalGradient.html#sec:trust-region-policy-optimization-trpo"><span class="toc-section-number">4.5.3</span> Trust Region Policy Optimization (TRPO)</a></li>
<li><a href="./NaturalGradient.html#sec:proximal-policy-optimization-ppo"><span class="toc-section-number">4.5.4</span> Proximal Policy Optimization (PPO)</a></li>
<li><a href="./NaturalGradient.html#sec:actor-critic-with-experience-replay-acer"><span class="toc-section-number">4.5.5</span> Actor-Critic with Experience Replay (ACER)</a></li>
</ul></li>
<li><a href="./EntropyRL.html#sec:maximum-entropy-rl"><span class="toc-section-number">4.6</span> Maximum Entropy RL</a><ul>
<li><a href="#sec:entropy-regularization-1"><span class="toc-section-number">4.6.1</span> Entropy regularization</a></li>
<li><a href="./EntropyRL.html#sec:soft-q-learning"><span class="toc-section-number">4.6.2</span> Soft Q-learning</a></li>
<li><a href="./EntropyRL.html#sec:soft-actor-critic-sac"><span class="toc-section-number">4.6.3</span> Soft Actor-Critic (SAC)</a></li>
</ul></li>
<li><a href="./DistributionalRL.html#sec:distributional-learning"><span class="toc-section-number">4.7</span> Distributional learning</a><ul>
<li><a href="./DistributionalRL.html#sec:categorical-dqn"><span class="toc-section-number">4.7.1</span> Categorical DQN</a></li>
<li><a href="./DistributionalRL.html#sec:the-reactor"><span class="toc-section-number">4.7.2</span> The Reactor</a></li>
</ul></li>
<li><a href="./OtherPolicyGradient.html#sec:miscellaneous-model-free-algorithm"><span class="toc-section-number">4.8</span> Miscellaneous model-free algorithm</a><ul>
<li><a href="./OtherPolicyGradient.html#sec:stochastic-value-gradient-svg"><span class="toc-section-number">4.8.1</span> Stochastic Value Gradient (SVG)</a></li>
<li><a href="./OtherPolicyGradient.html#sec:q-prop"><span class="toc-section-number">4.8.2</span> Q-Prop</a></li>
<li><a href="./OtherPolicyGradient.html#sec:normalized-advantage-function-naf"><span class="toc-section-number">4.8.3</span> Normalized Advantage Function (NAF)</a></li>
<li><a href="./OtherPolicyGradient.html#sec:fictitious-self-play-fsp"><span class="toc-section-number">4.8.4</span> Fictitious Self-Play (FSP)</a></li>
</ul></li>
<li><a href="./OtherPolicyGradient.html#sec:comparison-between-value-based-and-policy-gradient-methods"><span class="toc-section-number">4.9</span> Comparison between value-based and policy gradient methods</a></li>
<li><a href="./OtherPolicyGradient.html#sec:gradient-free-policy-search"><span class="toc-section-number">4.10</span> Gradient-free policy search</a><ul>
<li><a href="./OtherPolicyGradient.html#sec:cross-entropy-method-cem"><span class="toc-section-number">4.10.1</span> Cross-entropy Method (CEM)</a></li>
<li><a href="./OtherPolicyGradient.html#sec:evolutionary-search-es"><span class="toc-section-number">4.10.2</span> Evolutionary Search (ES)</a></li>
</ul></li>
</ul></li>
<li><a href="./ModelBased.html#sec:model-based-rl"><span class="toc-section-number">5</span> Model-based RL</a><ul>
<li><a href="./ModelBased.html#sec:dyna-q"><span class="toc-section-number">5.1</span> Dyna-Q</a></li>
<li><a href="./ModelBased.html#sec:unsorted-references"><span class="toc-section-number">5.2</span> Unsorted references</a></li>
</ul></li>
<li><a href="./Practice.html#sec:deep-rl-in-practice"><span class="toc-section-number">6</span> Deep RL in practice</a><ul>
<li><a href="./Practice.html#sec:limitations"><span class="toc-section-number">6.1</span> Limitations</a></li>
<li><a href="./Practice.html#sec:simulation-environments"><span class="toc-section-number">6.2</span> Simulation environments</a></li>
<li><a href="./Practice.html#sec:algorithm-implementations"><span class="toc-section-number">6.3</span> Algorithm implementations</a></li>
</ul></li>
<li><a href="./References.html#sec:references">References</a></li>
</ul>


</nav>



<h2 id="sec:natural-gradients"><span class="header-section-number">4.5</span> Natural Gradients</h2>
<p>The deep networks used as function approximators in the methods presented until now were all optimized (trained) using <strong>stochastic gradient descent</strong> (SGD) or any of its variants (RMSProp, Adam, etc). The basic idea is to change the parameters <span class="math inline">\(\theta\)</span> in the opposite direction of the gradient of the loss function (or the same direction as the policy gradient, in which case it is called gradient ascent), proportionally to a small learning rate <span class="math inline">\(\eta\)</span>:</p>
<p><span class="math display">\[
    \Delta \theta = - \eta \, \nabla_\theta \mathcal{L}(\theta)
\]</span></p>
<p>SGD is also called a <strong>steepest descent method</strong>: one searches for the smallest parameter change <span class="math inline">\(\Delta \theta\)</span> inducing the biggest negative change of the loss function. In classical supervised learning, this is what we want: we want to minimize the loss function as fast as possible, while keeping weight changes as small as possible, otherwise learning might become unstable (weight changes computed for a single minibatch might erase the changes made on previous minibatches). The main difficulty of supervised learning is to choose the right value for the learning rate: too high and learning is unstable; too low and learning takes forever.</p>
<p>In deep RL, we have an additional problem: the problem is not stationary (see Section <a href="./Valuebased.html#sec:limitations-of-deep-neural-networks-for-function-approximation">3.1</a>). In Q-learning, the target <span class="math inline">\(r(s, a, s&#39;) + \gamma \, \max_{a&#39;} Q_\theta(S&#39;, a&#39;)\)</span> is changing with <span class="math inline">\(\theta\)</span>. If the Q-values change a lot between two minibatches, the network will not get any stable target signal to learn from, and the policy will end up suboptimal. The trick is to use <strong>target networks</strong> to compute the target, which can be either an old copy of the current network (vanilla DQN), or a smoothed version of it (DDPG). Obviously, this introduces a bias (the targets are always wrong during training), but this bias converges to zero (after sufficient training, the targets will be almost correct), at the cost of a huge sample complexity.</p>
<p>Target networks cannot be used in <strong>on-policy</strong> methods, especially actor-critic architectures. As seen in Section <a href="./ImportanceSampling.html#sec:off-policy-actor-critic">4.3</a>, the critic must learn from transitions recently generated by the actor (although importance sampling and the Retrace algorithm might help). The problem with on-policy methods is that they waste a lot of data: they always need fresh samples to learn from and never reuse past experiences. The policy gradient theorem shows why:</p>
<p><span class="math display">\[
    \nabla_\theta J(\theta) =  \mathbb{E}_{s \sim \rho_\theta, a \sim \pi_\theta}[\nabla_\theta \log \pi_\theta(s, a) \, Q^{\pi_\theta}(s, a))] \approx  \mathbb{E}_{s \sim \rho_\theta, a \sim \pi_\theta}[\nabla_\theta \log \pi_\theta(s, a) \, Q_\varphi(s, a))]
\]</span></p>
<p>If the policy <span class="math inline">\(\pi_\theta\)</span> changes a lot between two updates, the estimated Q-value <span class="math inline">\(Q_\varphi(s, a)\)</span> will represent the value of the action for a totally different policy, not the true Q-value <span class="math inline">\(Q^{\pi_\theta}(s, a)\)</span>. The estimated policy gradient will then be strongly biased and learning will be suboptimal. In other words, the actor should not change much faster than the critic, and vice versa. A naive solution would be to use a very small learning rate for the actor, but this just slows down learning (adding to the sample complexity) without solving the problem.</p>
<p>To solve the problem, we should actually do the opposite of the steepest descent: <em>search for the biggest parameter change <span class="math inline">\(\Delta \theta\)</span> inducing the smallest change in the policy, but in the right direction</em>. If the parameter change is high, the actor will learn a lot internally from each experience. But if the policy change is small between two updates (although the parameters have changed a lot), we might be able to reuse past experiences, as the targets will not be that wrong.</p>
<p>This is where <strong>natural gradients</strong> come into play, which are originally a statistical method to optimize over spaces of probability distributions, for example for variational inference. The idea to use natural gradients to train neural networks comes from <span class="citation" data-cites="Amari1998">Amari (<a href="References.html#ref-Amari1998" role="doc-biblioref">1998</a>)</span>. <span class="citation" data-cites="Kakade2001">Kakade (<a href="References.html#ref-Kakade2001" role="doc-biblioref">2001</a>)</span> applied natural gradients to policy gradient methods, while <span class="citation" data-cites="Peters2008">Peters and Schaal (<a href="References.html#ref-Peters2008" role="doc-biblioref">2008</a>)</span> proposed a natural actor-critic algorithm for linear function approximators. The idea was adapted to deep RL by Schulman and colleagues, with Trust Region Policy Optimization <span class="citation" data-cites="Schulman2015">(TRPO, Schulman et al., <a href="References.html#ref-Schulman2015" role="doc-biblioref">2015</a><a href="References.html#ref-Schulman2015" role="doc-biblioref">a</a>)</span> and Proximal Policy Optimization <span class="citation" data-cites="Schulman2017">(PPO, Schulman et al., <a href="References.html#ref-Schulman2017" role="doc-biblioref">2017</a><a href="References.html#ref-Schulman2017" role="doc-biblioref">b</a>)</span>, the latter gaining momentum over DDPG as the go-to method for continuous RL problems, particularly because of its smaller sample complexity and its robustness to hyperparameters.</p>
<h3 id="sec:principle-of-natural-gradients"><span class="header-section-number">4.5.1</span> Principle of natural gradients</h3>
<figure>
<img src="img/naturalgradient.png" id="fig:naturalgradient" style="width:80.0%" alt="" /><figcaption>Figure 28: Euclidian distances in the parameter space do not represent well the statistical distance between probability distributions. The two Gaussians on the left (<span class="math inline">\(\mathcal{N}(0, 0.2)\)</span> and <span class="math inline">\(\mathcal{N}(1, 0.2)\)</span>) have the same Euclidian distance in the parameter space (<span class="math inline">\(d = \sqrt{(\mu_0 - \mu_1)^2+(\sigma_0 - \sigma_1)^2}\)</span>) than the two Gaussians on the right (<span class="math inline">\(\mathcal{N}(0, 10)\)</span> and <span class="math inline">\(\mathcal{N}(1, 10)\)</span>). However, the Gaussians on the right are much more similar than the two on the left: if you have a single sample, you could not say from which distribution it comes for the Gaussians on the right, while it is obvious for the Gaussians on the left.</figcaption>
</figure>
<p>Consider the two Gaussian distributions in the left part of Fig. <a href="#fig:naturalgradient">28</a> (<span class="math inline">\(\mathcal{N}(0, 0.2)\)</span> and <span class="math inline">\(\mathcal{N}(1, 0.2)\)</span>) and the two on the right (<span class="math inline">\(\mathcal{N}(0, 10)\)</span> and <span class="math inline">\(\mathcal{N}(1, 10)\)</span>). In both cases, the distance in the Euclidian space of parameters <span class="math inline">\(d = \sqrt{(\mu_0 - \mu_1)^2+(\sigma_0 - \sigma_1)^2}\)</span> is the same between the two Gaussians. Obviously, the two distributions on the left are however further away from each other than the two on the the right. This indicates that the Euclidian distance in the parameter space (which is what <em>regular</em> gradients act on) is not a correct measurement of the statistical distance between two distributions (which what we want to minimize between two iterations of PG).</p>
<p>In statistics, a common measurement of the statistical distance between two distributions <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span> is the <strong>Kullback-Leibler (KL) divergence</strong> <span class="math inline">\(D_{KL}(p||q)\)</span>, also called relative entropy or information gain. It is defined as:</p>
<p><span class="math display">\[
    D_{KL}(p || q) = \mathbb{E}_{x \sim p} [\log \frac{p(x)}{q(x)}]  = \int p(x) \, \log \frac{p(x)}{q(x)} \, dx
\]</span></p>
<p>Its minimum is 0 when <span class="math inline">\(p=q\)</span> (as <span class="math inline">\(\log \frac{p(x)}{q(x)}\)</span> is then 0) and is positive otherwise. Minimizing the KL divergence is equivalent to “matching” two distributions. Note that supervised methods in machine learning can all be interpreted as a minimization of the KL divergence: if <span class="math inline">\(p(x)\)</span> represents the distribution of the data (the label of a sample <span class="math inline">\(x\)</span>) and <span class="math inline">\(q(x)\)</span> the one of the model (the prediction of a neural network for the same sample <span class="math inline">\(x\)</span>), supervised methods want the output distribution of the model to match the distribution of the data, i.e. make predictions that are the same as the labels. For generative models, this is for example at the core of <em>generative adversarial networks</em> <span class="citation" data-cites="Goodfellow2014 Arjovsky2017">(Arjovsky et al., <a href="References.html#ref-Arjovsky2017" role="doc-biblioref">2017</a>; Goodfellow et al., <a href="References.html#ref-Goodfellow2014" role="doc-biblioref">2014</a>)</span> or <em>variational autoencoders</em> <span class="citation" data-cites="Kingma2013">(Kingma and Welling, <a href="References.html#ref-Kingma2013" role="doc-biblioref">2013</a>)</span>.</p>
<p>The KL divergence is however not symmetrical (<span class="math inline">\(D_{KL}(p || q) \neq D_{KL}(q || p)\)</span>), so a more useful divergence is the symmetric KL divergence, also known as Jensen-Shannon (JS) divergence:</p>
<p><span class="math display">\[
    D_{JS}(p || q) = \frac{D_{KL}(p || q) + D_{KL}(q || p)}{2}
\]</span></p>
<p>Other forms of divergence measurements exist, such as the Wasserstein distance which improves generative adversarial networks <span class="citation" data-cites="Arjovsky2017">(Arjovsky et al., <a href="References.html#ref-Arjovsky2017" role="doc-biblioref">2017</a>)</span>, but they are not relevant here. See <a href="https://www.alexirpan.com/2017/02/22/wasserstein-gan.html" class="uri">https://www.alexirpan.com/2017/02/22/wasserstein-gan.html</a> for more explanations.</p>
<p>We now have a global measurement of the similarity between two distributions on the whole input space, but which is hard to compute. How can we use it anyway in our optimization problem? As mentioned above, we search for the biggest parameter change <span class="math inline">\(\Delta \theta\)</span> inducing the smallest change in the policy. We need a metric linking changes in the parameters of the distribution (the weights of the network) to changes in the distribution itself. In other terms, we will apply gradient descent on the statistical manifold defined by the parameters rather than on the parameters themselves.</p>
<figure>
<img src="img/riemannian.png" id="fig:riemannian" style="width:50.0%" alt="" /><figcaption>Figure 29: Naive illustration of the Riemannian metric. The Euclidian distance between <span class="math inline">\(p(x; \theta)\)</span> and <span class="math inline">\(p(x; \theta + \Delta \theta)\)</span> depends on the Euclidian distance between <span class="math inline">\(\theta\)</span> and <span class="math inline">\(\theta + \Delta\theta\)</span>, i.e. <span class="math inline">\(\Delta \theta\)</span>. Riemannian metrics follow the geometry of the manifold to compute that distance, depending on its curvature. This figure is only for illustration: Riemannian metrics are purely local, <span class="math inline">\(\Delta \theta\)</span> should be much smaller.</figcaption>
</figure>
<p>Let’s consider a parameterized distribution <span class="math inline">\(p(x; \theta)\)</span> and its new value <span class="math inline">\(p(x; \theta + \Delta \theta)\)</span> after applying a small parameter change <span class="math inline">\(\Delta \theta\)</span>. As depicted on Fig. <a href="#fig:riemannian">29</a>, the Euclidian metric in the parameter space (<span class="math inline">\(||\theta + \Delta \theta - \theta||^2\)</span>) does not take the structure of the statistical manifold into account. We need to define a <strong>Riemannian metric</strong> which accounts locally for the curvature of the manifold between <span class="math inline">\(\theta\)</span> and <span class="math inline">\(\theta + \Delta \theta\)</span>. The Riemannian distance is defined by the dot product:</p>
<p><span class="math display">\[
    ||\Delta \theta||^2 = &lt; \Delta \theta , F(\theta) \, \Delta \theta &gt;
\]</span></p>
<p>where <span class="math inline">\(F(\theta)\)</span> is called the Riemannian metric tensor and is an inner product on the tangent space of the manifold at the point <span class="math inline">\(\theta\)</span>.</p>
<p>When using the symmetric KL divergence to measure the distance between two distributions, the corresponding Riemannian metric is the <strong>Fisher Information Matrix</strong> (FIM), defined as the Hessian matrix of the KL divergence around <span class="math inline">\(\theta\)</span>, i.e. the matrix of second order derivatives w.r.t the elements of <span class="math inline">\(\theta\)</span>. See <a href="https://stats.stackexchange.com/questions/51185/connection-between-fisher-metric-and-the-relative-entropy" class="uri">https://stats.stackexchange.com/questions/51185/connection-between-fisher-metric-and-the-relative-entropy</a> and <a href="https://wiseodd.github.io/techblog/2018/03/14/natural-gradient/" class="uri">https://wiseodd.github.io/techblog/2018/03/14/natural-gradient/</a> for an explanation of the link between the Fisher matrix and KL divergence.</p>
<p>The Fisher information matrix is defined as the Hessian of the KL divergence around <span class="math inline">\(\theta\)</span>, i.e. how the manifold locally changes around <span class="math inline">\(\theta\)</span>:</p>
<p><span class="math display">\[
    F(\theta) = \nabla^2 D_{JS}(p(x; \theta) || p(x; \theta + \Delta \theta))|_{\Delta \theta = 0}
\]</span></p>
<p>which necessitates to compute second order derivatives which are very complex and slow to obtain, especially when there are many parameters <span class="math inline">\(\theta\)</span> (the weights of the NN). Fortunately, it also has a simpler form which only depends on the outer product between the gradients of the log-likelihoods:</p>
<p><span class="math display">\[
    F(\theta) = \mathbb{E}_{x \sim p(x, \theta)}[ \nabla \log p(x; \theta)  (\nabla \log p(x; \theta))^T]
\]</span></p>
<p>which is something we can easily sample and compute.</p>
<p>Why is it useful? The Fisher Information matrix allows to <strong>locally</strong> approximate (for small <span class="math inline">\(\Delta \theta\)</span>) the KL divergence between the two close distributions (using a second-order Taylor series expansion):</p>
<p><span class="math display">\[
    D_{JS}(p(x; \theta) || p(x; \theta + \Delta \theta)) \approx \Delta \theta^T \, F(\theta) \, \Delta \theta
\]</span></p>
<p>The KL divergence is then locally quadratic, which means that the update rules obtained when minimizing the KL divergence with gradient descent will be linear. Suppose we want to minimize a loss function <span class="math inline">\(L\)</span> parameterized by <span class="math inline">\(\theta\)</span> and depending on the distribution <span class="math inline">\(p\)</span>. <strong>Natural gradient descent</strong> <span class="citation" data-cites="Amari1998">(Amari, <a href="References.html#ref-Amari1998" role="doc-biblioref">1998</a>)</span> attempts to move along the statistical manifold defined by <span class="math inline">\(p\)</span> by correcting the gradient of <span class="math inline">\(L(\theta)\)</span> using the local curvature of the KL-divergence surface, i.e. moving some given distance in the direction <span class="math inline">\(\tilde{\nabla_\theta} L(\theta)\)</span>:</p>
<p><span class="math display">\[
    \tilde{\nabla_\theta} L(\theta) = F(\theta)^{-1} \, \nabla_\theta L(\theta)
\]</span></p>
<p><span class="math inline">\(\tilde{\nabla_\theta} L(\theta)\)</span> is the <strong>natural gradient</strong> of <span class="math inline">\(L(\theta)\)</span>. Natural gradient descent simply takes steps in this direction:</p>
<p><span class="math display">\[
    \Delta \theta = - \eta \, \tilde{\nabla_\theta} L(\theta)
\]</span></p>
<p>When the manifold is not curved (<span class="math inline">\(F(\theta)\)</span> is the identity matrix), natural gradient descent is the regular gradient descent.</p>
<p>But what is the advantage of natural gradients? The problem with regular gradient descent is that it relies on a fixed learning rate. In regions where the loss function is flat (a plateau), the gradient will be almost zero, leading to very slow improvements. Because the natural gradient depends on the inverse of the curvature (Fisher), the magnitude of the gradient will be higher in flat regions, leading to bigger steps, and smaller in very steep regions (around minima). Natural GD therefore converges faster and better than regular GD.</p>
<p>Natural gradient descent is a generic optimization method, it can for example be used to train more efficiently deep networks in supervised learning <span class="citation" data-cites="Pascanu2013">(Pascanu and Bengio, <a href="References.html#ref-Pascanu2013" role="doc-biblioref">2013</a>)</span>. Its main drawback is the necessity to inverse the Fisher information matrix, whose size depends on the number of free parameters (if you have N weights in the NN, you need to inverse a NxN matrix). Several approximations allows to remediate to this problem, for example Conjugate Gradients or Kronecker-Factored Approximate Curvature (K-FAC).</p>
<p><strong>Additional resources</strong> to understand natural gradients:</p>
<ul>
<li><a href="http://andymiller.github.io/2016/10/02/natural_gradient_bbvi.html" class="uri">http://andymiller.github.io/2016/10/02/natural_gradient_bbvi.html</a></li>
<li><a href="https://hips.seas.harvard.edu/blog/2013/01/25/the-natural-gradient/" class="uri">https://hips.seas.harvard.edu/blog/2013/01/25/the-natural-gradient/</a></li>
<li><a href="http://kvfrans.com/what-is-the-natural-gradient-and-where-does-it-appear-in-trust-region-policy-optimization" class="uri">http://kvfrans.com/what-is-the-natural-gradient-and-where-does-it-appear-in-trust-region-policy-optimization</a></li>
<li><a href="https://wiseodd.github.io/techblog/2018/03/14/natural-gradient/" class="uri">https://wiseodd.github.io/techblog/2018/03/14/natural-gradient/</a></li>
<li>A tutorial by John Schulman (OpenAI) <a href="https://www.youtube.com/watch?v=xvRrgxcpaHY" class="uri">https://www.youtube.com/watch?v=xvRrgxcpaHY</a></li>
<li>A blog post on the related Hessian-free optimization and conjuguate gradients <a href="http://andrew.gibiansky.com/blog/machine-learning/hessian-free-optimization/" class="uri">http://andrew.gibiansky.com/blog/machine-learning/hessian-free-optimization/</a></li>
<li>K-FAC: <a href="https://syncedreview.com/2017/03/25/optimizing-neural-networks-using-structured-probabilistic-models/" class="uri">https://syncedreview.com/2017/03/25/optimizing-neural-networks-using-structured-probabilistic-models/</a></li>
<li>Conjugate gradients: <a href="https://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf" class="uri">https://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf</a></li>
</ul>
<p><strong>Note:</strong> Natural gradients can also be used to train DQN architectures, resulting in more efficient and stable learning behaviors <span class="citation" data-cites="Knight2018">(Knight and Lerner, <a href="References.html#ref-Knight2018" role="doc-biblioref">2018</a>)</span>.</p>
<h3 id="sec:natural-policy-gradient-and-natural-actor-critic-nac"><span class="header-section-number">4.5.2</span> Natural policy gradient and Natural Actor Critic (NAC)</h3>
<p><span class="citation" data-cites="Kakade2001">Kakade (<a href="References.html#ref-Kakade2001" role="doc-biblioref">2001</a>)</span> applied the principle of natural gradients proposed by <span class="citation" data-cites="Amari1998">Amari (<a href="References.html#ref-Amari1998" role="doc-biblioref">1998</a>)</span> to the policy gradient theorem:</p>
<p><span class="math display">\[
    \nabla_\theta J(\theta) =  \mathbb{E}_{s \sim \rho_\theta, a \sim \pi_\theta}[\nabla_\theta \log \pi_\theta(s, a) \, Q^{\pi_\theta}(s, a)]
\]</span></p>
<p>This <em>regular</em> gradient does not take into account the underlying structure of the policy distribution <span class="math inline">\(\pi(s, a)\)</span>. The Fisher information matrix for the policy is defined by:</p>
<p><span class="math display">\[
    F(\theta) = \mathbb{E}_{s \sim \rho_\theta, a \sim \pi_\theta}[ \nabla \log \pi_\theta(s, a)  (\nabla \log \pi_\theta(s, a))^T]
\]</span></p>
<p>The natural policy gradient is simply:</p>
<p><span class="math display">\[
    \tilde{\nabla}_\theta J(\theta) = F(\theta)^{-1} \, \nabla_\theta J(\theta)  = \mathbb{E}_{s \sim \rho_\theta, a \sim \pi_\theta}[ F(\theta)^{-1} \,  \nabla_\theta \log \pi_\theta(s, a) \, Q^{\pi_\theta}(s, a)]
\]</span></p>
<p><span class="citation" data-cites="Kakade2001">Kakade (<a href="References.html#ref-Kakade2001" role="doc-biblioref">2001</a>)</span> also shows that you can replace the true Q-value <span class="math inline">\(Q^{\pi_\theta}(s, a)\)</span> with a compatible approximation <span class="math inline">\(Q_\varphi(s, a)\)</span> (as long as it minimizes the quadratic error) and still obtained an unbiased natural gradient. An important theoretical result is that policy improvement is guaranteed with natural gradients: the new policy after an update is always better (more expected returns) than before. He experimented this new rule on various simple MDPs and observed drastic improvements over vanilla PG.</p>
<p><span class="citation" data-cites="Peters2008">Peters and Schaal (<a href="References.html#ref-Peters2008" role="doc-biblioref">2008</a>)</span> extended on the work of <span class="citation" data-cites="Kakade2001">Kakade (<a href="References.html#ref-Kakade2001" role="doc-biblioref">2001</a>)</span> to propose the natural actor-critic (NAC). The exact derivations would be too complex to summarize here, but the article is an interesting read. He particularly reviews the progress at that time on policy gradient for its use in robotics. He showed that the <span class="math inline">\(F(\theta)\)</span>) is a true Fisher information matrix even when using sampled episodes, and derived a baseline <span class="math inline">\(b\)</span> to reduce the variance of the natural policy gradient. He demonstrated the power of this algorithm by letting a robot learning motor primitives for baseball.</p>
<h3 id="sec:trust-region-policy-optimization-trpo"><span class="header-section-number">4.5.3</span> Trust Region Policy Optimization (TRPO)</h3>
<h4 id="sec:principle" class="unnumbered">Principle</h4>
<p><span class="citation" data-cites="Schulman2015">Schulman et al. (<a href="References.html#ref-Schulman2015" role="doc-biblioref">2015</a><a href="References.html#ref-Schulman2015" role="doc-biblioref">a</a>)</span> extended the idea of natural gradients to allow their use for non-linear function approximators (e.g. deep networks), as the previous algorithms only worked efficiently for linear approximators. The proposed algorithm, Trust Region Policy Optimization (TRPO), has now been replaced in practice by Proximal Policy Optimization (PPO, see next section) but its novel ideas are important to understand already.</p>
<p>Let’s note the expected return of a policy <span class="math inline">\(\pi\)</span> as:</p>
<p><span class="math display">\[
    \eta(\pi) = \mathbb{E}_{s \sim \rho_\pi, a \sim \pi}[\sum_{t=0}^\infty \gamma^t \, r(s_t, a_t, s_{t+1})]
\]</span></p>
<p>where <span class="math inline">\(\rho_\pi\)</span> is the discounted visitation frequency distribution (the probability that a state <span class="math inline">\(s\)</span> will be visited at some point in time by the policy <span class="math inline">\(\pi\)</span>):</p>
<p><span class="math display">\[
    \rho_\pi(s) = P(s_0=s) + \gamma \, P(s_1=s) + \gamma^2 \, P(s_2=s) + \ldots
\]</span></p>
<p><span class="citation" data-cites="Kakade2002">Kakade and Langford (<a href="References.html#ref-Kakade2002" role="doc-biblioref">2002</a>)</span> had shown that it is possible to relate the expected return of two policies <span class="math inline">\(\pi_\theta\)</span> and <span class="math inline">\(\pi_{\theta_\text{old}}\)</span> using advantages (omitting <span class="math inline">\(\pi\)</span> in the notations):</p>
<p><span class="math display">\[
    \eta(\theta) = \eta(\theta_\text{old}) + \mathbb{E}_{s \sim \rho_{\pi_\theta}, a \sim \pi_\theta} [A_{\pi_{\theta_\text{old}}}(s, a)]
\]</span></p>
<p>The advantage <span class="math inline">\(A_{\pi_{\theta_\text{old}}}(s, a)\)</span> denotes the change in the expected return obtained after <span class="math inline">\((s, a)\)</span> when using the new policy <span class="math inline">\(\pi_\theta\)</span>, in comparison to the one obtained with the old policy <span class="math inline">\(\pi_{\theta_\text{old}}\)</span>. While this formula seems interesting (it measures how good the new policy is with regard to the average performance of the old policy, so we could optimize directly), it is difficult to estimate as the mathematical expectation depends on state-action pairs generated by the new policy : <span class="math inline">\(s \sim \rho_{\pi_\theta}, a \sim \pi_\theta\)</span>.</p>
<p><span class="citation" data-cites="Schulman2015">Schulman et al. (<a href="References.html#ref-Schulman2015" role="doc-biblioref">2015</a><a href="References.html#ref-Schulman2015" role="doc-biblioref">a</a>)</span> propose an approximation to this formula, by considering that if the two policies <span class="math inline">\(\pi_\theta\)</span> and <span class="math inline">\(\pi_{\theta_\text{old}}\)</span> are not very different from another, one can sample the states from the old distribution:</p>
<p><span class="math display">\[
    \eta(\theta) \approx \eta(\theta_\text{old}) + \mathbb{E}_{s \sim \rho_{\pi_{\theta_\text{old}}}, a \sim \pi_\theta} [A_{\pi_{\theta_\text{old}}}(s, a)]
\]</span></p>
<p>One can already recognize the main motivation behind natural gradients: finding a weight update that moves the policy in the right direction (getting more rewards) while keeping the change in the policy distribution as small as possible (to keep the assumption correct).</p>
<p>Let’s now define the following objective function:</p>
<p><span class="math display">\[
    J_{\theta_\text{old}}(\theta) = \eta(\theta_\text{old}) + \mathbb{E}_{s \sim \rho_{\pi_{\theta_\text{old}}}, a \sim \pi_\theta} [A_{\pi_{\theta_\text{old}}}(s, a)]
\]</span></p>
<p>It is easy to see that <span class="math inline">\(J_{\theta_\text{old}}(\theta_\text{old}) = \eta(\theta_\text{old})\)</span> by definition of the advantage, and that its gradient w.r.t to <span class="math inline">\(\theta\)</span> taken in <span class="math inline">\(\theta_\text{old}\)</span> is the same as the one of <span class="math inline">\(\eta(\theta_\text{old})\)</span>:</p>
<p><span class="math display">\[
    \nabla_\theta J_{\theta_\text{old}}(\theta)|_{\theta = \theta_\text{old}} = \nabla_\theta \eta(\theta)|_{\theta = \theta_\text{old}}
\]</span></p>
<p>This means that, at least locally, one maximization step of <span class="math inline">\(J_{\theta_\text{old}}(\theta)\)</span> goes in the same direction as maximizing <span class="math inline">\(\eta(\theta)\)</span> if we do not go too far. <span class="math inline">\(J\)</span> is called a <strong>surrogate objective function</strong>: it is not what we want to optimize, but it leads to the same result. TRPO belongs to the class of <strong>minorization-majorization</strong> algorithms (MM, we first find a local lower bound and then maximize it, iteratively).</p>
<p>Let’s now suppose that we can find its maximum, i.e. a policy <span class="math inline">\(\pi&#39;\)</span> that maximizes the advantage of each state-action pair over <span class="math inline">\(\pi_{\theta_\text{old}}\)</span>. There would be no guarantee that <span class="math inline">\(\pi&#39;\)</span> and <span class="math inline">\(\pi_{\theta_\text{old}}\)</span> are close enough so that the assumption stands. We could therefore make only a small step in its direction and hope for the best:</p>
<p><span id="eq:linesearch"><span class="math display">\[
    \pi_\theta(s, a) = (1-\alpha) \, \pi_{\theta_\text{old}}(s, a) + \alpha \, \pi&#39;(s,a)
\qquad(14)\]</span></span></p>
<p>This is the conservative policy iteration method of <span class="citation" data-cites="Kakade2002">Kakade and Langford (<a href="References.html#ref-Kakade2002" role="doc-biblioref">2002</a>)</span>, where a bound on the difference between <span class="math inline">\(\eta(\pi_{\theta_\text{old}})\)</span> and <span class="math inline">\(J_{\theta_\text{old}}(\theta)\)</span> can be derived.</p>
<p><span class="citation" data-cites="Schulman2015">Schulman et al. (<a href="References.html#ref-Schulman2015" role="doc-biblioref">2015</a><a href="References.html#ref-Schulman2015" role="doc-biblioref">a</a>)</span> propose to penalize instead the objective function by the KL divergence between the new and old policies. There are basically two ways to penalize an optimization problem:</p>
<ol type="1">
<li>Adding a hard constraint on the KL divergence, leading to a constrained optimization problem (where Lagrange methods can be applied):</li>
</ol>
<p><span class="math display">\[
    \text{maximize}_\theta \qquad J_{\theta_\text{old}}(\theta) \\
    \qquad \text{subject to} \qquad D_{KL}(\pi_{\theta_\text{old}} || \pi_\theta) \leq \delta
\]</span></p>
<ol start="2" type="1">
<li>Regularizing the objective function with the KL divergence:</li>
</ol>
<p><span class="math display">\[
    \text{maximize}_\theta \qquad L(\theta) = J_{\theta_\text{old}}(\pi_\theta) - C \,  D_{KL}(\pi_{\theta_\text{old}} || \pi_\theta)
\]</span></p>
<p>In the first case, we force the KL divergence to stay below a certain threshold. In the second case, we penalize solutions that would maximize <span class="math inline">\(J_{\theta_\text{old}}(\theta)\)</span> but would be too different from the previous policy. In both cases, we want to find a policy <span class="math inline">\(\pi_\theta\)</span> maximizing the expected return (the objective), but which is still close (in terms of KL divergence) from the current one. Both methods are however sensible to the choice of the parameters <span class="math inline">\(\delta\)</span> and <span class="math inline">\(C\)</span>.</p>
<p>Formally, the KL divergence <span class="math inline">\(D_{KL}(\pi_{\theta_\text{old}} || \pi_\theta)\)</span> should be the maximum KL divergence over the state space:</p>
<p><span class="math display">\[
    D^\text{max}_{KL}(\pi_{\theta_\text{old}} || \pi_\theta) = \max_s D_{KL}(\pi_{\theta_\text{old}}(s, .) || \pi_\theta(s, .))
\]</span></p>
<p>This maximum KL divergence over the state space would be very hard to compute. Empirical evaluations showed however that it is safe to use the mean KL divergence, or even to sample it:</p>
<p><span class="math display">\[
    \bar{D}_{KL}(\pi_{\theta_\text{old}} || \pi_\theta) = \mathbb{E}_s [D_{KL}(\pi_{\theta_\text{old}}(s, .) || \pi_\theta(s, .))] \approx \frac{1}{N} \sum_{i=1}^N D_{KL}(\pi_{\theta_\text{old}}(s_i, .) || \pi_\theta(s_i, .))
\]</span></p>
<h4 id="sec:trust-regions" class="unnumbered">Trust regions</h4>
<figure>
<img src="img/trustregion.png" id="fig:trustregion" style="width:70.0%" alt="" /><figcaption>Figure 30: Graphical illustration of trust regions. From the current parameters <span class="math inline">\(\theta_\text{old}\)</span>, we search for the maximum <span class="math inline">\(\theta^*\)</span> of the real objective <span class="math inline">\(\eta(\theta)\)</span>. The unconstrained objective <span class="math inline">\(J_{\theta_\text{old}}(\theta)\)</span> is locally similar to <span class="math inline">\(\eta(\theta)\)</span> but quickly diverge as <span class="math inline">\(\pi_\theta\)</span> and <span class="math inline">\(\pi_{\theta_\text{old}}\)</span> become very different. The surrogate objective <span class="math inline">\(L(\theta) = J_{\theta_\text{old}} (\theta) - C \, D_{KL}(\pi_{\theta_\text{old}} || \pi_\theta)\)</span> is always smaller than <span class="math inline">\(\eta(\theta)\)</span> and has a maximum close to <span class="math inline">\(\theta^*\)</span> which keeps <span class="math inline">\(\pi_\theta\)</span> and <span class="math inline">\(\pi_{\theta_\text{old}}\)</span> close from each other in terms of KL divergence. The region around <span class="math inline">\(\theta_\text{old}\)</span> where big optimization steps can be taken without changing the policy too much is called the trust region.</figcaption>
</figure>
<p>Before diving further into how these optimization problems can be solved, let’s wonder why the algorithm is called <strong>trust region policy optimization</strong> using the regularized objective. Fig. <a href="#fig:trustregion">30</a> illustrates the idea. The “real” objective function <span class="math inline">\(\eta(\theta)\)</span> should be maximized (with gradient descent or similar) starting from the parameters <span class="math inline">\(\theta_\text{old}\)</span>. We cannot estimate the objective function directly, so we build a surrogate objective function <span class="math inline">\(L(\theta) = J_{\theta_\text{old}} (\theta) - C \, D_{KL}(\pi_{\theta_\text{old}} || \pi_\theta)\)</span>. We know that:</p>
<ol type="1">
<li>The two objectives have the same value in <span class="math inline">\(\theta_\text{old}\)</span>: <span class="math display">\[L(\theta_\text{old}) = J_{\theta_\text{old}}(\theta_\text{old}) - C \,  D_{KL}(\pi_{\theta_\text{old}} || \pi_{\theta_\text{old}}) = \eta(\theta_\text{old})\]</span></li>
<li>Their gradient w.r.t <span class="math inline">\(\theta\)</span> are the same in <span class="math inline">\(\theta_\text{old}\)</span>: <span class="math display">\[\nabla_\theta L(\theta)|_{\theta = \theta_\text{old}} = \nabla_\theta \eta(\theta)|_{\theta = \theta_\text{old}}\]</span></li>
<li>The surrogate objective is always smaller than the real objective, as the KL divergence is positive: <span class="math display">\[\eta(\theta) \geq J_{\theta_\text{old}}(\theta) - C \,  D_{KL}(\pi_{\theta_\text{old}} || \pi_\theta)\]</span>.</li>
</ol>
<p>Under these conditions, the surrogate objective is also called a <strong>lower bound</strong> of the primary objective. The interesting fact is that the value of <span class="math inline">\(\theta\)</span> that maximizes <span class="math inline">\(L(\theta)\)</span> is at the same time:</p>
<ul>
<li>A big step in the parameter space towards the maximum of <span class="math inline">\(\eta(\theta)\)</span>, as <span class="math inline">\(\theta\)</span> and <span class="math inline">\(\theta_\text{old}\)</span> can be very different.</li>
<li>A small step in the policy distribution space, as the KL divergence between the previous and the new policies is kept small.</li>
</ul>
<p>Exactly what we needed! The parameter region around <span class="math inline">\(\theta_\text{old}\)</span> where the KL divergence s kept small is called the <strong>trust region</strong>. This means that we can safely take big optimization steps (e.g. with a high learning rate or even analytically) without risking to violate the initial assumptions.</p>
<h4 id="sec:sample-based-formulation" class="unnumbered">Sample-based formulation</h4>
<p>Although the theoretical proofs in <span class="citation" data-cites="Schulman2015">Schulman et al. (<a href="References.html#ref-Schulman2015" role="doc-biblioref">2015</a><a href="References.html#ref-Schulman2015" role="doc-biblioref">a</a>)</span> used the regularized optimization method, the practical implementation uses the constrained optimization problem:</p>
<p><span class="math display">\[
    \text{maximize}_\theta \qquad J_{\theta_\text{old}}(\theta) = \eta(\theta_\text{old}) + \mathbb{E}_{s \sim \rho_{\pi_{\theta_\text{old}}}, a \sim \pi_\theta} [A_{\pi_{\theta_\text{old}}}(s, a)] \\
    \qquad \text{subject to} \qquad D_{KL}(\pi_{\theta_\text{old}} || \pi_\theta) \leq \delta
\]</span></p>
<p>The first thing to notice is that <span class="math inline">\(\eta(\theta_\text{old})\)</span> does not depend on <span class="math inline">\(\theta\)</span>, so it is constant in the optimization problem. We only need to maximize the advantage of the actions taken by <span class="math inline">\(\pi_\theta\)</span> in each state visited by <span class="math inline">\(\pi_{\theta_\text{old}}\)</span>. The problem is that <span class="math inline">\(\pi_\theta\)</span> is what we search, so we can not sample actions from it. The solution is to use <strong>importance sampling</strong> (see Section <a href="./ImportanceSampling.html#sec:importance-sampling">4.3.1</a>) to allow sampling actions from <span class="math inline">\(\pi_{\theta_\text{old}}\)</span>. This is possible as long as we correct the objective with the importance sampling weight:</p>
<p><span class="math display">\[
    \text{maximize}_\theta \qquad \mathbb{E}_{s \sim \rho_{\pi_{\theta_\text{old}}}, a \sim \pi_{\theta_\text{old}}} [\frac{\pi_\theta(s, a)}{\pi_{\theta_\text{old}}(s, a)} \, A_{\pi_{\theta_\text{old}}}(s, a)] \\
    \qquad \text{subject to} \qquad D_{KL}(\pi_{\theta_\text{old}} || \pi_\theta) \leq \delta
\]</span></p>
<p>Now that states and actions are generated by the old policy, we can safely sample many trajectories using <span class="math inline">\(\pi_{\theta_\text{old}}\)</span> (<span class="citation" data-cites="Schulman2015">Schulman et al. (<a href="References.html#ref-Schulman2015" role="doc-biblioref">2015</a><a href="References.html#ref-Schulman2015" role="doc-biblioref">a</a>)</span> proposes two methods called single path and Vine, but we ignore it here), compute the advantages of all state-action pairs (using real rewards along the trajectories), form the surrogate objective function and optimize it using second-order optimization methods.</p>
<p>One last thing to notice is that the advantages <span class="math inline">\(A_{\pi_{\theta_\text{old}}}(s, a) = Q_{\pi_{\theta_\text{old}}}(s, a) - V_{\pi_{\theta_\text{old}}}(s)\)</span> depend on the value of the states encountered by <span class="math inline">\(\pi_{\theta_\text{old}}\)</span>. The state values do not depend on the policies, they are constant for each optimization step, so they can also be safely removed:</p>
<p><span class="math display">\[
    \text{maximize}_\theta \qquad \mathbb{E}_{s \sim \rho_{\pi_{\theta_\text{old}}}, a \sim \pi_{\theta_\text{old}}} [\frac{\pi_\theta(s, a)}{\pi_{\theta_\text{old}}(s, a)} \, Q_{\pi_{\theta_\text{old}}}(s, a)] \\
    \qquad \text{subject to} \qquad D_{KL}(\pi_{\theta_\text{old}} || \pi_\theta) \leq \delta
\]</span></p>
<p>Here we go, that’s TRPO. It could seem a bit disappointing to come up with such a simple formulation (find a policy which maximizes the Q-value of sampled actions while being not too different from the previous one) after so many mathematical steps, but that is also the beauty of it: not only it works, but it is guaranteed to work. With TRPO, each optimization step brings the policy closer from an optimum, what is called <strong>monotonic improvement</strong>.</p>
<h4 id="sec:practical-implementation" class="unnumbered">Practical implementation</h4>
<p>Now, how do we solve the constrained optimization problem? And what is the link with natural gradients?</p>
<p>To solve constrained optimization problems, we can form a Lagrange function with an additional parameter <span class="math inline">\(\lambda\)</span> and search for its maximum:</p>
<p><span class="math display">\[
    \mathcal{L}(\theta, \lambda) = J_{\theta_\text{old}}(\theta)  - \lambda \, (D_{KL}(\pi_{\theta_\text{old}} || \pi_\theta) - \delta)
\]</span></p>
<p>Notice how close the Lagrange function is from the regularized problem used in the theory. We can form a first-order approximation of <span class="math inline">\(J_{\theta_\text{old}}(\theta)\)</span>:</p>
<p><span class="math display">\[
    J_{\theta_\text{old}}(\theta) = \nabla_\theta J_{\theta_\text{old}}(\theta) \, (\theta- \theta_\text{old})
\]</span></p>
<p>as <span class="math inline">\(J_{\theta_\text{old}}(\theta_\text{old}) = 0\)</span>. <span class="math inline">\(g = \nabla_\theta J_{\theta_\text{old}}(\theta)\)</span> is the now familiar <strong>policy gradient</strong> with importance sampling. Higher-order terms do not matter, as they are going to be dominated by the KL divergence term.</p>
<p>As in Section <a href="./NaturalGradient.html#sec:principle-of-natural-gradients">4.5.1</a>, we will use a second-order approximation of the KL divergence term using the Fisher Information Matrix:</p>
<p><span class="math display">\[
    D_{KL}(\pi_{\theta_\text{old}} || \pi_\theta) = (\theta- \theta_\text{old})^T \, F(\theta_\text{old}) \,  (\theta- \theta_\text{old})
\]</span></p>
<p>We get the following Lagrangian function:</p>
<p><span class="math display">\[
    \mathcal{L}(\theta, \lambda) = \nabla_\theta J_{\theta_\text{old}}(\theta) \, (\theta- \theta_\text{old})  - \lambda \, (\theta- \theta_\text{old})^T \, F(\theta_\text{old}) \,  (\theta- \theta_\text{old})
\]</span></p>
<p>which is quadratic in <span class="math inline">\(\Delta \theta = \theta- \theta_\text{old}\)</span>. It has therefore a unique maximum, characterized by a first-order derivative equal to 0:</p>
<p><span class="math display">\[
    \nabla_\theta J_{\theta_\text{old}}(\theta) = \lambda \, F(\theta_\text{old}) \,  \Delta \theta
\]</span></p>
<p>or:</p>
<p><span class="math display">\[
    \Delta \theta  = \frac{1}{\lambda} \, F(\theta_\text{old})^{-1} \,  \nabla_\theta J_{\theta_\text{old}}(\theta)
\]</span></p>
<p>which is the <strong>natural gradient descent</strong>! The size of the step <span class="math inline">\(\frac{1}{\lambda}\)</span> still has to be determined, but it can also be replaced by a fixed hyperparameter.</p>
<p>The main problem is now to compute and inverse the Fisher information matrix, which is quadratic with the number of parameters <span class="math inline">\(\theta\)</span>, i.e. with the number of weights in the NN. <span class="citation" data-cites="Schulman2015">Schulman et al. (<a href="References.html#ref-Schulman2015" role="doc-biblioref">2015</a><a href="References.html#ref-Schulman2015" role="doc-biblioref">a</a>)</span> proposes to used <strong>conjugate gradients</strong> to iteratively approximate the Fisher, a second-order method which will not be presented here (see <a href="https://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf" class="uri">https://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf</a> for a detailed introduction). After the conjugate gradient optimization step, the constraint <span class="math inline">\(D_{KL}(\pi_{\theta_\text{old}} || \pi_\theta) \leq \delta\)</span> is however not ensured anymore, so a line search is made as in Eq. <a href="#eq:linesearch">14</a> until that criteria is met.</p>
<h4 id="sec:summary" class="unnumbered">Summary</h4>
<p>TRPO is a policy gradient method using natural gradients to monotonically improve the expected return associated to the policy. As a minorization-maximization (MM) method, it uses a surrogate objective function (a lower bound on the expected return) to iteratively change the parameters of the policy using large steps, but without changing the policy too much (as measured by the KL divergence). Its main advantage over DDPG is that it is much less sensible to the choice of the learning rate.</p>
<p>However, it has several limitations:</p>
<ul>
<li>It is hard to use with neural networks having multiple outputs (e.g. the policy and the value function, as in actor-critic methods) as natural gradients are dependent on the policy distribution and its relationship with the parameters.</li>
<li>It works well when the NN has only fully-connected layers, but empirically performs poorly on tasks requiring convolutional layers or recurrent layers.</li>
<li>The use of conjugate gradients makes the implementation much more complicated and less flexible than regular SGD.</li>
</ul>
<p><strong>Additional resources</strong></p>
<ul>
<li><a href="http://178.79.149.207/posts/trpo.html" class="uri">http://178.79.149.207/posts/trpo.html</a></li>
<li><a href="https://towardsdatascience.com/introduction-to-various-reinforcement-learning-algorithms-part-ii-trpo-ppo-87f2c5919bb9" class="uri">https://towardsdatascience.com/introduction-to-various-reinforcement-learning-algorithms-part-ii-trpo-ppo-87f2c5919bb9</a></li>
<li><a href="https://medium.com/@sanketgujar95/trust-region-policy-optimization-trpo-and-proximal-policy-optimization-ppo-e6e7075f39ed" class="uri">https://medium.com/@sanketgujar95/trust-region-policy-optimization-trpo-and-proximal-policy-optimization-ppo-e6e7075f39ed</a></li>
<li><a href="https://www.depthfirstlearning.com/2018/TRPO" class="uri">https://www.depthfirstlearning.com/2018/TRPO</a></li>
<li><a href="http://rll.berkeley.edu/deeprlcoursesp17/docs/lec5.pdf" class="uri">http://rll.berkeley.edu/deeprlcoursesp17/docs/lec5.pdf</a></li>
</ul>
<h3 id="sec:proximal-policy-optimization-ppo"><span class="header-section-number">4.5.4</span> Proximal Policy Optimization (PPO)</h3>
<p>Proximal Policy Optimization (PPO) was proposed by <span class="citation" data-cites="Schulman2017">Schulman et al. (<a href="References.html#ref-Schulman2017" role="doc-biblioref">2017</a><a href="References.html#ref-Schulman2017" role="doc-biblioref">b</a>)</span> to overcome the problems of TRPO (complexity, inability to share parameters or to use complex NN architectures) while increasing the range of tasks learnable by the system (compared to DQN) and improving the sample complexity (compared to online PG methods, which perform only one update per step).</p>
<p>For that, they investigated various surrogate objectives (lower bounds) that could be solved using first-order optimization techniques (gradient descent). Let’s rewrite the surrogate loss of TRPO in the following manner:</p>
<p><span class="math display">\[
    L^\text{CPI}(\theta) = \mathbb{E}_{t} [\frac{\pi_\theta(s_t, a_t)}{\pi_{\theta_\text{old}}(s_t, a_t)} \, A_{\pi_{\theta_\text{old}}}(s_t, a_t)] = \mathbb{E}_{t} [\rho_t(\theta) \, A_{\pi_{\theta_\text{old}}}(s_t, a_t)]
\]</span></p>
<p>by making the dependency over time explicit and noting the importance sampling weight <span class="math inline">\(\rho_t(\theta)\)</span>. The superscript CPI refers to conservative policy iteration <span class="citation" data-cites="Kakade2002">(Kakade and Langford, <a href="References.html#ref-Kakade2002" role="doc-biblioref">2002</a>)</span>. Without a constraint, the maximization of <span class="math inline">\(L^\text{CPI}\)</span> would lead to an excessively large policy updates. The authors searched how to modify the objective, in order to penalize changes to the policy that make <span class="math inline">\(\rho_t(\theta)\)</span> very different from 1, i.e. where the KL divergence between the new and old policies would become high. They ended up with the following surrogate loss:</p>
<p><span class="math display">\[
    L^\text{CLIP}(\theta) = \mathbb{E}_{t} [ \min (\rho_t(\theta) \, A_{\pi_{\theta_\text{old}}}(s_t, a_t), \text{clip}(\rho_t(\theta) , 1- \epsilon, 1+\epsilon) \,  A_{\pi_{\theta_\text{old}}}(s_t, a_t)]
\]</span></p>
<p>The left part of the min operator is the surrogate objective of TRPO <span class="math inline">\(L^\text{CPI}(\theta)\)</span>. The right part restricts the importance sampling weight between <span class="math inline">\(1-\epsilon\)</span> and <span class="math inline">\(1 +\epsilon\)</span>. Let’s consider two cases (depicted on Fig. <a href="#fig:ppo">31</a>):</p>
<figure>
<img src="img/ppo.png" id="fig:ppo" style="width:80.0%" alt="" /><figcaption>Figure 31: Illustration of the effect of clipping the importance sampling weight. Taken from <span class="citation" data-cites="Schulman2017">Schulman et al. (<a href="References.html#ref-Schulman2017" role="doc-biblioref">2017</a><a href="References.html#ref-Schulman2017" role="doc-biblioref">b</a>)</span>.</figcaption>
</figure>
<ol type="1">
<li><p>the transition <span class="math inline">\(s_t, a_t\)</span> has a positive advantage, i.e. it is a better action than expected. The probability of selecting that action again should be increased, i.e. <span class="math inline">\(\pi_\theta(s_t, a_t) &gt; \pi_{\theta_\text{old}}(s_t, a_t)\)</span>. However, the importance sampling weight could become very high (a change from 0.01 to 0.05 is a ration of <span class="math inline">\(\rho_t(\theta) = 5\)</span>). In that case, <span class="math inline">\(\rho_t(\theta)\)</span> will be clipped to <span class="math inline">\(1+\epsilon\)</span>, for example 1.2. As a consequence, the parameters <span class="math inline">\(\theta\)</span> will move in the right direction, but the distance between the new and the old policies will stay small.</p></li>
<li><p>the transition <span class="math inline">\(s_t, a_t\)</span> has a negative advantage, i.e. it is a worse action than expected. Its probability will be decreased and the importance sampling weight might become much smaller than 1. Clipping it to <span class="math inline">\(1-\epsilon\)</span> avoids drastic changes to the policy, while still going in the right direction.</p></li>
</ol>
<p>Finally, they take the minimum of the clipped and unclipped objective, so that the final objective is a lower bound of the unclipped objective. In the original paper, they use <strong>generalized advantage estimation</strong> (GAE, Section <a href="./ActorCritic.html#sec:generalized-advantage-estimation-gae">4.2.3</a>) to estimate <span class="math inline">\(A_{\pi_{\theta_\text{old}}}(s_t, a_t)\)</span>, but anything could be used (n-steps, etc). Transitions are sampled by multiple actors in parallel, as in A2C (Section <a href="./ActorCritic.html#sec:advantage-actor-critic-a2c">4.2.1</a>).</p>
<p>The pseudo-algorithm of PPO is as follows:</p>
<hr />
<ul>
<li><p>Initialize an actor <span class="math inline">\(\pi_\theta\)</span> and a critic <span class="math inline">\(V_\varphi\)</span> with random weights.</p></li>
<li><p>while not converged :</p>
<ul>
<li><p>for <span class="math inline">\(N\)</span> actors in parallel:</p>
<ul>
<li><p>Collect <span class="math inline">\(T\)</span> transitions using <span class="math inline">\(\pi_\text{old}\)</span>.</p></li>
<li><p>Compute the generalized advantage of each transition using the critic.</p></li>
</ul></li>
<li><p>for <span class="math inline">\(K\)</span> epochs:</p>
<ul>
<li><p>Sample <span class="math inline">\(M\)</span> transitions from the ones previously collected.</p></li>
<li><p>Train the actor to maximize the clipped surrogate objective.</p></li>
<li><p>Train the critic to minimize the mse using TD learning.</p></li>
</ul></li>
<li><p><span class="math inline">\(\theta_\text{old} \leftarrow \theta\)</span></p></li>
</ul></li>
</ul>
<hr />
<p>The main advantage of PPO with respect to TRPO is its simplicity: the clipped objective can be directly maximized using first-order methods like stochastic gradient descent or Adam. It does not depend on assumptions about the parameter space: CNNs and RNNs can be used for the policy. It is sample-efficient, as several epochs of parameter updates are performed between two transition samplings: the policy network therefore needs less fresh samples that strictly on-policy algorithms to converge.</p>
<p>The only drawbacks of PPO is that there no convergence guarantee (although in practice it converges more often than other state-of-the-art methods) and that the right value for <span class="math inline">\(\epsilon\)</span> has to be determined. PPO has improved the state-of-the-art on Atari games and Mujoco robotic tasks. It has become the go-to method for continuous control problems.</p>
<p><strong>Additional resources</strong></p>
<ul>
<li>More explanations and demos from OpenAI: <a href="https://blog.openai.com/openai-baselines-ppo" class="uri">https://blog.openai.com/openai-baselines-ppo</a></li>
</ul>
<h3 id="sec:actor-critic-with-experience-replay-acer"><span class="header-section-number">4.5.5</span> Actor-Critic with Experience Replay (ACER)</h3>
<p>The natural gradient methods presented above are stochastic actor-critic methods, therefore strictly on-policy. Off-policy methods such as DQN or DDPG allow to reuse past transitions through the usage of an <strong>experience replay memory</strong>, potentially reducing the sample complexity at the cost of a higher variance and worse stability (Section <a href="./ImportanceSampling.html#sec:off-policy-actor-critic">4.3</a>). <span class="citation" data-cites="Wang2017">Wang et al. (<a href="References.html#ref-Wang2017" role="doc-biblioref">2017</a>)</span> proposed an off-policy actor-critic architecture using variance reduction techniques, the off-policy Retrace algorithm <span class="citation" data-cites="Munos2016">(Munos et al., <a href="References.html#ref-Munos2016" role="doc-biblioref">2016</a>, Section <a href="./ImportanceSampling.html#sec:retrace" role="doc-biblioref">4.3.3</a>)</span>, parallel training of multiple actor-learners <span class="citation" data-cites="Mnih2016">(Mnih et al., <a href="References.html#ref-Mnih2016" role="doc-biblioref">2016</a>, Section <a href="./ActorCritic.html#sec:asynchronous-advantage-actor-critic-a3c" role="doc-biblioref">4.2.2</a>)</span>, truncated importance sampling with bias correction (Section <a href="./ImportanceSampling.html#sec:importance-sampling">4.3.1</a>), stochastic duelling network architectures <span class="citation" data-cites="Wang2016">(Wang et al., <a href="References.html#ref-Wang2016" role="doc-biblioref">2016</a>, Section <a href="./Valuebased.html#sec:duelling-network" role="doc-biblioref">3.5</a>)</span>, and efficient trust region policy optimization. It can be seen as the off-policy counterpart to A3C.</p>
<p>The first aspect of ACER is that it interleaves on-policy learning with off-policy: the agent samples a trajectory <span class="math inline">\(\tau\)</span>, learns from it on-policy, stores it in the replay buffer, samples <span class="math inline">\(n\)</span> trajectories from the replay buffer and learns off-policy from each of them:</p>
<hr />
<ul>
<li><p>Sample a trajectory <span class="math inline">\(\tau\)</span> using the current policy.</p></li>
<li><p>Apply ACER on-policy on <span class="math inline">\(\tau\)</span>.</p></li>
<li><p>Store <span class="math inline">\(\tau\)</span> in the replay buffer.</p></li>
<li><p>Sample <span class="math inline">\(n\)</span> trajectories from the replay buffer.</p></li>
<li><p>for each sampled trajectory <span class="math inline">\(\tau_k\)</span>:</p>
<ul>
<li>Apply ACER off-policy on <span class="math inline">\(\tau_k\)</span>.</li>
</ul></li>
</ul>
<hr />
<p>Mixing on-policy learning with off-policy is quite similar to the Self-Imitation Learning approach of <span class="citation" data-cites="Oh2018">(Oh et al., <a href="References.html#ref-Oh2018" role="doc-biblioref">2018</a>, Section <a href="./ImportanceSampling.html#sec:self-imitation-learning-sil" role="doc-biblioref">4.3.4</a>)</span>.</p>
<h4 id="sec:retrace-evaluation" class="unnumbered">Retrace evaluation</h4>
<p>ACER comes in two flavors: one for discrete action spaces, one for continuous spaces. The discrete version is simpler, so let’s focus on this one. As any policy-gradient method, ACER tries to estimate the policy gradient for each transition of a trajectory, but using importance sampling <span class="citation" data-cites="Degris2012">(Degris et al., <a href="References.html#ref-Degris2012" role="doc-biblioref">2012</a>, Section <a href="./ImportanceSampling.html#sec:linear-off-policy-actor-critic-off-pac" role="doc-biblioref">4.3.2</a>)</span>:</p>
<p><span class="math display">\[
    \nabla_\theta J(\theta)  = \mathbb{E}_{s_t, a_t \sim \rho_b} [\frac{\pi_\theta(s_t, a_t)}{b(s_t, a_t)} \, Q_\varphi(s_t, a_t) \, \nabla_\theta \log \pi_\theta(s_t, a_t)]
\]</span></p>
<p>The problem is now to train the critic <span class="math inline">\(Q_\varphi(s_t, a_t)\)</span> by computing the correct target. ACER learning builds on the Retrace algorithm <span class="citation" data-cites="Munos2016">(Munos et al., <a href="References.html#ref-Munos2016" role="doc-biblioref">2016</a>, Section <a href="./ImportanceSampling.html#sec:retrace" role="doc-biblioref">4.3.3</a>)</span>:</p>
<p><span class="math display">\[
    \Delta Q_\varphi(s_t, a_t) = \alpha \, \sum_{t&#39;=t}^T (\gamma)^{t&#39;-t} \left(\prod_{s=t+1}^{t&#39;} c_s \right) \, \delta_{t&#39;}
\]</span></p>
<p>with <span class="math inline">\(c_s = \lambda \min (1, \frac{\pi_\theta(s_s, a_s)}{b(s_s, a_s)})\)</span> being the clipped importance sampling weight and <span class="math inline">\(\delta_{t&#39;}\)</span> is the TD error at time <span class="math inline">\(t&#39;&gt;t\)</span>:</p>
<p><span class="math display">\[
    \delta_{t&#39;} = r_{t&#39;+1} + \gamma \, V(s_{t&#39;+1}) - V(s_{t&#39;})
\]</span></p>
<p>By noting <span class="math inline">\(Q^\text{ret}\)</span> the target value for the update of the critic (neglecting the learning rate <span class="math inline">\(\alpha\)</span>):</p>
<p><span class="math display">\[
    Q^\text{ret}(s_t, a_t) = Q_\varphi(s_t, a_t) +  \sum_{t&#39;=t}^T (\gamma)^{t&#39;-t} \left(\prod_{s=t+1}^{t&#39;} c_s \right) \, \delta_{t&#39;}
\]</span></p>
<p>we can transform the Retrace formula into a recurrent one:</p>
<p><span class="math display">\[
\begin{aligned}
    Q^\text{ret}(s_t, a_t) &amp; = Q_\varphi(s_t, a_t) + \delta_t + \sum_{t&#39;=t+1}^T (\gamma)^{t&#39;-t} \left(\prod_{s=t+1}^{t&#39;} c_s \right) \, \delta_{t&#39;} \\
    &amp; = Q_\varphi(s_t, a_t) + \delta_t + \gamma \, c_{t+1} \, (Q^\text{ret}(s_{t+1}, a_{t+1}) - Q_\varphi(s_{t+1}, a_{t+1})) \\
\end{aligned}
\]</span></p>
<p><span class="math inline">\(Q_\varphi(s_t, a_t) + \delta_t = Q_\varphi(s_t, a_t) + r_{t+1} + \gamma \, V(s_{t+1}) - V(s_t)\)</span> can furthermore be reduced to <span class="math inline">\(r_{t+1} + \gamma \, V(s_{t+1})\)</span> by considering that <span class="math inline">\(Q_\varphi(s_t, a_t) \approx V(s_t)\)</span> (the paper does not justify this assumption, but it should be true in expectation).</p>
<p>This gives us the following target value for the Q-values:</p>
<p><span class="math display">\[
    Q^\text{ret}(s_t, a_t) = r_{t+1} + \gamma \, c_{t+1} \, (Q^\text{ret}(s_{t+1}, a_{t+1}) - Q_\varphi(s_{t+1}, a_{t+1})) + \gamma \, V(s_{t+1})
\]</span></p>
<p>One remaining issue is that the critic would also need to output the value of each state <span class="math inline">\(V(s_{t+1})\)</span>, in addition to the Q-values <span class="math inline">\(Q_\varphi(s_t, a_t)\)</span>. In the discrete case, this is not necessary, as the value of a state is the expectation of the value of the available actions under the current policy:</p>
<p><span class="math display">\[
    V(s_{t+1}) = \mathbb{E}_{a_{t+1} \sim \pi_\theta} [Q_\varphi(s_{t+1}, a_{t+1})] = \sum_a \pi_\theta(s_{t+1}, a) \, Q_\varphi(s_{t+1}, a))
\]</span></p>
<p>The value of the next state can be easily computed when we have the policy <span class="math inline">\(\pi_\theta(s, a)\)</span> (actor) and the Q-value <span class="math inline">\(Q_\varphi(s, a)\)</span> (critic) of each action <span class="math inline">\(a\)</span> in a state <span class="math inline">\(s\)</span>.</p>
<p>The actor-critic architecture needed for ACER is therefore the following:</p>
<ul>
<li><p>The <strong>actor</strong> <span class="math inline">\(\pi_\theta\)</span> takes a state <span class="math inline">\(s\)</span> as input and outputs a vector of probabilities <span class="math inline">\(\pi_\theta\)</span> for each available action.</p></li>
<li><p>The <strong>critic</strong> <span class="math inline">\(Q_\varphi\)</span> takes a state <span class="math inline">\(s\)</span> as input and outputs a vector of Q-values.</p></li>
</ul>
<p>This is different from the architecture of A3C, where the critic “only” had to output the value of a state <span class="math inline">\(V_\varphi(s)\)</span>: it is now a vector of Q-values. Note that the actor and the critic can share most of their parameters: the network only needs to output two different vectors <span class="math inline">\(\pi_\theta(s)\)</span> and <span class="math inline">\(Q_\varphi(s)\)</span> for each input state <span class="math inline">\(s\)</span> (Fig. <a href="#fig:acer">32</a>). This makes a “two heads” NN, similar to the <strong>duelling architecture</strong> of <span class="citation" data-cites="Wang2016">(Wang et al., <a href="References.html#ref-Wang2016" role="doc-biblioref">2016</a>, Section <a href="./Valuebased.html#sec:duelling-network" role="doc-biblioref">3.5</a>)</span>.</p>
<figure>
<img src="img/acer.png" id="fig:acer" style="width:90.0%" alt="" /><figcaption>Figure 32: Architecture of the ACER actor-critic.</figcaption>
</figure>
<p>The target Q-value <span class="math inline">\(Q^\text{ret}(s, a)\)</span> can be found recursively by iterating backwards over the episode:</p>
<hr />
<ul>
<li><p>Initialize <span class="math inline">\(Q^\text{ret}(s_T, a_T)\)</span>, <span class="math inline">\(Q_\varphi(s_T, a_T)\)</span> and <span class="math inline">\(V(s_T)\)</span> to 0, as the terminal state has no value.</p></li>
<li><p>for <span class="math inline">\(t \in [T-1, \ldots, 0]\)</span>:</p>
<ul>
<li>Update the target Q-value using the received reward, the critic and the previous target value:</li>
</ul>
<p><span class="math display">\[
      Q^\text{ret}(s_t, a_t) = r_{t+1} + \gamma \, c_{t+1} \, (Q^\text{ret}(s_{t+1}, a_{t+1}) - Q_\varphi(s_{t+1}, a_{t+1})) + \gamma \, V(s_{t+1})
  \]</span></p>
<ul>
<li>Apply the critic on the current action:</li>
</ul>
<p><span class="math display">\[
      Q_\varphi(s_t, a_t)
  \]</span></p>
<ul>
<li>Estimate the value of the state using the critic:</li>
</ul>
<p><span class="math display">\[
      V(s_t) = \sum_a \pi_\theta(s_t) \, Q_\varphi(s_t, a)
  \]</span></p></li>
</ul>
<hr />
<p>As the target value <span class="math inline">\(Q^\text{ret}(s, a)\)</span> use multiple “real” rewards <span class="math inline">\(r_{t+1}\)</span>, it is actually less biased than the critic <span class="math inline">\(Q_\varphi(s, a)\)</span>. It is then better to use it to update the actor:</p>
<p><span class="math display">\[
    \nabla_\theta J(\theta)  = \mathbb{E}_{s_t, a_t \sim \rho_b} [\frac{\pi_\theta(s_t, a_t)}{b(s_t, a_t)} \, Q^\text{ret}(s_t, a_t) \, \nabla_\theta \log \pi_\theta(s_t, a_t)]
\]</span></p>
<p>The critic just has to minimize the mse with the target value:</p>
<p><span class="math display">\[
    \mathcal{L}(\varphi) = \mathbb{E}_{s_t, a_t \sim \rho_b} [(Q^\text{ret}(s, a) - Q_\varphi(s, a))^2]
\]</span></p>
<h4 id="sec:importance-weight-truncation-with-bias-correction" class="unnumbered">Importance weight truncation with bias correction</h4>
<p>When updating the actor, we rely on the importance sampling weight <span class="math inline">\(\rho_t = \frac{\pi_\theta(s_t, a_t)}{b(s_t, a_t)}\)</span> which can vary a lot and destabilize learning.</p>
<p><span class="math display">\[
    \nabla_\theta J(\theta)  = \mathbb{E}_{s_t, a_t \sim \rho_b} [\rho_t \, Q^\text{ret}(s_t, a_t) \, \nabla_\theta \log \pi_\theta(s_t, a_t)]
\]</span></p>
<p>PPO (Section <a href="./NaturalGradient.html#sec:proximal-policy-optimization-ppo">4.5.4</a>) solved this problem by clipping the importance sampling weight between <span class="math inline">\(1- \epsilon\)</span> and <span class="math inline">\(1+\epsilon\)</span>. ACER uses a similar strategy, but only using an upper bound <span class="math inline">\(c = 10\)</span> on the weight:</p>
<p><span class="math display">\[
    \bar{\rho}_t = \min(c, \rho_t)
\]</span></p>
<p>Using <span class="math inline">\(\bar{\rho}_t\)</span> in the policy gradient directly would introduce a bias: actions whose importance sampling weight <span class="math inline">\(\rho_t\)</span> is higher than <span class="math inline">\(c\)</span> would contribute to the policy gradient with a smaller value than they should, introducing a bias.</p>
<p>The solution in ACER is to add a <strong>bias correcting term</strong>, that corrects the policy gradient when an action has a weight higher than <span class="math inline">\(c\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
    \nabla_\theta J(\theta)  = &amp; \mathbb{E}_{s_t \sim \rho_b} [\mathbb{E}_{a_t \sim b} [\bar{\rho}_t \, Q^\text{ret}(s_t, a_t) \, \nabla_\theta \log \pi_\theta(s_t, a_t)] \\
    &amp; + \mathbb{E}_{a \sim \pi_\theta}[(\frac{\rho_t(a) - c}{\rho_t(a)})^+ \, Q_\varphi(s_t, a) \, \nabla_\theta \log \pi_\theta(s_t, a)]] \\
\end{aligned}
\]</span></p>
<p>The left part of that equation is the same policy gradient as before, but using a clipped importance sampling weight.</p>
<p>The right part requires to integrate over all possible actions in the state <span class="math inline">\(s_t\)</span> according to the learned policy <span class="math inline">\(\pi_\theta\)</span>, although only the action <span class="math inline">\(a_t\)</span> was selected by the behavior policy <span class="math inline">\(b\)</span>. The term <span class="math inline">\((\frac{\rho_t(a) - c}{\rho_t(a)})^+\)</span> is zero for all actions having an importance sampling weight smaller than c, and has a maximum of 1. In practice, this correction term can be computed using the vectors <span class="math inline">\(\pi_\theta(s, a)\)</span> and <span class="math inline">\(Q_\varphi(s, a)\)</span>, which are the outputs of the actor and the critic, respectively.</p>
<p>Finally, the Q-values <span class="math inline">\(Q^\text{ret}(s_t, a_t)\)</span> and <span class="math inline">\(Q_\varphi(s_t, a)\)</span> are transformed into advantages <span class="math inline">\(Q^\text{ret}(s_t, a_t) - V_\varphi(s_t)\)</span> and <span class="math inline">\(Q_\varphi(s_t, a) - V_\varphi(s_t)\)</span> by substracting the value of the state in order to reduce the variance of the policy gradient.</p>
<p>In short, we now have an estimator of the policy gradient which is <strong>unbiased</strong> and of smaller variance.</p>
<h4 id="sec:efficient-trust-region-policy-optimization" class="unnumbered">Efficient trust region policy optimization</h4>
<p>However, the variance is still too high. The last important step of ACER is an efficient TRPO update for the parameters of the actor.</p>
<p>A first component of their TRPO update is they use a <strong>target actor network</strong> <span class="math inline">\(\theta&#39;\)</span> (called averaged policy network in the paper) slowly tracking the actor <span class="math inline">\(\theta\)</span> after each update:</p>
<p><span class="math display">\[
    \theta&#39; \leftarrow \alpha \, \theta&#39; + (1 - \alpha) \, \theta
\]</span></p>
<p>A second component is that the actor is decomposed into two components:</p>
<ol type="1">
<li>a distribution <span class="math inline">\(f\)</span>.</li>
<li>the statistics <span class="math inline">\(\Phi_\theta(x)\)</span> of this distribution.</li>
</ol>
<p>This is what you do when you apply the softmax action selection on Q-values: the distribution is the Gibbs (or Boltzmann) distribution and the Q-values are its statistics. In the discrete case, they take a categorical (or multinouilli) distribution: <span class="math inline">\(\Phi_\theta(s)\)</span> is the probability for each action to be taken and the distribution selects one of them. Think of a dice with one side per action and probabilities governed by the policy. In the continuous case, it could be anything, for example a normal distribution.</p>
<p>Let’s rewrite the policy gradient with that formulation (we omit here the bias correction, but ACER uses it), but only w.r.t the output of the actor <span class="math inline">\(\Phi_\theta(s_t)\)</span> for a state <span class="math inline">\(s_t\)</span>:</p>
<p><span class="math display">\[
    \hat{g_t}^\text{ACER} = \nabla_{\Phi_\theta(s_t)} J(\theta)  = \bar{\rho}_t \, (Q^\text{ret}(s_t, a_t) - V_\phi(s_t) ) \, \nabla_{\Phi_\theta(s_t)} \log f(a_t | \Phi_\theta(s_t))
\]</span></p>
<p>To compute the policy gradient, we would only need to apply the chain rule:</p>
<p><span class="math display">\[
    \nabla_\theta J(\theta) = \mathbb{E}_{s_t, a_t \sim \rho_b} [ \hat{g_t}^\text{ACER} \, \nabla_\theta \Phi_\theta(s_t) ]
\]</span></p>
<p>The variance of <span class="math inline">\(\hat{g_t}^\text{ACER}\)</span> is too high. ACER defines the following TRPO problem: we search for a gradient <span class="math inline">\(z\)</span> solution of:</p>
<p><span class="math display">\[
    \min_z ||\hat{g_t}^\text{ACER} - z ||^2 \\
    \text{s.t.} \quad \nabla_{\Phi_\theta(s_t)} D_{KL}( f(\cdot | \Phi_{\theta&#39;}(s_t) ) || f(\cdot | \Phi_{\theta&#39;}(s_t)) )^T \times z &lt; \delta
\]</span></p>
<p>The exact meaning of the constraint is hard to grasp, but here some intuition: the change of policy <span class="math inline">\(z\)</span> (remember that <span class="math inline">\(\hat{g_t}^\text{ACER}\)</span> is defined w.r.t the output of the actor) should be as orthogonal as possible (within a margin <span class="math inline">\(\delta\)</span>) to the change of the <strong>Kullback-Leibler</strong> divergence between the policy defined by the actor (<span class="math inline">\(\theta\)</span>) and the one defined by the <strong>target actor</strong> (<span class="math inline">\(\theta&#39;\)</span>). In other words, we want to update the actor, but without making the new policy too different from its past values (the target actor).</p>
<p>The advantage of this formulation is that the objective function is quadratic in <span class="math inline">\(z\)</span> and the constraint is linear. We can therefore very easily find its solution using KKT optimization:</p>
<p><span class="math display">\[
    z^* = \hat{g_t}^\text{ACER} - \max(0, \frac{k^T \, \hat{g_t}^\text{ACER} - \delta}{||k||^2}) \, k
\]</span></p>
<p>where <span class="math inline">\(k = \nabla_{\Phi_\theta(s_t)} D_{KL}( f(\cdot | \Phi_{\theta&#39;}(s_t) ) || f(\cdot | \Phi_{\theta&#39;}(s_t)) )\)</span>.</p>
<p>Having obtained <span class="math inline">\(z^*\)</span>, we can safely update the parameters of the actor in the direction of:</p>
<p><span class="math display">\[
    \nabla_\theta J(\theta) = \mathbb{E}_{s_t, a_t \sim \rho_b} [ z^* \, \nabla_\theta \Phi_\theta(s_t) ]
\]</span></p>
<p>As noted in the paper: <em>“The trust region step is carried out in the space of the statistics of the distribution <span class="math inline">\(f\)</span> , and not in the space of the policy parameters. This is done deliberately so as to avoid an additional back-propagation step through the policy network”</em>. We indeed need only one network update per transition. If the KL divergence was computed with respect to <span class="math inline">\(\pi_\theta\)</span> directly, one would need to apply backpropagation on the target network too.</p>
<p>The target network <span class="math inline">\(\theta&#39;\)</span> is furthermore used as the behavior policy <span class="math inline">\(b(s, a)\)</span>. here is also a <strong>target critic network</strong> <span class="math inline">\(\varphi&#39;\)</span>, which is primarily used to compute the value of the states <span class="math inline">\(V_{\varphi&#39;}(s)\)</span> for variance reduction.</p>
<p>For a complete description of the algorithm, refer to the paper… To summarize, ACER is an actor-critic architecture using Retrace estimated values, importance weight truncation with bias correction and efficient TRPO. Its variant for continuous action spaces furthermore uses a <strong>Stochastic Dueling Network</strong> (SDN) in order estimate both <span class="math inline">\(Q_\varphi(s, a)\)</span> and <span class="math inline">\(V_\varphi(s)\)</span>. It is straightforward in the discrete case (multiply the policy with the Q-values and take the average) but hard in the continuous case.</p>
<p>ACER improved the performance and/or the sample efficiency of the state-of-the-art (A3C, DDPG, etc) on a variety of tasks (Atari, Mujoco). Apart from truncation with bias correction, all aspects of the algorithm are essential to obtain this improvement, as shown by ablation studies.</p>

<br>
<div class="arrows">
<a href="DPG.html" class="previous">&laquo; Previous</a>
<a href="EntropyRL.html" class="next">Next &raquo;</a>
</div>


</article>
</body>
</html>
