<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <meta name="author" content="Julien Vitay">
  <title>Deep Reinforcement Learning</title>


  <link rel="stylesheet" href="assets/syntax.css">
  <link rel="stylesheet" href="assets/github.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->

</head>

<body class="markdown-body">

<header>
  <h1 class="title"><strong>Deep Reinforcement Learning</strong></h1>
  <p class="author">Julien Vitay - <a href="mailto:julien.vitay@informatik.tu-chemnitz.de">julien.vitay@informatik.tu-chemnitz.de</a></p>
</header>


<article>

<nav id="TOC" class ="toc">
  <h2><strong>Deep Reinforcement Learning</strong></h2>
  <p class="author">Julien Vitay</p>

<ul>
<li><a href="#sec:introduction"><span class="toc-section-number">1</span> Introduction</a></li>
<li><a href="#sec:basics"><span class="toc-section-number">2</span> Basics</a><ul>
<li><a href="#sec:reinforcement-learning-and-markov-decision-process"><span class="toc-section-number">2.1</span> Reinforcement learning and Markov Decision Process</a><ul>
<li><a href="#sec:policy-and-value-functions"><span class="toc-section-number">2.1.1</span> Policy and value functions</a></li>
<li><a href="#sec:bellman-equations"><span class="toc-section-number">2.1.2</span> Bellman equations</a></li>
<li><a href="#sec:dynamic-programming"><span class="toc-section-number">2.1.3</span> Dynamic programming</a></li>
<li><a href="#sec:monte-carlo-sampling"><span class="toc-section-number">2.1.4</span> Monte-Carlo sampling</a></li>
<li><a href="#sec:temporal-difference"><span class="toc-section-number">2.1.5</span> Temporal Difference</a></li>
<li><a href="#sec:eligibility-traces"><span class="toc-section-number">2.1.6</span> Eligibility traces</a></li>
<li><a href="#sec:actor-critic-architectures"><span class="toc-section-number">2.1.7</span> Actor-critic architectures</a></li>
<li><a href="#sec:function-approximation"><span class="toc-section-number">2.1.8</span> Function approximation</a></li>
</ul></li>
<li><a href="#sec:deep-learning"><span class="toc-section-number">2.2</span> Deep learning</a><ul>
<li><a href="#sec:deep-neural-networks"><span class="toc-section-number">2.2.1</span> Deep neural networks</a></li>
<li><a href="#sec:convolutional-networks"><span class="toc-section-number">2.2.2</span> Convolutional networks</a></li>
<li><a href="#sec:recurrent-neural-networks"><span class="toc-section-number">2.2.3</span> Recurrent neural networks</a></li>
</ul></li>
</ul></li>
<li><a href="#sec:value-based-methods"><span class="toc-section-number">3</span> Value-based methods</a><ul>
<li><a href="#sec:limitations-of-deep-neural-networks-for-function-approximation"><span class="toc-section-number">3.1</span> Limitations of deep neural networks for function approximation</a></li>
<li><a href="#sec:deep-q-network-dqn"><span class="toc-section-number">3.2</span> Deep Q-Network (DQN)</a></li>
<li><a href="#sec:double-dqn"><span class="toc-section-number">3.3</span> Double DQN</a></li>
<li><a href="#sec:prioritized-experience-replay"><span class="toc-section-number">3.4</span> Prioritized experience replay</a></li>
<li><a href="#sec:duelling-network"><span class="toc-section-number">3.5</span> Duelling network</a></li>
<li><a href="#sec:distributed-dqn-gorila"><span class="toc-section-number">3.6</span> Distributed DQN (GORILA)</a></li>
<li><a href="#sec:deep-recurrent-q-learning-drqn"><span class="toc-section-number">3.7</span> Deep Recurrent Q-learning (DRQN)</a></li>
<li><a href="#sec:other-variants-of-dqn"><span class="toc-section-number">3.8</span> Other variants of DQN</a></li>
</ul></li>
<li><a href="#sec:policy-gradient-methods"><span class="toc-section-number">4</span> Policy Gradient methods</a><ul>
<li><a href="#sec:reinforce"><span class="toc-section-number">4.1</span> REINFORCE</a><ul>
<li><a href="#sec:estimating-the-policy-gradient"><span class="toc-section-number">4.1.1</span> Estimating the policy gradient</a></li>
<li><a href="#sec:reducing-the-variance"><span class="toc-section-number">4.1.2</span> Reducing the variance</a></li>
<li><a href="#sec:policy-gradient-theorem"><span class="toc-section-number">4.1.3</span> Policy Gradient theorem</a></li>
</ul></li>
<li><a href="#sec:advantage-actor-critic-methods"><span class="toc-section-number">4.2</span> Advantage Actor-Critic methods</a><ul>
<li><a href="#sec:advantage-actor-critic-a2c"><span class="toc-section-number">4.2.1</span> Advantage Actor-Critic (A2C)</a></li>
<li><a href="#sec:asynchronous-advantage-actor-critic-a3c"><span class="toc-section-number">4.2.2</span> Asynchronous Advantage Actor-Critic (A3C)</a></li>
<li><a href="#sec:generalized-advantage-estimation-gae"><span class="toc-section-number">4.2.3</span> Generalized Advantage Estimation (GAE)</a></li>
<li><a href="#sec:stochastic-actor-critic-for-continuous-action-spaces"><span class="toc-section-number">4.2.4</span> Stochastic actor-critic for continuous action spaces</a></li>
</ul></li>
<li><a href="#sec:off-policy-actor-critic"><span class="toc-section-number">4.3</span> Off-policy Actor-Critic</a><ul>
<li><a href="#sec:importance-sampling"><span class="toc-section-number">4.3.1</span> Importance sampling</a></li>
<li><a href="#sec:linear-off-policy-actor-critic-off-pac"><span class="toc-section-number">4.3.2</span> Linear Off-Policy Actor-Critic (Off-PAC)</a></li>
<li><a href="#sec:retrace"><span class="toc-section-number">4.3.3</span> Retrace</a></li>
<li><a href="#sec:self-imitation-learning-sil"><span class="toc-section-number">4.3.4</span> Self-Imitation Learning (SIL)</a></li>
</ul></li>
<li><a href="#sec:deterministic-policy-gradient-dpg"><span class="toc-section-number">4.4</span> Deterministic Policy Gradient (DPG)</a><ul>
<li><a href="#sec:deterministic-policy-gradient-theorem"><span class="toc-section-number">4.4.1</span> Deterministic policy gradient theorem</a></li>
<li><a href="#sec:deep-deterministic-policy-gradient-ddpg"><span class="toc-section-number">4.4.2</span> Deep Deterministic Policy Gradient (DDPG)</a></li>
<li><a href="#sec:distributed-distributional-ddpg-d4pg"><span class="toc-section-number">4.4.3</span> Distributed Distributional DDPG (D4PG)</a></li>
</ul></li>
<li><a href="#sec:natural-gradients"><span class="toc-section-number">4.5</span> Natural Gradients</a><ul>
<li><a href="#sec:principle-of-natural-gradients"><span class="toc-section-number">4.5.1</span> Principle of natural gradients</a></li>
<li><a href="#sec:natural-policy-gradient-and-natural-actor-critic-nac"><span class="toc-section-number">4.5.2</span> Natural policy gradient and Natural Actor Critic (NAC)</a></li>
<li><a href="#sec:trust-region-policy-optimization-trpo"><span class="toc-section-number">4.5.3</span> Trust Region Policy Optimization (TRPO)</a></li>
<li><a href="#sec:proximal-policy-optimization-ppo"><span class="toc-section-number">4.5.4</span> Proximal Policy Optimization (PPO)</a></li>
<li><a href="#sec:actor-critic-with-experience-replay-acer"><span class="toc-section-number">4.5.5</span> Actor-Critic with Experience Replay (ACER)</a></li>
</ul></li>
<li><a href="#sec:maximum-entropy-rl"><span class="toc-section-number">4.6</span> Maximum Entropy RL</a><ul>
<li><a href="#sec:entropy-regularization-1"><span class="toc-section-number">4.6.1</span> Entropy regularization</a></li>
<li><a href="#sec:soft-actor-critic-sac"><span class="toc-section-number">4.6.2</span> Soft Actor-Critic (SAC)</a></li>
</ul></li>
<li><a href="#sec:distributional-learning"><span class="toc-section-number">4.7</span> Distributional learning</a><ul>
<li><a href="#sec:the-reactor"><span class="toc-section-number">4.7.1</span> The Reactor</a></li>
</ul></li>
<li><a href="#sec:other-policy-search-methods"><span class="toc-section-number">4.8</span> Other policy search methods</a><ul>
<li><a href="#sec:stochastic-value-gradient-svg"><span class="toc-section-number">4.8.1</span> Stochastic Value Gradient (SVG)</a></li>
<li><a href="#sec:q-prop"><span class="toc-section-number">4.8.2</span> Q-Prop</a></li>
<li><a href="#sec:normalized-advantage-function-naf"><span class="toc-section-number">4.8.3</span> Normalized Advantage Function (NAF)</a></li>
<li><a href="#sec:fictitious-self-play-fsp"><span class="toc-section-number">4.8.4</span> Fictitious Self-Play (FSP)</a></li>
</ul></li>
<li><a href="#sec:comparison-between-value-based-and-policy-gradient-methods"><span class="toc-section-number">4.9</span> Comparison between value-based and policy gradient methods</a></li>
<li><a href="#sec:gradient-free-policy-search"><span class="toc-section-number">4.10</span> Gradient-free policy search</a><ul>
<li><a href="#sec:cross-entropy-method-cem"><span class="toc-section-number">4.10.1</span> Cross-entropy Method (CEM)</a></li>
<li><a href="#sec:evolutionary-search-es"><span class="toc-section-number">4.10.2</span> Evolutionary Search (ES)</a></li>
</ul></li>
</ul></li>
<li><a href="#sec:deep-rl-in-practice"><span class="toc-section-number">5</span> Deep RL in practice</a><ul>
<li><a href="#sec:limitations"><span class="toc-section-number">5.1</span> Limitations</a></li>
<li><a href="#sec:reward-shaping"><span class="toc-section-number">5.2</span> Reward shaping</a></li>
<li><a href="#sec:simulation-environments"><span class="toc-section-number">5.3</span> Simulation environments</a></li>
<li><a href="#sec:algorithm-implementations"><span class="toc-section-number">5.4</span> Algorithm implementations</a></li>
</ul></li>
<li><a href="#sec:references">References</a></li>
</ul>


</nav>




<h1 id="sec:introduction"><span class="header-section-number">1</span> Introduction</h1>
<p>The goal of this document is to keep track the state-of-the-art in deep reinforcement learning. It starts with basics in reinforcement learning and deep learning to introduce the notations and covers different classes of deep RL methods, value-based or policy-based, model-free or model-based, etc.</p>
<p>Different classes of deep RL methods can be identified. This document will focus on the following ones:</p>
<ol type="1">
<li>Value-based algorithms (DQN…) used mostly for discrete problems like video games.</li>
<li>Policy-gradient algorithms (A3C, DDPG…) used for continuous control problems such as robotics.</li>
<li>Recurrent attention models (RAM…) for partially observable problems.</li>
<li>Model-based RL to reduce the sample complexity by incorporating a model of the environment.</li>
<li>Application of deep RL to robotics</li>
</ol>
<p>One could extend the list and talk about hierarchical RL, inverse RL, imitation-based RL, etc…</p>
<p><strong>Additional resources</strong></p>
<p>See <span class="citation" data-cites="Li2017">Li (<a href="#ref-Li2017" role="doc-biblioref">2017</a>)</span>, <span class="citation" data-cites="Arulkumaran2017">Arulkumaran et al. (<a href="#ref-Arulkumaran2017" role="doc-biblioref">2017</a>)</span> and <span class="citation" data-cites="Mousavi2018">Mousavi, Schukat, and Howley (<a href="#ref-Mousavi2018" role="doc-biblioref">2018</a>)</span> for recent overviews of deep RL.</p>
<p>The CS294 course of Sergey Levine at Berkeley is incredibly complete: <a href="http://rll.berkeley.edu/deeprlcourse/" class="uri">http://rll.berkeley.edu/deeprlcourse/</a>. The Reinforcement Learning course by David Silver at UCL covers also the whole field: <a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html" class="uri">http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html</a>.</p>
<p>This series of posts from Arthur Juliani <a href="https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0" class="uri">https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0</a> also provide a very good introduction to deep RL, associated to code samples using tensorflow.</p>
<p><strong>Notes</strong></p>
<p>This document is meant to stay <em>work in progress</em> forever, as new algorithms will be added as they are published. Feel free to comment, correct, suggest, pull request by writing to <a href="mailto:julien.vitay@informatik.tu-chemnitz.de" class="email">julien.vitay@informatik.tu-chemnitz.de</a>.</p>
<p>For some reason, this document is better printed using chrome. Use the single file version <a href="./DeepRL.html">here</a> and print it to pdf. Alternatively, a pdf version generated using LaTeX is available <a href="./DeepRL.pdf">here</a> (some images may disappear, as LaTeX does not support .gif or .svg images).</p>
<p>The style is adapted from the Github-Markdown CSS template <a href="https://www.npmjs.com/package/github-markdown-css" class="uri">https://www.npmjs.com/package/github-markdown-css</a>. The document is written in Pandoc’s Markdown and converted to html and pdf using pandoc-citeproc and pandoc-crossref.</p>
<p>Some figures are taken from the original publication (“Taken from” or “Source” in the caption). Their copyright stays to the respective authors, naturally. The rest is my own work and can be distributed, reproduced and modified under CC-BY-SA-NC 4.0.</p>
<p><strong>Thanks</strong></p>
<p>Thanks to all the students who helped me dive into that exciting research field, in particular: Winfried Lötzsch, Johannes Jung, Frank Witscher, Danny Hofmann, Oliver Lange, Vinayakumar Murganoor.</p>
<p><strong>Copyright</strong></p>
<p>Except where otherwise noted, this work is licensed under a <a href="http://creativecommons.org/licenses/by-nc-sa/4.0">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.</p>
<!--PAGEBREAK-->
<h1 id="sec:basics"><span class="header-section-number">2</span> Basics</h1>
<p>Deep reinforcement learning (deep RL) is the integration of deep learning methods, classically used in supervised or unsupervised learning contexts, with reinforcement learning (RL), a well-studied adaptive control method used in problems with delayed and partial feedback <span class="citation" data-cites="Sutton1998">(Sutton and Barto <a href="#ref-Sutton1998" role="doc-biblioref">1998</a>)</span>. This section starts with the basics of RL, mostly to set the notations, and provides a quick overview of deep neural networks.</p>
<h2 id="sec:reinforcement-learning-and-markov-decision-process"><span class="header-section-number">2.1</span> Reinforcement learning and Markov Decision Process</h2>
<p>RL methods apply to problems where an agent interacts with an environment in discrete time steps (Fig. <a href="#fig:agentenv">1</a>). At time <span class="math inline">\(t\)</span>, the agent is in state <span class="math inline">\(s_t\)</span> and decides to perform an action <span class="math inline">\(a_t\)</span>. At the next time step, it arrives in the state <span class="math inline">\(s_{t+1}\)</span> and obtains the reward <span class="math inline">\(r_{t+1}\)</span>. The goal of the agent is to maximize the reward obtained on the long term. The textbook by <span class="citation" data-cites="Sutton1998">Sutton and Barto (<a href="#ref-Sutton1998" role="doc-biblioref">1998</a>)</span> (updated in <span class="citation" data-cites="Sutton2017">Sutton and Barto (<a href="#ref-Sutton2017" role="doc-biblioref">2017</a>)</span>) defines the field extensively.</p>
<figure>
<img src="img/rl-agent.jpg" alt="Figure 1: Interaction between an agent and its environment. Taken from Sutton and Barto (1998)." id="fig:agentenv" style="width:50.0%" /><figcaption>Figure 1: Interaction between an agent and its environment. Taken from <span class="citation" data-cites="Sutton1998">Sutton and Barto (<a href="#ref-Sutton1998" role="doc-biblioref">1998</a>)</span>.</figcaption>
</figure>
<p>Reinforcement learning problems are described as <strong>Markov Decision Processes</strong> (MDP) defined by five quantities:</p>
<ul>
<li>a state space <span class="math inline">\(\mathcal{S}\)</span> where each state <span class="math inline">\(s\)</span> respects the Markovian property. It can be finite or infinite.</li>
<li>an action space <span class="math inline">\(\mathcal{A}\)</span> of actions <span class="math inline">\(a\)</span>, which can be finite or infinite, discrete or continuous.</li>
<li>an initial state distribution <span class="math inline">\(p_0(s_0)\)</span> (from which states is the agent likely to start).</li>
<li>a transition dynamics model with density <span class="math inline">\(p(s&#39;|s, a)\)</span>, sometimes noted <span class="math inline">\(\mathcal{P}_{ss&#39;}^a\)</span>. It defines the probability of arriving in the state <span class="math inline">\(s&#39;\)</span> at time <span class="math inline">\(t+1\)</span> when being in the state <span class="math inline">\(s\)</span> and performing the action <span class="math inline">\(a\)</span>.</li>
<li>a reward function <span class="math inline">\(r(s, a, s&#39;) : \mathcal{S}\times\mathcal{A}\times\mathcal{S} \rightarrow \Re\)</span> defining the (stochastic) reward obtained after performing <span class="math inline">\(a\)</span> in state <span class="math inline">\(s\)</span> and arriving in <span class="math inline">\(s&#39;\)</span>.</li>
</ul>
<p>The behavior of the agent over time is a <strong>trajectory</strong> (also called episode, history or roll-out) <span class="math inline">\(\tau = (s_0, a_0, s_1, a_, \ldots, s_T, a_T)\)</span> defined by the dynamics of the MDP. Each transition occurs with a probability <span class="math inline">\(p(s&#39;|s, a)\)</span> and provides a certain amount of reward defined by <span class="math inline">\(r(s, a, s&#39;)\)</span>. In episodic tasks, the horizon <span class="math inline">\(T\)</span> is finite, while in continuing tasks <span class="math inline">\(T\)</span> is infinite.</p>
<p>Importantly, the <strong>Markovian property</strong> states that:</p>
<p><span class="math display">\[
    p(s_{t+1}|s_t, a_t) = p(s_{t+1}|s_t, a_t, s_{t-1}, a_{t-1}, \dots s_0, a_0)
\]</span></p>
<p>i.e. you do not need the full history of the agent to predict where it will arrive after an action. In simple problems, this is just a question of providing enough information to the description of a state: if a transition depends on what happened in the past, just put that information in the state description.</p>
<p>If the Markov property is not met, RL methods may not converge (or poorly). In many problems, one does not have access to the true states of the agent, but one can only indirectly observe them. For example, in a video game, the true state is defined by a couple of variables: coordinates <span class="math inline">\((x, y)\)</span> of the two players, position of the ball, speed, etc. However, all you have access to are the raw pixels: sometimes the ball may be hidden behind a wall or a tree, but it still exists in the state space. Speed information is also not observable in a single frame.</p>
<p>In a <strong>Partially Observable Markov Decision Process</strong> (POMDP), observations <span class="math inline">\(o_t\)</span> come from a space <span class="math inline">\(\mathcal{O}\)</span> and are linked to underlying states using the density function <span class="math inline">\(p(o_t| s_t)\)</span>. Observations are usually not Markovian, so the full history of observations <span class="math inline">\(h_t = (o_0, a_0, \dots o_t, a_t)\)</span> is needed to solve the problem.</p>
<h3 id="sec:policy-and-value-functions"><span class="header-section-number">2.1.1</span> Policy and value functions</h3>
<p>The policy defines the behavior of the agent: which action should be taken in each state. One distinguishes two kinds of policies:</p>
<ul>
<li>a stochastic policy <span class="math inline">\(\pi : \mathcal{S} \rightarrow P(\mathcal{A})\)</span> defines the probability distribution <span class="math inline">\(P(\mathcal{A})\)</span> of performing an action.</li>
<li>a deterministic policy <span class="math inline">\(\mu(s_t)\)</span> is a discrete mapping of <span class="math inline">\(\mathcal{S} \rightarrow \mathcal{A}\)</span>.</li>
</ul>
<p>The policy can be used to explore the environment and generate trajectories of states, rewards and actions. The performance of a policy is determined by calculating the <strong>expected discounted return</strong>, i.e. the sum of all rewards received from time step <span class="math inline">\(t\)</span> onwards:</p>
<p><span class="math display">\[
    R_t = \sum_{k=0}^{T} \gamma^k \, r_{t+k+1}
\]</span></p>
<p>where <span class="math inline">\(0 &lt; \gamma \leq 1\)</span> is the discount rate and <span class="math inline">\(r_{t+1}\)</span> represents the reward obtained during the transition from <span class="math inline">\(s_t\)</span> to <span class="math inline">\(s_{t+1}\)</span>.</p>
<p>The <strong>discount rate</strong> <span class="math inline">\(\gamma\)</span> is a critical hyperparameter of RL: chosen too small, only immediate rewards will matter (i.e. participate to <span class="math inline">\(R_t\)</span>) and the agent will be greedy. Chosen too close from 1, hypothetical rewards delivered in one year from now will count as much as slightly smaller rewards delivered for certain now.</p>
<p>If the task is episodic (<span class="math inline">\(T\)</span> is finite, the trajectories ends after a finite number of transitions), <span class="math inline">\(\gamma\)</span> can be set to 1, but if the task is continuing (<span class="math inline">\(T=\infty\)</span>, trajectories have no end), <span class="math inline">\(\gamma\)</span> must be chosen smaller than 1.</p>
<p>The Q-value of a state-action pair <span class="math inline">\((s, a)\)</span> is defined as the expected discounted reward received if the agent takes <span class="math inline">\(a\)</span> from a state <span class="math inline">\(s\)</span> and follows the policy distribution <span class="math inline">\(\pi\)</span> thereafter:</p>
<p><span class="math display">\[
    Q^{\pi}(s, a) = \mathbb{E}_{\pi}[R_t | s_t = s, a_t=a]
\]</span></p>
<p>More precisely, the Q-value of a state-action pair is the mathematical expectation of the expected return over all trajectories starting in <span class="math inline">\((s, a)\)</span> defined by the policy <span class="math inline">\(\pi\)</span>.</p>
<p>Similarly, the value of a state <span class="math inline">\(s\)</span> is the expected discounted reward received if the agent starts in <span class="math inline">\(s\)</span> and thereafter follows its policy <span class="math inline">\(\pi\)</span>.</p>
<p><span class="math display">\[
    V^{\pi}(s) = \mathbb{E}_{\pi}[R_t | s_t = s]
\]</span></p>
<p>Obviously, these quantities depend on the states/actions themselves (some chessboard configurations are intrinsically better than others, i.e. you are more likely to win from that state), but also on the policy (if you can kill your opponent in one move - meaning you are in an intrinsically good state - but systematically take the wrong decision and lose, this is actually a bad state).</p>
<h3 id="sec:bellman-equations"><span class="header-section-number">2.1.2</span> Bellman equations</h3>
<p>The V- and Q-values are obviously linked with each other. The value of state depend on the value of the actions possible in that state, modulated by the probability that an action will be taken (i.e. the policy):</p>
<p><span id="eq:v-value"><span class="math display">\[
    V^{\pi}(s) = \sum_{a \in \mathcal{A}} \pi(s, a) \, Q^\pi(s,a)
\qquad(1)\]</span></span></p>
<p>For a deterministic policy (<span class="math inline">\(\pi(s, a) = 1\)</span> if <span class="math inline">\(a=a^*\)</span> and <span class="math inline">\(0\)</span> otherwise), the value of a state is the same as the value of the action that will be systematically taken.</p>
<p>Noting that:</p>
<p><span id="eq:return"><span class="math display">\[
    R_t = r_{t+1} + \gamma R_{t+1}
\qquad(2)\]</span></span></p>
<p>i.e. that the expected return at time <span class="math inline">\(t\)</span> is the sum of the immediate reward received during the next transition <span class="math inline">\(r_{t+1}\)</span> and of the expected return at the next state (<span class="math inline">\(R_{t+1}\)</span>, discounted by <span class="math inline">\(\gamma\)</span>), we can also write:</p>
<p><span id="eq:q-value"><span class="math display">\[
    Q^{\pi}(s, a) = \sum_{s&#39; \in \mathcal{S}} p(s&#39;|s, a) [r(s, a, s&#39;) + \gamma \, V^\pi(s&#39;)]
\qquad(3)\]</span></span></p>
<p>The value of an action depends on which state you arrive in (<span class="math inline">\(s&#39;\)</span>), with which probability (<span class="math inline">\(p(s&#39;|s, a)\)</span>) this transition occurs, how much reward you receive immediately (<span class="math inline">\(r(s, a, s&#39;)\)</span>) and how much you will receive later (summarized by <span class="math inline">\(V^\pi(s&#39;)\)</span>).</p>
<p>Putting together Eq. <a href="#eq:v-value">1</a> and Eq. <a href="#eq:q-value">3</a>, we obtain the <strong>Bellman equations</strong>:</p>
<p><span class="math display">\[
    V^{\pi}(s) = \sum_{a \in \mathcal{A}} \pi(s, a) \, \sum_{s&#39; \in \mathcal{S}} p(s&#39;|s, a) [r(s, a, s&#39;) + \gamma \, V^\pi(s&#39;)]
\]</span></p>
<p><span class="math display">\[
    Q^{\pi}(s, a) = \sum_{s&#39; \in \mathcal{S}} p(s&#39;|s, a) [r(s, a, s&#39;) + \gamma \, \sum_{a&#39; \in \mathcal{A}} \pi(s&#39;, a&#39;) \, Q^\pi(s&#39;,a&#39;)]
\]</span></p>
<p>The Bellman equations mean that the value of a state (resp. state-action pair) depends on the value of all other states (resp. state-action pairs), the current policy <span class="math inline">\(\pi\)</span> and the dynamics of the MDP (<span class="math inline">\(p(s&#39;|s, a)\)</span> and <span class="math inline">\(r(s, a, s&#39;)\)</span>).</p>
<figure>
<img src="img/backup.png" alt="Figure 2: Backup diagrams corresponding to the Bellman equations. Taken from Sutton and Barto (1998)." id="fig:backup" style="width:50.0%" /><figcaption>Figure 2: Backup diagrams corresponding to the Bellman equations. Taken from <span class="citation" data-cites="Sutton1998">Sutton and Barto (<a href="#ref-Sutton1998" role="doc-biblioref">1998</a>)</span>.</figcaption>
</figure>
<h3 id="sec:dynamic-programming"><span class="header-section-number">2.1.3</span> Dynamic programming</h3>
<p>The interesting property of the Bellman equations is that, if the states have the Markovian property, they admit <em>one and only one</em> solution. This means that for a given policy, if the dynamics of the MDP are known, it is possible to compute the value of all states or state-action pairs by solving the Bellman equations for all states or state-action pairs (<em>policy evaluation</em>).</p>
<p>Once the values are known for a given policy, it is possible to improve the policy by selecting with the highest probability the action with the highest Q-value. For example, if the current policy chooses the action <span class="math inline">\(a_1\)</span> over <span class="math inline">\(a_2\)</span> in <span class="math inline">\(s\)</span> (<span class="math inline">\(\pi(s, a_1) &gt; \pi(s, a_2)\)</span>), but after evaluating the policy it turns out that <span class="math inline">\(Q^\pi(s, a_2) &gt; Q^\pi(s, a_1)\)</span> (the expected return after <span class="math inline">\(a_2\)</span> is higher than after <span class="math inline">\(a_1\)</span>), it makes more sense to preferentially select <span class="math inline">\(a_2\)</span>, as there is more reward afterwards. We can then create a new policy <span class="math inline">\(\pi&#39;\)</span> where <span class="math inline">\(\pi&#39;(s, a_2) &gt; \pi&#39;(s, a_1)\)</span>, which is is <em>better</em> policy than <span class="math inline">\(\pi\)</span> as more reward can be gathered after <span class="math inline">\(s\)</span>.</p>
<figure>
<img src="img/dynamicprogramming.png" alt="Figure 3: Dynamic programming alternates between policy evaluation and policy improvement. Taken from Sutton and Barto (1998)." id="fig:dynamicprogramming" style="width:20.0%" /><figcaption>Figure 3: Dynamic programming alternates between policy evaluation and policy improvement. Taken from <span class="citation" data-cites="Sutton1998">Sutton and Barto (<a href="#ref-Sutton1998" role="doc-biblioref">1998</a>)</span>.</figcaption>
</figure>
<p><strong>Dynamic programming</strong> (DP) alternates between policy evaluation and policy improvement. If the problem is Markovian, it can be shown that DP converges to the <em>optimal policy</em> <span class="math inline">\(\pi^*\)</span>, i.e. the policy where the expected return is maximal in all states.</p>
<p>Note that by definition the optimal policy is <em>deterministic</em> and <em>greedy</em>: if there is an action with a maximal Q-value for the optimal policy, it should be systematically taken. For the optimal policy <span class="math inline">\(\pi^*\)</span>, the Bellman equations become:</p>
<p><span class="math display">\[
    V^{*}(s) = \max_{a \in \mathcal{A}} \sum_{s \in \mathcal{S}} p(s&#39; | s, a) \cdot [ r(s, a, s&#39;) + \gamma \cdot V^{*} (s&#39;) ]
\]</span></p>
<p><span class="math display">\[
    Q^{*}(s, a) = \sum_{s&#39; \in \mathcal{S}} p(s&#39; | s, a) \cdot [r(s, a, s&#39;) + \gamma \max_{a&#39; \in \mathcal{A}} Q^* (s&#39;, a&#39;) ]
\]</span></p>
<p>Dynamic programming can only be used when:</p>
<ul>
<li>the dynamics of the MDP (<span class="math inline">\(p(s&#39;|s, a)\)</span> and <span class="math inline">\(r(s, a, s&#39;)\)</span>) are fully known.</li>
<li>the number of states and state-action pairs is small (one Bellman equation per state or state/action to solve).</li>
</ul>
<p>In practice, sample-based methods such as Monte-Carlo or temporal difference are used.</p>
<h3 id="sec:monte-carlo-sampling"><span class="header-section-number">2.1.4</span> Monte-Carlo sampling</h3>
<figure>
<img src="img/unifiedreturn.png" alt="Figure 4: Monte-Carlo methods accumulate rewards over a complete episode. Taken from Sutton and Barto (1998)." id="fig:mc" style="width:50.0%" /><figcaption>Figure 4: Monte-Carlo methods accumulate rewards over a complete episode. Taken from <span class="citation" data-cites="Sutton1998">Sutton and Barto (<a href="#ref-Sutton1998" role="doc-biblioref">1998</a>)</span>.</figcaption>
</figure>
<p>When the environment is <em>a priori</em> unknown, it has to be explored in order to build estimates of the V or Q value functions. The key idea of <strong>Monte-Carlo</strong> sampling (MC) is rather simple:</p>
<ol type="1">
<li>Start from a state <span class="math inline">\(s_0\)</span>.</li>
<li>Perform an episode (sequence of state-action transitions) until a terminal state <span class="math inline">\(s_T\)</span> is reached using your current policy <span class="math inline">\(\pi\)</span>.</li>
<li>Accumulate the rewards into the actual return for that episode <span class="math inline">\(R_t^{(e)} = \sum_{k=0}^T r_{t+k+1}\)</span> for each time step.</li>
<li>Repeat often enough so that the value of a state <span class="math inline">\(s\)</span> can be approximated by the average of many actual returns:</li>
</ol>
<p><span class="math display">\[V^\pi(s) = \mathbb{E}_\pi[R_t | s_t = s] \approx \frac{1}{M} \sum_{e=1}^M R_t^{(e)}\]</span></p>
<p>Monte-carlo sampling is a classical method to estimate quantities defined by a mathematical expectation: the <em>true</em> value of <span class="math inline">\(V^\pi(s)\)</span> is defined over <strong>all</strong> trajectories starting in <span class="math inline">\(s\)</span>, what is impossible to compute in most problems. In MC methods, the true value is approximated by the average of a sufficient number of sampled trajectories, the million dollar question being: what means <em>sufficient</em>?</p>
<p>In practice, the estimated values are updated using continuous updates:</p>
<p><span class="math display">\[
    V^\pi(s) \leftarrow V^\pi(s) + \alpha (R_t - V^\pi(s))
\]</span></p>
<p>Q-values can also be approximated using the same procedure:</p>
<p><span class="math display">\[
    Q^\pi(s, a) \leftarrow Q^\pi(s, a) + \alpha (R_t - Q^\pi(s, a))
\]</span></p>
<p>The two main drawbacks of MC methods are:</p>
<ol type="1">
<li>The task must be episodic, i.e. stop after a finite amount of transitions. Updates are only applied at the end of an episode.</li>
<li>A sufficient level of exploration has to be ensured to make sure the estimates converge to the optimal values.</li>
</ol>
<p>The second issue is linked to the <strong>exploration-exploitation</strong> dilemma: the episode is generated using the current policy (or a policy derived from it, see later). If the policy always select the same actions from the beginning (exploitation), the agent will never discover better alternatives: the values will converge to a local minimum. If the policy always pick randomly actions (exploration), the policy which is evaluated is not the current policy <span class="math inline">\(\pi\)</span>, but the random policy. A trade-off between the two therefore has to be maintained: usually a lot of exploration at the beginning of learning to accumulate knowledge about the environment, less towards the end to actually use the knowledge and perform optimally.</p>
<p>There are two types of methods trying to cope with exploration:</p>
<ul>
<li><strong>On-policy</strong> methods generate the episodes using the learned policy <span class="math inline">\(\pi\)</span>, but it has to be <em><span class="math inline">\(\epsilon\)</span>-soft</em>, i.e. stochastic: it has to let a probability of at least <span class="math inline">\(\epsilon\)</span> of selecting another action than the greedy action (the one with the highest estimated Q-value).</li>
<li><strong>Off-policy</strong> methods use a second policy called the <em>behavior policy</em> to generate the episodes, but learn a different policy for exploitation, which can even be deterministic.</li>
</ul>
<p><span class="math inline">\(\epsilon\)</span>-soft policies are easy to create. The simplest one is the <strong><span class="math inline">\(\epsilon\)</span>-greedy</strong> action selection method, which assigns a probability <span class="math inline">\((1-\epsilon)\)</span> of selecting the greedy action (the one with the highest Q-value), and a probability <span class="math inline">\(\epsilon\)</span> of selecting any of the other available actions:</p>
<p><span class="math display">\[
    a_t = \begin{cases} a_t^* \quad \text{with probability} \quad (1 - \epsilon) \\
                       \text{any other action with probability } \epsilon \end{cases}
\]</span></p>
<p>Another solution is the <strong>Softmax</strong> (or Gibbs distribution) action selection method, which assigns to each action a probability of being selected depending on their relative Q-values:</p>
<p><span class="math display">\[
    P(s, a) = \frac{\exp Q^\pi(s, a) / \tau}{ \sum_b \exp Q^\pi(s, b) / \tau}
\]</span></p>
<p><span class="math inline">\(\tau\)</span> is a positive parameter called the temperature: high temperatures cause the actions to be nearly equiprobable, while low temperatures cause <span class="math inline">\(\tau\)</span> is a positive parameter called the temperature.</p>
<p>The advantage of off-policy methods is that domain knowledge can be used to restrict the search in the state-action space. For example, only moves actually played by chess experts in a given state will be actually explored, not random stupid moves. The obvious drawback being that if the optimal solution is not explored by the behavior policy, the agent has no way to discover it by itself.</p>
<h3 id="sec:temporal-difference"><span class="header-section-number">2.1.5</span> Temporal Difference</h3>
<p>The main drawback of Monte-Carlo methods is that the task must be composed of finite episodes. Not only is it not always possible, but value updates have to wait for the end of the episode, what slows learning down. <strong>Temporal difference</strong> methods simply replace the actual return obtained after a state or an action, by an estimation composed of the reward immediately received plus the value of the next state or action, as in Eq. <a href="#eq:return">2</a>:</p>
<p><span class="math display">\[
    R_t \approx r(s, a, s&#39;) + \gamma \, V^\pi(s&#39;) \approx r + \gamma \, Q^\pi(s&#39;, a&#39;)
\]</span></p>
<p>This gives us the following learning rules:</p>
<p><span class="math display">\[
    V^\pi(s) \leftarrow V^\pi(s) + \alpha (r(s, a, s&#39;) + \gamma \, V^\pi(s&#39;) - V^\pi(s))
\]</span></p>
<p><span class="math display">\[
    Q^\pi(s, a) \leftarrow Q^\pi(s, a) + \alpha (r(s, a, s&#39;) + \gamma \, Q^\pi(s&#39;, a&#39;) - Q^\pi(s, a))
\]</span></p>
<p>The quantity:</p>
<p><span class="math display">\[
 \delta = r(s, a, s&#39;) + \gamma \, V^\pi(s&#39;) - V^\pi(s)
\]</span></p>
<p>or:</p>
<p><span class="math display">\[
    \delta = r(s, a, s&#39;) + \gamma \, Q^\pi(s&#39;, a&#39;) - Q^\pi(s, a)
\]</span></p>
<p>is called the <strong>reward-prediction error</strong> (RPE) or <strong>TD error</strong>: it defines the surprise between the current reward prediction (<span class="math inline">\(V^\pi(s)\)</span> or <span class="math inline">\(Q^\pi(s, a)\)</span>) and the sum of the immediate reward plus the reward prediction in the next state / after the next action.</p>
<ul>
<li>If <span class="math inline">\(\delta &gt; 0\)</span>, the transition was positively surprising: one obtains more reward or lands in a better state than expected. The initial state or action was actually underrated, so its estimated value must be increased.</li>
<li>If <span class="math inline">\(\delta &lt; 0\)</span>, the transition was negatively surprising. The initial state or action was overrated, its value must be decreased.</li>
<li>If <span class="math inline">\(\delta = 0\)</span>, the transition was fully predicted: one obtains as much reward as expected, so the values should stay as they are.</li>
</ul>
<p>The main advantage of this learning method is that the update of the V- or Q-value can be applied immediately after a transition: no need to wait until the end of an episode, or even to have episodes at all: this is called <strong>online learning</strong> and allows very fast learning from single transitions. The main drawback is that the updates depend on other estimates, which are initially wrong: it will take a while before all estimates are correct.</p>
<figure>
<img src="img/backup-TD.png" alt="Figure 5: Temporal difference algorithms update values after a single transition. Taken from Sutton and Barto (1998)." id="fig:td" style="width:3.0%" /><figcaption>Figure 5: Temporal difference algorithms update values after a single transition. Taken from <span class="citation" data-cites="Sutton1998">Sutton and Barto (<a href="#ref-Sutton1998" role="doc-biblioref">1998</a>)</span>.</figcaption>
</figure>
<p>When learning Q-values directly, the question is which next action <span class="math inline">\(a&#39;\)</span> should be used in the update rule: the action that will actually be taken for the next transition (defined by <span class="math inline">\(\pi(s&#39;, a&#39;)\)</span>), or the greedy action (<span class="math inline">\(a^* = \text{argmax}_a Q^\pi(s&#39;, a)\)</span>). This relates to the <em>on-policy / off-policy</em> distinction already seen for MC methods:</p>
<ul>
<li><strong>On-policy</strong> TD learning is called <strong>SARSA</strong> (state-action-reward-state-action). It uses the next action sampled from the policy <span class="math inline">\(\pi(s&#39;, a&#39;)\)</span> to update the current transition. This selected action could be noted <span class="math inline">\(\pi(s&#39;)\)</span> for simplicity. It is required that this next action will actually be performed for the next transition. The policy must be <span class="math inline">\(\epsilon\)</span>-soft, for example <span class="math inline">\(\epsilon\)</span>-greedy or softmax:</li>
</ul>
<p><span class="math display">\[
    \delta = r(s, a, s&#39;) + \gamma \, Q^\pi(s&#39;, \pi(s&#39;)) - Q^\pi(s, a)
\]</span></p>
<ul>
<li><strong>Off-policy</strong> TD learning is called <strong>Q-learning</strong> <span class="citation" data-cites="Watkins1989">(Watkins <a href="#ref-Watkins1989" role="doc-biblioref">1989</a>)</span>. The greedy action in the next state (the one with the highest Q-value) is used to update the current transition. It does not mean that the greedy action will actually have to be selected for the next transition. The learned policy can therefore also be deterministic:</li>
</ul>
<p><span class="math display">\[
    \delta = r(s, a, s&#39;) + \gamma \, \max_{a&#39;} Q^\pi(s&#39;, a&#39;) - Q^\pi(s, a)
\]</span></p>
<p>In Q-learning, the behavior policy has to ensure exploration, while this is achieved implicitly by the learned policy in SARSA, as it must be <span class="math inline">\(\epsilon\)</span>-soft. An easy way of building a behavior policy based on a deterministic learned policy is <span class="math inline">\(\epsilon\)</span>-greedy: the deterministic action <span class="math inline">\(\mu(s_t)\)</span> is chosen with probability 1 - <span class="math inline">\(\epsilon\)</span>, the other actions with probability <span class="math inline">\(\epsilon\)</span>. In continuous action spaces, additive noise (e.g. Ohrstein-Uhlenbeck, see Section <a href="#sec:deep-deterministic-policy-gradient-ddpg">4.4.2</a>) can be added to the action.</p>
<p>Alternatively, domain knowledge can be used to create the behavior policy and restrict the search to meaningful actions: compilation of expert moves in games, approximate solutions, etc. Again, the risk is that the behavior policy never explores the actually optimal actions. See Section <a href="#sec:off-policy-actor-critic">4.3</a> for more details on the difference between on-policy and off-policy methods.</p>
<h3 id="sec:eligibility-traces"><span class="header-section-number">2.1.6</span> Eligibility traces</h3>
<p>The main drawback of TD learning is that learning can be slow and necessitate many transitions to converge (sample complexity). This is particularly true when the problem provides <strong>sparse rewards</strong> (as opposed to dense rewards). For example in a game like chess, a reward is given only at the end of a game (+1 for winning, -1 for losing). All other actions receive a reward of 0, although they are as important as the last one in order to win.</p>
<p>Imagine you initialize all Q-values to 0 and apply Q-learning. During the first episode, all actions but the last one will receive a reward <span class="math inline">\(r(s, a, s&#39;)\)</span> of 0 and arrive in a state where the greedy action has a value <span class="math inline">\(Q^\pi(s&#39;, a&#39;)\)</span> of 0, so the TD error <span class="math inline">\(\delta\)</span> is 0 and their Q-value will not change. Only the very last action will receive a non-zero reward and update its value slightly (because of the learning rate <span class="math inline">\(\alpha\)</span>). When this episode is performed again, the last action will again be updated, but also the one just before: <span class="math inline">\(Q^\pi(s&#39;, a&#39;)\)</span> is now different from 0 for this action, so the TD error is now different from 0. It is straightforward to see that if the episode has a length of 100 moves, the agent will need at least 100 episodes to “backpropagate” the final sparse reward to the first action of the episode. In practice, this is even worse: the learning rate <span class="math inline">\(\alpha\)</span> and the discount rate <span class="math inline">\(\gamma\)</span> will slow learning down even more. MC methods suffer less from this problem, as the first action of the episode would be updated using the actual return, which contains the final reward (although it is discounted by <span class="math inline">\(\gamma\)</span>).</p>
<p><strong>Eligibility traces</strong> can be seen a trick to mix the advantages of MC (faster updates) with the ones of TD (online learning, smaller variance). The idea is that the TD error at time <span class="math inline">\(t\)</span> (<span class="math inline">\(\delta_t\)</span>) will be used not only to update the action taken at time <span class="math inline">\(t\)</span> (<span class="math inline">\(\Delta Q(s_t, a_t) = \alpha \, \delta_t\)</span>), but also all the preceding actions, which are also responsible for the success or failure of the action taken at time <span class="math inline">\(t\)</span>. A parameter <span class="math inline">\(\lambda\)</span> between 0 and 1 (decaying factor) controls how far back in time a single TD error influences past actions. This is important when the policy is mostly exploratory: initial actions may be mostly random and finally find the the reward by chance. They should learn less from the reward than the last one, otherwise they would be systematically reproduced. Fig. <a href="#fig:eligibilitytraces">6</a> shows the principle of eligibility traces in a simple Gridworld environment.</p>
<figure>
<img src="img/gridworld-lambda.png" alt="Figure 6: Principle of eligibility traces applied to the Gridworld problem using SARSA(\lambda). Taken from Sutton and Barto (1998)." id="fig:eligibilitytraces" style="width:80.0%" /><figcaption>Figure 6: Principle of eligibility traces applied to the Gridworld problem using SARSA(<span class="math inline">\(\lambda\)</span>). Taken from <span class="citation" data-cites="Sutton1998">Sutton and Barto (<a href="#ref-Sutton1998" role="doc-biblioref">1998</a>)</span>.</figcaption>
</figure>
<p>There are many possible implementations of eligibility traces (Watkin’s, Peng, Tree Backup, etc. See the Chapter 12 of <span class="citation" data-cites="Sutton2017">Sutton and Barto (<a href="#ref-Sutton2017" role="doc-biblioref">2017</a>)</span>). Generally, one distinguished a forward and a backward view of eligibility traces.</p>
<ul>
<li>The <em>forward view</em> considers that one transition <span class="math inline">\((s_t, a_t)\)</span> gathers the TD errors made at future time steps <span class="math inline">\(t&#39;\)</span> and discounts them with the parameter <span class="math inline">\(\lambda\)</span>:</li>
</ul>
<p><span class="math display">\[
    Q^\pi(s_t, a_t) \leftarrow  Q^\pi(s_t, a_t) + \alpha \, \sum_{t&#39;=t}^T (\gamma \lambda)^{t&#39;-t} \delta_{t&#39;}
\]</span></p>
<p>From this equation, <span class="math inline">\(\gamma\)</span> and <span class="math inline">\(\lambda\)</span> seem to play a relatively similar role, but remember that <span class="math inline">\(\gamma\)</span> is also used in the TD error, so they control different aspects of learning. The drawback of this approach is that the future transitions at <span class="math inline">\(t&#39;&gt;t\)</span> and their respective TD errors must be known when updating the transition, so this prevents online learning (the episode must be terminated to apply the updates).</p>
<ul>
<li>The <em>backward view</em> considers that the TD error made at time <span class="math inline">\(t\)</span> is sent backwards in time to all transitions previously executed. The easiest way to implement this is to update an eligibility trace <span class="math inline">\(e(s,a)\)</span> for each possible transition, which is incremented every time a transition is visited and otherwise decays exponentially with a speed controlled by <span class="math inline">\(\lambda\)</span>:</li>
</ul>
<p><span class="math display">\[
    e(s, a) = \begin{cases} e(s, a) + 1 \quad \text{if} \quad s=s_t \quad \text{and} \quad a=a_t \\
                            \lambda \, e(s, a) \quad \text{otherwise.}
              \end{cases}
\]</span></p>
<p>The Q-value of <strong>all</strong> transitions <span class="math inline">\((s, a)\)</span> (not only the one just executed) is then updated proportionally to the corresponding trace and the current TD error:</p>
<p><span class="math display">\[
    Q^\pi(s, a) \leftarrow  Q^\pi(s, a) + \alpha \, e(s, a) \, \delta_{t} \quad \forall s, a
\]</span></p>
<p>The forward and backward implementations are equivalent: the first requires to know the future, the second requires to update many transitions at each time step. The best solution will depend on the complexity of the problem.</p>
<p>TD learning, SARSA and Q-learning can all be efficiently extended using eligibility traces. This gives the algorithms TD(<span class="math inline">\(\lambda\)</span>), SARSA(<span class="math inline">\(\lambda\)</span>) and Q(<span class="math inline">\(\lambda\)</span>), which can learn much faster than their 1-step equivalent, at the cost of more computations.</p>
<h3 id="sec:actor-critic-architectures"><span class="header-section-number">2.1.7</span> Actor-critic architectures</h3>
<p>Let’s consider the TD error based on state values:</p>
<p><span class="math display">\[
 \delta = r(s, a, s&#39;) + \gamma \, V^\pi(s&#39;) - V^\pi(s)
\]</span></p>
<p>As noted in the previous sections, the TD error represents how surprisingly good (or bad) a transition between two states has been (ergo the corresponding action). It can be used to update the value of the state <span class="math inline">\(s_t\)</span>:</p>
<p><span class="math display">\[
    V^\pi(s) \leftarrow V^\pi(s) + \alpha \, \delta
\]</span></p>
<p>This allows to estimate the values of all states for the current policy. However, this does not help to 1) directy select the best action or 2) improve the policy. When only the V-values are given, one can only want to reach the next state <span class="math inline">\(V^\pi(s&#39;)\)</span> with the highest value: one needs to know which action leads to this better state, i.e. have a model of the environment. Actually, one selects the action with the highest Q-value:</p>
<p><span class="math display">\[
    Q^{\pi}(s, a) = \sum_{s&#39; \in \mathcal{S}} p(s&#39;|s, a) [r(s, a, s&#39;) + \gamma \, V^\pi(s&#39;)]
\]</span></p>
<p>An action may lead to a high-valued state, but with such a small probability that it is actually not worth it. <span class="math inline">\(p(s&#39;|s, a)\)</span> and <span class="math inline">\(r(s, a, s&#39;)\)</span> therefore have to be known (or at least approximated), what defeats the purpose of sample-based methods.</p>
<figure>
<img src="img/actorcritic.png" alt="Figure 7: Actor-critic architecture (Sutton and Barto 1998)." id="fig:actorcritic" style="width:30.0%" /><figcaption>Figure 7: Actor-critic architecture <span class="citation" data-cites="Sutton1998">(Sutton and Barto <a href="#ref-Sutton1998" role="doc-biblioref">1998</a>)</span>.</figcaption>
</figure>
<p><strong>Actor-critic</strong> architectures have been proposed to solve this problem:</p>
<ol type="1">
<li>The <strong>critic</strong> learns to estimate the value of a state <span class="math inline">\(V^\pi(s)\)</span> and compute the RPE <span class="math inline">\(\delta = r(s, a, s&#39;) + \gamma \, V^\pi(s&#39;) - V^\pi(s)\)</span>.</li>
<li>The <strong>actor</strong> uses the RPE to update a <em>preference</em> for the executed action: action with positive RPEs (positively surprising) should be reinforced (i.e. taken again in the future), while actions with negative RPEs should be avoided in the future.</li>
</ol>
<p>The main interest of this architecture is that the actor can take any form (neural network, decision tree), as long as it able to use the RPE for learning. The simplest actor would be a softmax action selection mechanism, which maintains a <em>preference</em> <span class="math inline">\(p(s, a)\)</span> for each action and updates it using the TD error:</p>
<p><span class="math display">\[
    p(s, a) \leftarrow p(s, a) + \alpha \, \delta_t
\]</span></p>
<p>The policy uses the softmax rule on these preferences:</p>
<p><span class="math display">\[
    \pi(s, a) = \frac{p(s, a)}{\sum_a p(s, a)}
\]</span></p>
<p>Actor-critic algorithms learn at the same time two aspects of the problem:</p>
<ul>
<li>A value function (e.g. <span class="math inline">\(V^\pi(s)\)</span>) to compute the TD error in the critic,</li>
<li>A policy <span class="math inline">\(\pi\)</span> in the actor.</li>
</ul>
<p>Classical TD learning only learn a value function (<span class="math inline">\(V^\pi(s)\)</span> or <span class="math inline">\(Q^\pi(s, a)\)</span>): these methods are called <strong>value-based</strong> methods. Actor-critic architectures are particularly important in <strong>policy search</strong> methods (see Section <a href="#sec:policy-gradient-methods">4</a>).</p>
<h3 id="sec:function-approximation"><span class="header-section-number">2.1.8</span> Function approximation</h3>
<p>All the methods presented before are <em>tabular methods</em>, as one needs to store one value per state-action pair: either the Q-value of the action or a preference for that action. In most useful applications, the number of values to store would quickly become prohibitive: when working on raw images, the number of possible states alone is untractable. Moreover, these algorithms require that each state-action pair is visited a sufficient number of times to converge towards the optimal policy: if a single state-action pair is never visited, there is no guarantee that the optimal policy will be found. The problem becomes even more obvious when considering <em>continuous</em> state or action spaces.</p>
<p>However, in a lot of applications, the optimal action to perform in two very close states is likely to be the same: changing one pixel in a video game does not change which action should be applied. It would therefore be very useful to be able to <em>interpolate</em> Q-values between different states: only a subset of all state-action pairs has to explored; the others will be “guessed” depending on the proximity between the states and/or the actions. The problem is now <strong>generalization</strong>, i.e. transferring acquired knowledge to unseen but similar situations.</p>
<p>This is where <strong>function approximation</strong> becomes useful: the Q-values or the policy are not stored in a table, but rather learned by a function approximator. The type of function approximator does not really matter here: in deep RL we are of course interested in deep neural networks (Section <a href="#sec:deep-learning">2.2</a>), but any kind of regressor theoretically works (linear algorithms, radial-basis function network, SVR…).</p>
<h4 id="sec:value-based-function-approximation" class="unnumbered">Value-based function approximation</h4>
<p>In <strong>value-based</strong> methods, we want to approximate the Q-values <span class="math inline">\(Q^\pi(s,a)\)</span> of all possible state-action pairs for a given policy. The function approximator depends on a set of parameters <span class="math inline">\(\theta\)</span>. <span class="math inline">\(\theta\)</span> can for example represent all the weights and biases of a neural network. The approximated Q-value can now be noted <span class="math inline">\(Q(s, a ;\theta)\)</span> or <span class="math inline">\(Q_\theta(s, a)\)</span>. As the parameters will change over time during learning, we can omit the time <span class="math inline">\(t\)</span> from the notation. Similarly, action selection is usually <span class="math inline">\(\epsilon\)</span>-greedy or softmax, so the policy <span class="math inline">\(\pi\)</span> depends directly on the estimated Q-values and can therefore on the parameters: it is noted <span class="math inline">\(\pi_\theta\)</span>.</p>
<figure>
<img src="img/functionapprox.png" alt="Figure 8: Function approximators can either take a state-action pair as input and output the Q-value, or simply take a state as input and output the Q-values of all possible actions." id="fig:functionapprox" style="width:50.0%" /><figcaption>Figure 8: Function approximators can either take a state-action pair as input and output the Q-value, or simply take a state as input and output the Q-values of all possible actions.</figcaption>
</figure>
<p>There are basically two options regarding the structure of the function approximator (Fig. <a href="#fig:functionapprox">8</a>):</p>
<ol type="1">
<li>The approximator takes a state-action pair <span class="math inline">\((s, a)\)</span> as input and returns a single Q-value <span class="math inline">\(Q(s, a)\)</span>.</li>
<li>It takes a state <span class="math inline">\(s\)</span> as input and returns the Q-value of all possible actions in that state.</li>
</ol>
<p>The second option is of course only possible when the action space is discrete, but has the advantage to generalize better over similar states.</p>
<p>The goal of a function approximator is to minimize a <em>loss function</em> (or cost function) <span class="math inline">\(\mathcal{L}(\theta)\)</span>, so that the estimated Q-values converge for all state-pairs towards their target value, depending on the chosen algorithm:</p>
<ul>
<li>Monte-Carlo methods: the Q-value of each <span class="math inline">\((s, a)\)</span> pair should converge towards the mean expected return (in expectation):</li>
</ul>
<p><span class="math display">\[
    \mathcal{L}(\theta) = \mathbb{E}_\pi[(R_t - Q_\theta(s, a))^2]
\]</span></p>
<p>If we learn over <span class="math inline">\(N\)</span> episodes of length <span class="math inline">\(T\)</span>, the loss function can be approximated as:</p>
<p><span class="math display">\[
    \mathcal{L}(\theta) \approx \frac{1}{N} \sum_{e=1}^N \sum_{t = 1}^T [R^e_t - Q_\theta(s_t, a_t)]^2
\]</span></p>
<ul>
<li><p>Temporal difference methods: the Q-values should converge towards an estimation of the mean expected return.</p>
<ul>
<li>For SARSA:</li>
</ul>
<p><span class="math display">\[
  \mathcal{L}(\theta) = \mathbb{E}_\pi[(r(s, a, s&#39;) + \gamma \, Q_\theta(s&#39;, \pi(s&#39;)) - Q_\theta(s, a))^2]
  \]</span></p>
<ul>
<li>For Q-learning:</li>
</ul>
<p><span class="math display">\[
  \mathcal{L}(\theta) = \mathbb{E}_\pi[(r(s, a, s&#39;) + \gamma \, \max_{a&#39;} Q_\theta(s&#39;, a&#39;) - Q_\theta(s, a))^2]
  \]</span></p></li>
</ul>
<p>Any function approximator able to minimize these loss functions can be used.</p>
<h4 id="sec:policy-based-function-approximation" class="unnumbered">Policy-based function approximation</h4>
<p>In policy-based function approximation, we want to directly learn a policy <span class="math inline">\(\pi_\theta(s, a)\)</span> that maximizes the expected return of each possible transition, i.e. the ones which are selected by the policy. The <strong>objective function</strong> to be maximized is defined over all trajectories <span class="math inline">\(\tau = (s_0, a_0, s_1, a_1, \ldots, s_T, a_T)\)</span> conditioned by the policy:</p>
<p><span class="math display">\[
    J(\theta) = \mathbb{E}_{\tau \sim \rho_\theta} [R_t]
\]</span></p>
<p>In short, the learned policy <span class="math inline">\(\pi_\theta\)</span> should only produce trajectories <span class="math inline">\(\tau\)</span> where each state is associated to a high expected return <span class="math inline">\(R_t\)</span> and avoid trajectories with low expected returns. Although this objective function leads to the desired behavior, it is not computationally tractable as we would need to integrate over all possible trajectories. The methods presented in Section <a href="#sec:policy-gradient-methods">4</a> will provide estimates of the gradient of this objective function.</p>
<!--PAGEBREAK-->
<h2 id="sec:deep-learning"><span class="header-section-number">2.2</span> Deep learning</h2>
<p>Deep RL uses deep neural networks as function approximators, allowing complex representations of the value of state-action pairs to be learned. This section provides a very quick overview of deep learning. For additional details, refer to the excellent book of <span class="citation" data-cites="Goodfellow2016">Goodfellow, Bengio, and Courville (<a href="#ref-Goodfellow2016" role="doc-biblioref">2016</a>)</span>.</p>
<h3 id="sec:deep-neural-networks"><span class="header-section-number">2.2.1</span> Deep neural networks</h3>
<p>A deep neural network (DNN) consists of one input layer <span class="math inline">\(\mathbf{x}\)</span>, one or several hidden layers <span class="math inline">\(\mathbf{h_1}, \mathbf{h_2}, \ldots, \mathbf{h_n}\)</span> and one output layer <span class="math inline">\(\mathbf{y}\)</span> (Fig. <a href="#fig:dnn">9</a>).</p>
<figure>
<img src="img/dnn.png" alt="Figure 9: Architecture of a deep neural network. Figure taken from Nielsen (2015), CC-BY-NC." id="fig:dnn" style="width:60.0%" /><figcaption>Figure 9: Architecture of a deep neural network. Figure taken from <span class="citation" data-cites="Nielsen2015">Nielsen (<a href="#ref-Nielsen2015" role="doc-biblioref">2015</a>)</span>, CC-BY-NC.</figcaption>
</figure>
<p>Each layer <span class="math inline">\(k\)</span> (called <em>fully-connected</em>) transforms the activity of the previous layer (the vector <span class="math inline">\(\mathbf{h_{k-1}}\)</span>) into another vector <span class="math inline">\(\mathbf{h_{k}}\)</span> by multiplying it with a <strong>weight matrix</strong> <span class="math inline">\(W_k\)</span>, adding a <strong>bias</strong> vector <span class="math inline">\(\mathbf{b_k}\)</span> and applying a non-linear <strong>activation function</strong> <span class="math inline">\(f\)</span>.</p>
<p><span id="eq:fullyconnected"><span class="math display">\[
    \mathbf{h_{k}} = f(W_k \times \mathbf{h_{k-1}} + \mathbf{b_k})
\qquad(4)\]</span></span></p>
<p>The activation function can theoretically be of any type as long as it is non-linear (sigmoid, tanh…), but modern neural networks use preferentially the <strong>Rectified Linear Unit</strong> (ReLU) function <span class="math inline">\(f(x) = \max(0, x)\)</span> or its parameterized variants.</p>
<p>The goal of learning is to find the weights and biases <span class="math inline">\(\theta\)</span> minimizing a given <strong>loss function</strong> on a training set <span class="math inline">\(\mathcal{D}\)</span>.</p>
<ul>
<li>In <em>regression</em> problems, the <strong>mean square error</strong> (mse) is minimized:</li>
</ul>
<p><span class="math display">\[
    \mathcal{L}(\theta) = \mathbb{E}_{\mathbf{x}, \mathbf{t} \in \mathcal{D}} [||\mathbf{t} - \mathbf{y}||^2]
\]</span></p>
<p>where <span class="math inline">\(\mathbf{x}\)</span> is the input, <span class="math inline">\(\mathbf{t}\)</span> the true output (defined in the training set) and <span class="math inline">\(\mathbf{y}\)</span> the prediction of the NN for the input <span class="math inline">\(\mathbf{x}\)</span>. The closer the prediction from the true value, the smaller the mse.</p>
<ul>
<li>In <em>classification</em> problems, the <strong>cross entropy</strong> (or negative log-likelihood) is minimized:</li>
</ul>
<p><span class="math display">\[
    \mathcal{L}(\theta) = - \mathbb{E}_{\mathbf{x}, \mathbf{t} \in \mathcal{D}} [\sum_i t_i \log y_i]
\]</span></p>
<p>where the log-likelihood of the prediction <span class="math inline">\(\mathbf{y}\)</span> to match the data <span class="math inline">\(\mathbf{t}\)</span> is maximized over the training set. The mse could be used for classification problems too, but the output layer usually has a softmax activation function for classification problems, which works nicely with the cross entropy loss function. See <a href="https://rdipietro.github.io/friendly-intro-to-cross-entropy-loss" class="uri">https://rdipietro.github.io/friendly-intro-to-cross-entropy-loss</a> for the link between cross entropy and log-likelihood and <a href="https://deepnotes.io/softmax-crossentropy" class="uri">https://deepnotes.io/softmax-crossentropy</a> for the interplay between softmax and cross entropy.</p>
<p>Once the loss function is defined, it has to be minimized by searching optimal values for the free parameters <span class="math inline">\(\theta\)</span>. This optimization procedure is based on <strong>gradient descent</strong>, which is an iterative procedure modifying estimates of the free parameters in the opposite direction of the gradient of the loss function:</p>
<p><span class="math display">\[
\Delta \theta = -\eta \, \nabla_\theta \mathcal{L}(\theta) = -\eta \, \frac{\partial \mathcal{L}(\theta)}{\partial \theta}
\]</span></p>
<p>The learning rate <span class="math inline">\(\eta\)</span> is chosen very small to ensure a smooth convergence. Intuitively, the gradient (or partial derivative) represents how the loss function changes when each parameter is slightly increased. If the gradient w.r.t a single parameter (e.g. a weight <span class="math inline">\(w\)</span>) is positive, increasing the weight increases the loss function (i.e. the error), so the weight should be slightly decreased instead. If the gradient is negative, one should increase the weight.</p>
<p>The question is now to compute the gradient of the loss function w.r.t all the parameters of the DNN, i.e. each single weight and bias. The solution is given by the <strong>backpropagation</strong> algorithm, which is simply an application of the <strong>chain rule</strong> to feedforward neural networks:</p>
<p><span class="math display">\[
    \frac{\partial \mathcal{L}(\theta)}{\partial W_k} = \frac{\partial \mathcal{L}(\theta)}{\partial \mathbf{y}} \times \frac{\partial \mathbf{y}}{\partial \mathbf{h_n}} \times \frac{\partial \mathbf{h_n}}{\partial \mathbf{h_{n-1}}} \times \ldots \times \frac{\partial \mathbf{h_k}}{\partial W_k}
\]</span></p>
<p>Each layer of the network adds a contribution to the gradient when going <strong>backwards</strong> from the loss function to the parameters. Importantly, all functions used in a NN are differentiable, i.e. those partial derivatives exist (and are easy to compute). For the fully connected layer represented by Eq. <a href="#eq:fullyconnected">4</a>, the partial derivative is given by:</p>
<p><span class="math display">\[
    \frac{\partial \mathbf{h_{k}}}{\partial \mathbf{h_{k-1}}} = f&#39;(W_k \times \mathbf{h_{k-1}} + \mathbf{b_k}) \, W_k
\]</span></p>
<p>and its dependency on the parameters is:</p>
<p><span class="math display">\[
    \frac{\partial \mathbf{h_{k}}}{\partial W_k} = f&#39;(W_k \times \mathbf{h_{k-1}} + \mathbf{b_k}) \, \mathbf{h_{k-1}}
\]</span> <span class="math display">\[
    \frac{\partial \mathbf{h_{k}}}{\partial \mathbf{b_k}} = f&#39;(W_k \times \mathbf{h_{k-1}} + \mathbf{b_k})
\]</span></p>
<p>Activation functions are chosen to have an easy-to-compute derivative, such as the ReLU function:</p>
<p><span class="math display">\[
    f&#39;(x) = \begin{cases} 1 \quad \text{if} \quad x &gt; 0 \\ 0 \quad \text{otherwise.} \end{cases}
\]</span></p>
<p>Partial derivatives are automatically computed by the underlying libraries, such as tensorflow, theano, pytorch, etc. The next step is choose an <strong>optimizer</strong>, i.e. a gradient-based optimization method allow to modify the free parameters using the gradients. Optimizers do not work on the whole training set, but use <strong>minibatches</strong> (a random sample of training examples: their number is called the <em>batch size</em>) to compute iteratively the loss function. The most popular optimizers are:</p>
<ul>
<li>SGD (stochastic gradient descent): vanilla gradient descent on random minibatches.</li>
<li>SGD with momentum (Nesterov or not): additional momentum to avoid local minima of the loss function.</li>
<li>Adagrad</li>
<li>Adadelta</li>
<li>RMSprop</li>
<li>Adam</li>
<li>Many others. Check the doc of keras to see what is available: <a href="https://keras.io/optimizers" class="uri">https://keras.io/optimizers</a></li>
</ul>
<p>See this useful post for a comparison of the different optimizers: <a href="http://ruder.io/optimizing-gradient-descent" class="uri">http://ruder.io/optimizing-gradient-descent</a> <span class="citation" data-cites="Ruder2016">(Ruder <a href="#ref-Ruder2016" role="doc-biblioref">2016</a>)</span>. The common wisdom is that SGD with Nesterov momentum works best (i.e. it finds a better minimum) but its meta-parameters (learning rate, momentum) are hard to find, while Adam works out-of-the-box, at the cost of a slightly worse minimum. For deep RL, Adam is usually preferred, as the goal is to quickly find a working solution, not to optimize it to the last decimal.</p>
<!-- ![Comparison of different optimizers. Source: @Ruder2016, <http://ruder.io/optimizing-gradient-descent>.](img/optimizers.gif){#fig:optimizers width=50%} -->
<p>Additional regularization mechanisms are now typically part of DNNs in order to avoid overfitting (learning by heart the training set but failing to generalize): L1/L2 regularization, dropout, batch normalization, etc. Refer to <span class="citation" data-cites="Goodfellow2016">Goodfellow, Bengio, and Courville (<a href="#ref-Goodfellow2016" role="doc-biblioref">2016</a>)</span> for further details.</p>
<h3 id="sec:convolutional-networks"><span class="header-section-number">2.2.2</span> Convolutional networks</h3>
<p>Convolutional Neural Networks (CNN) are an adaptation of DNNs to deal with highly dimensional input spaces such as images. The idea is that neurons in the hidden layer reuse (“share”) weights over the input image, as the features learned by early layers are probably local in visual classification tasks: in computer vision, an edge can be detected by the same filter all over the input image.</p>
<p>A <strong>convolutional layer</strong> learns to extract a given number of features (typically 16, 32, 64, etc) represented by 3x3 or 5x5 matrices. These matrices are then convoluted over the whole input image (or the previous convolutional layer) to produce <strong>feature maps</strong>. If the input image has a size NxMx1 (grayscale) or NxMx3 (colored), the convolutional layer will be a tensor of size NxMxF, where F is the number of extracted features. Padding issues may reduce marginally the spatial dimensions. One important aspect is that the convolutional layer is fully differentiable, so backpropagation and the usual optimizers can be used to learn the filters.</p>
<figure>
<img src="img/convlayer.gif" alt="Figure 10: Convolutional layer. Source: https://github.com/vdumoulin/conv_arithmetic." id="fig:convlayer" style="width:50.0%" /><figcaption>Figure 10: Convolutional layer. Source: <a href="https://github.com/vdumoulin/conv_arithmetic" class="uri">https://github.com/vdumoulin/conv_arithmetic</a>.</figcaption>
</figure>
<p>After a convolutional layer, the spatial dimensions are preserved. In classification tasks, it does not matter where the object is in the image, the only thing that matters is what it is: classification requires <strong>spatial invariance</strong> in the learned representations. The <strong>max-pooling layer</strong> was introduced to downsample each feature map individually and increase their spatial invariance. Each feature map is divided into 2x2 blocks (generally): only the maximal feature activation in that block is preserved in the max-pooling layer. This reduces the spatial dimensions by a factor two in each direction, but keeps the number of features equal.</p>
<figure>
<img src="img/maxpooling.png" alt="Figure 11: Max-pooling layer. Source: Stanford’s CS231n course http://cs231n.github.io/convolutional-networks" id="fig:maxpooling" /><figcaption>Figure 11: Max-pooling layer. Source: Stanford’s CS231n course <a href="http://cs231n.github.io/convolutional-networks" class="uri">http://cs231n.github.io/convolutional-networks</a></figcaption>
</figure>
<p>A convolutional neural network is simply a sequence of convolutional layers and max-pooling layers (sometime two convolutional layers are applied in a row before max-pooling, as in VGG <span class="citation" data-cites="Simonyan2015">(Simonyan and Zisserman <a href="#ref-Simonyan2015" role="doc-biblioref">2015</a>)</span>), followed by a couple of fully-connected layers and a softmax output layer. Fig. <a href="#fig:alexnet">12</a> shows the architecture of AlexNet, the winning architecture of the ImageNet challenge in 2012 <span class="citation" data-cites="Krizhevsky2012">(Krizhevsky, Sutskever, and Hinton <a href="#ref-Krizhevsky2012" role="doc-biblioref">2012</a>)</span>.</p>
<figure>
<img src="img/alexnet.png" alt="Figure 12: Architecture of the AlexNet CNN. Taken from Krizhevsky, Sutskever, and Hinton (2012)." id="fig:alexnet" /><figcaption>Figure 12: Architecture of the AlexNet CNN. Taken from <span class="citation" data-cites="Krizhevsky2012">Krizhevsky, Sutskever, and Hinton (<a href="#ref-Krizhevsky2012" role="doc-biblioref">2012</a>)</span>.</figcaption>
</figure>
<p>Many improvements have been proposed since 2012 (e.g. ResNets <span class="citation" data-cites="He2015">(He et al. <a href="#ref-He2015" role="doc-biblioref">2015</a>)</span>) but the idea stays similar. Generally, convolutional and max-pooling layers are alternated until the spatial dimensions are so reduced (around 10x10) that they can be put into a single vector and fed into a fully-connected layer. This is <strong>NOT</strong> the case in deep RL! Contrary to object classification, spatial information is crucial in deep RL: position of the ball, position of the body, etc. It matters whether the ball is to the right or to the left of your paddle when you decide how to move it. Max-pooling layers are therefore omitted and the CNNs only consist of convolutional and fully-connected layers. This greatly increases the number of weights in the networks, hence the number of training examples needed to train the network. This is still the main limitation of using CNNs in deep RL.</p>
<h3 id="sec:recurrent-neural-networks"><span class="header-section-number">2.2.3</span> Recurrent neural networks</h3>
<p>Feedforward neural networks learn to efficiently map static inputs <span class="math inline">\(\mathbf{x}\)</span> to outputs <span class="math inline">\(\mathbf{y}\)</span> but have no memory or context: the output at time <span class="math inline">\(t\)</span> does not depend on the inputs at time <span class="math inline">\(t-1\)</span> or <span class="math inline">\(t-2\)</span>, only the one at time <span class="math inline">\(t\)</span>. This is problematic when dealing with video sequences for example: if the task is to classify videos into happy/sad, a frame by frame analysis is going to be inefficient (most frames a neutral). Concatenating all frames in a giant input vector would increase dramatically the complexity of the classifier and no generalization can be expected.</p>
<p>Recurrent Neural Networks (RNN) are designed to deal with time-varying inputs, where the relevant information to take a decision at time <span class="math inline">\(t\)</span> may have happened at different times in the past. The general structure of a RNN is depicted on Fig. <a href="#fig:rnn">13</a>:</p>
<figure>
<img src="img/RNN-unrolled.png" alt="Figure 13: Architecture of a RNN. Left: recurrent architecture. Right: unrolled network, showing that a RNN is equivalent to a deep network. Taken from http://colah.github.io/posts/2015-08-Understanding-LSTMs." id="fig:rnn" style="width:90.0%" /><figcaption>Figure 13: Architecture of a RNN. Left: recurrent architecture. Right: unrolled network, showing that a RNN is equivalent to a deep network. Taken from <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs" class="uri">http://colah.github.io/posts/2015-08-Understanding-LSTMs</a>.</figcaption>
</figure>
<p>The output <span class="math inline">\(\mathbf{h}_t\)</span> of the RNN at time <span class="math inline">\(t\)</span> depends on its current input <span class="math inline">\(\mathbf{x}_t\)</span>, but also on its previous output <span class="math inline">\(\mathbf{h}_{t-1}\)</span>, which, by recursion, depends on the whole history of inputs <span class="math inline">\((x_0, x_1, \ldots, x_t)\)</span>.</p>
<p><span class="math display">\[
    \mathbf{h}_t = f(W_x \, \mathbf{x}_{t} + W_h \, \mathbf{h}_{t-1} + \mathbf{b})
\]</span></p>
<p>Once unrolled, a RNN is equivalent to a deep network, with <span class="math inline">\(t\)</span> layers of weights between the first input <span class="math inline">\(\mathbf{x}_0\)</span> and the current output <span class="math inline">\(\mathbf{h}_t\)</span>. The only difference with a feedforward network is that weights are reused between two time steps / layers. <strong>Backpropagation though time</strong> (BPTT) can be used to propagate the gradient of the loss function backwards in time and learn the weights <span class="math inline">\(W_x\)</span> and <span class="math inline">\(W_h\)</span> using the usual optimizer (SGD, Adam…).</p>
<p>However, this kind of RNN can only learn short-term dependencies because of the <strong>vanishing gradient problem</strong> <span class="citation" data-cites="Hochreiter1991">(Hochreiter <a href="#ref-Hochreiter1991" role="doc-biblioref">1991</a>)</span>. When the gradient of the loss function travels backwards from <span class="math inline">\(\mathbf{h}_t\)</span> to <span class="math inline">\(\mathbf{x}_0\)</span>, it will be multiplied <span class="math inline">\(t\)</span> times by the recurrent weights <span class="math inline">\(W_h\)</span>. If <span class="math inline">\(|W_h| &gt; 1\)</span>, the gradient will explode with increasing <span class="math inline">\(t\)</span>, while if <span class="math inline">\(|W_h| &lt; 1\)</span>, the gradient will vanish to 0.</p>
<p>The solution to this problem is provided by <strong>long short-term memory networks</strong> <span class="citation" data-cites="Hochreiter1997">(LSTM; Hochreiter and Schmidhuber <a href="#ref-Hochreiter1997" role="doc-biblioref">1997</a>)</span>. LSTM layers maintain additionally a state <span class="math inline">\(\mathbf{C}_t\)</span> (also called context or memory) which is manipulated by three learnable gates (input, forget and output gates). As in regular RNNs, a <em>candidate state</em> <span class="math inline">\(\tilde{\mathbf{C}_t}\)</span> is computed based on the current input and the previous output:</p>
<p><span class="math display">\[
    \tilde{\mathbf{C}_t} = f(W_x \, \mathbf{x}_{t} + W_h \, \mathbf{h}_{t-1} + \mathbf{b})
\]</span></p>
<figure>
<img src="img/LSTM.png" alt="Figure 14: Architecture of a LSTM layer. Taken from http://colah.github.io/posts/2015-08-Understanding-LSTMs." id="fig:lstm" style="width:40.0%" /><figcaption>Figure 14: Architecture of a LSTM layer. Taken from <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs" class="uri">http://colah.github.io/posts/2015-08-Understanding-LSTMs</a>.</figcaption>
</figure>
<p>The activation function <span class="math inline">\(f\)</span> is usually a tanh function. The input and forget learn to decide how the candidate state should be used to update the current state:</p>
<ul>
<li>The input gate decides which part of the candidate state <span class="math inline">\(\tilde{\mathbf{C}_t}\)</span> will be used to update the current state <span class="math inline">\(\mathbf{C}_t\)</span>:</li>
</ul>
<p><span class="math display">\[
    \mathbf{i}_t = \sigma(W^i_x \, \mathbf{x}_{t} + W^i_h \, \mathbf{h}_{t-1} + \mathbf{b}^i)
\]</span></p>
<p>The sigmoid activation function <span class="math inline">\(\sigma\)</span> is used to output a number between 0 and 1 for each neuron: 0 means the candidate state will not be used at all, 1 means completely.</p>
<ul>
<li>The forget gate decides which part of the current state should be kept or forgotten:</li>
</ul>
<p><span class="math display">\[
    \mathbf{f}_t = \sigma(W^f_x \, \mathbf{x}_{t} + W^f_h \, \mathbf{h}_{t-1} + \mathbf{b}^f)
\]</span></p>
<p>Similarly, 0 means that the corresponding element of the current state will be erased, 1 that it will be kept.</p>
<p>Once the input and forget gates are computed, the current state can be updated based on its previous value and the candidate state:</p>
<p><span class="math display">\[
   \mathbf{C}_t =  \mathbf{i}_t \odot \tilde{\mathbf{C}_t} + \mathbf{f}_t \odot \mathbf{C}_{t-1}
\]</span></p>
<p>where <span class="math inline">\(\odot\)</span> is the element-wise multiplication.</p>
<ul>
<li>The output gate finally learns to select which part of the current state <span class="math inline">\(\mathbf{C}_t\)</span> should be used to produce the current output <span class="math inline">\(\mathbf{h}_t\)</span>:</li>
</ul>
<p><span class="math display">\[
    \mathbf{o}_t = \sigma(W^o_x \, \mathbf{x}_{t} + W^o_h \, \mathbf{h}_{t-1} + \mathbf{b}^o)
\]</span></p>
<p><span class="math display">\[
    \mathbf{h}_t = \mathbf{o}_t \odot \tanh \mathbf{C}_t
\]</span></p>
<p>The architecture may seem complex, but everything is differentiable: backpropagation though time can be used to learn not only the input and recurrent weights for the candidate state, but also the weights and and biases of the gates. The main advantage of LSTMs is that they solve the vanishing gradient problem: if the input at time <span class="math inline">\(t=0\)</span> is important to produce a response at time <span class="math inline">\(t\)</span>, the input gate will learn to put it into the memory and the forget gate will learn to maintain in the current state until it is not needed anymore. During this “working memory” phase, the gradient is multiplied by exactly one as nothing changes: the dependency can be learned with arbitrary time delays!</p>
<p>There are alternatives to the classical LSTM layer such as the gated recurrent unit <span class="citation" data-cites="Cho2014">(GRU; Cho et al. <a href="#ref-Cho2014" role="doc-biblioref">2014</a>)</span> or peephole connections <span class="citation" data-cites="Gers2001">(Gers <a href="#ref-Gers2001" role="doc-biblioref">2001</a>)</span>. See <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs" class="uri">http://colah.github.io/posts/2015-08-Understanding-LSTMs</a>, <a href="https://medium.com/mlreview/understanding-lstm-and-its-diagrams-37e2f46f1714" class="uri">https://medium.com/mlreview/understanding-lstm-and-its-diagrams-37e2f46f1714</a> or <a href="http://blog.echen.me/2017/05/30/exploring-lstms/" class="uri">http://blog.echen.me/2017/05/30/exploring-lstms/</a> for more visual explanations of LSTMs and their variants.</p>
<p>RNNs are particularly useful for deep RL when considering POMDPs, i.e. partially observable problems. If an observation does not contain enough information about the underlying state (e.g. a single image does not contain speed information), LSTM can integrate these observations over time and learn to implicitly represent speed in its context vector, allowing efficient policies to be learned.</p>
<!--PAGEBREAK-->
<h1 id="sec:value-based-methods"><span class="header-section-number">3</span> Value-based methods</h1>
<h2 id="sec:limitations-of-deep-neural-networks-for-function-approximation"><span class="header-section-number">3.1</span> Limitations of deep neural networks for function approximation</h2>
<p>The goal of value-based deep RL is to approximate the Q-value of each possible state-action pair using a deep (convolutional) neural network. As shown on Fig. <a href="#fig:functionapprox2">15</a>, the network can either take a state-action pair as input and return a single output value, or take only the state as input and return the Q-value of all possible actions (only possible if the action space is discrete), In both cases, the goal is to learn estimates <span class="math inline">\(Q_\theta(s, a)\)</span> with a NN with parameters <span class="math inline">\(\theta\)</span>.</p>
<figure>
<img src="img/functionapprox.png" alt="Figure 15: Function approximators can either associate a state-action pair (s, a) to its Q-value (left), or associate a state s to the Q-values of all actions possible in that state (right)." id="fig:functionapprox2" style="width:60.0%" /><figcaption>Figure 15: Function approximators can either associate a state-action pair <span class="math inline">\((s, a)\)</span> to its Q-value (left), or associate a state <span class="math inline">\(s\)</span> to the Q-values of all actions possible in that state (right).</figcaption>
</figure>
<p>When using Q-learning, we have already seen in Section <a href="#sec:function-approximation">2.1.8</a> that the problem is a regression problem, where the following mse loss function has to be minimized:</p>
<p><span class="math display">\[
    \mathcal{L}(\theta) = \mathbb{E}_\pi[(r_t + \gamma \, \max_{a&#39;} Q_\theta(s&#39;, a&#39;) - Q_\theta(s, a))^2]
\]</span></p>
<p>In short, we want to reduce the prediction error, i.e. the mismatch between the estimate of the value of an action <span class="math inline">\(Q_\theta(s, a)\)</span> and the real expected return, here approximated with <span class="math inline">\(r(s, a, s&#39;) + \gamma \, \text{max}_{a&#39;} Q_\theta(s&#39;, a&#39;)\)</span>.</p>
<p>We can compute this loss by gathering enough samples <span class="math inline">\((s, a, r, s&#39;)\)</span> (i.e. single transitions), concatenating them randomly in minibatches, and let the DNN learn to minimize the prediction error using backpropagation and SGD, indirectly improving the policy. The following pseudocode would describe the training procedure when gathering transitions <strong>online</strong>, i.e. when directly interacting with the environment:</p>
<hr />
<ul>
<li>Initialize value network <span class="math inline">\(Q_{\theta}\)</span> with random weights.</li>
<li>Initialize empty minibatch <span class="math inline">\(\mathcal{D}\)</span> of maximal size <span class="math inline">\(n\)</span>.</li>
<li>Observe the initial state <span class="math inline">\(s_0\)</span>.</li>
<li>for <span class="math inline">\(t \in [0, T_\text{total}]\)</span>:
<ul>
<li>Select the action <span class="math inline">\(a_t\)</span> based on the behavior policy derived from <span class="math inline">\(Q_\theta(s_t, a)\)</span> (e.g. softmax).</li>
<li>Perform the action <span class="math inline">\(a_t\)</span> and observe the next state <span class="math inline">\(s_{t+1}\)</span> and the reward <span class="math inline">\(r_{t+1}\)</span>.</li>
<li>Predict the Q-value of the greedy action in the next state <span class="math inline">\(\max_{a&#39;} Q_\theta(s_{t+1}, a&#39;)\)</span></li>
<li>Store <span class="math inline">\((s_t, a_t, r_{t+1} + \gamma \, \max_{a&#39;} Q_\theta(s_{t+1}, a&#39;))\)</span> in the minibatch.</li>
<li>If minibatch <span class="math inline">\(\mathcal{D}\)</span> is full:
<ul>
<li>Train the value network <span class="math inline">\(Q_{\theta}\)</span> on <span class="math inline">\(\mathcal{D}\)</span> to minimize <span class="math inline">\(\mathcal{L}(\theta) = \mathbb{E}_\mathcal{D}[(r(s, a, s&#39;) + \gamma \, \text{max}_{a&#39;} Q_\theta(s&#39;, a&#39;) - Q_\theta(s, a))^2]\)</span></li>
<li>Empty the minibatch <span class="math inline">\(\mathcal{D}\)</span>.</li>
</ul></li>
</ul></li>
</ul>
<hr />
<p>However, the definition of the loss function uses the mathematical expectation operator <span class="math inline">\(E\)</span> over all transitions, which can only be approximated by <strong>randomly</strong> sampling the distribution (the MDP). This implies that the samples concatenated in a minibatch should be independent from each other (i.i.d). When gathering transitions online, the samples are correlated: <span class="math inline">\((s_t, a_t, r_{t+1}, s_{t+1})\)</span> will be followed by <span class="math inline">\((s_{t+1}, a_{t+1}, r_{t+2}, s_{t+2})\)</span>, etc. When playing video games, two successive frames will be very similar (a few pixels will change, or even none if the sampling rate is too high) and the optimal action will likely not change either (to catch the ball in pong, you will need to perform the same action - going left - many times in a row).</p>
<p><strong>Correlated inputs/outputs</strong> are very bad for deep neural networks: the DNN will overfit and fall into a very bad local minimum. That is why stochastic gradient descent works so well: it randomly samples values from the training set to form minibatches and minimize the loss function on these uncorrelated samples (hopefully). If all samples of a minibatch were of the same class (e.g. zeros in MNIST), the network would converge poorly. This is the first problem preventing an easy use of deep neural networks as function approximators in RL.</p>
<p>The second major problem is the <strong>non-stationarity</strong> of the targets in the loss function. In classification or regression, the desired values <span class="math inline">\(\mathbf{t}\)</span> are fixed throughout learning: the class of an object does not change in the middle of the training phase.</p>
<p><span class="math display">\[
    \mathcal{L}(\theta) = - \mathbb{E}_{\mathbf{x}, \mathbf{t} \in \mathcal{D}}[ ||\mathbf{t} - \mathbf{y}||^2]
\]</span></p>
<p>In Q-learning, the target <span class="math inline">\(r(s, a, s&#39;) + \gamma \, \max_{a&#39;} Q_\theta(s&#39;, a&#39;)\)</span> will change during learning, as <span class="math inline">\(Q_\theta(s&#39;, a&#39;)\)</span> depends on the weights <span class="math inline">\(\theta\)</span> and will hopefully increase as the performance improves. This is the second problem of deep RL: deep NN are particularly bad on non-stationary problems, especially feedforward networks. They iteratively converge towards the desired value, but have troubles when the target also moves (like a dog chasing its tail).</p>
<h2 id="sec:deep-q-network-dqn"><span class="header-section-number">3.2</span> Deep Q-Network (DQN)</h2>
<p><span class="citation" data-cites="Mnih2015">Mnih et al. (<a href="#ref-Mnih2015" role="doc-biblioref">2015</a>)</span> (originally arXived in <span class="citation" data-cites="Mnih2013">Mnih et al. (<a href="#ref-Mnih2013" role="doc-biblioref">2013</a>)</span>) proposed an elegant solution to the problems of correlated inputs/outputs and non-stationarity inherent to RL. This article is a milestone of deep RL and it is fair to say that it started or at least strongly renewed the interest for deep RL.</p>
<p>The first idea proposed by <span class="citation" data-cites="Mnih2015">Mnih et al. (<a href="#ref-Mnih2015" role="doc-biblioref">2015</a>)</span> solves the problem of correlated input/outputs and is actually quite simple: instead of feeding successive transitions into a minibatch and immediately training the NN on it, transitions are stored in a huge buffer called <strong>experience replay memory</strong> (ERM) or <strong>replay buffer</strong> able to store 100000 transitions. When the buffer is full, new transitions replace the old ones. SGD can now randomly sample the ERM to form minibatches and train the NN.</p>
<figure>
<img src="img/ERM.png" alt="Figure 16: Experience replay memory. Interactions with the environment are stored in the ERM. Random minibatches are sampled from it to train the DQN value network." id="fig:erm" style="width:40.0%" /><figcaption>Figure 16: Experience replay memory. Interactions with the environment are stored in the ERM. Random minibatches are sampled from it to train the DQN value network.</figcaption>
</figure>
<p>The second idea solves the non-stationarity of the targets <span class="math inline">\(r(s, a, s&#39;) + \gamma \, \max_{a&#39;} Q_\theta(s&#39;, a&#39;)\)</span>. Instead of computing it with the current parameters <span class="math inline">\(\theta\)</span> of the NN, they are computed with an old version of the NN called the <strong>target network</strong> with parameters <span class="math inline">\(\theta&#39;\)</span>. The target network is updated only infrequently (every thousands of iterations or so) with the learned weights <span class="math inline">\(\theta\)</span>. As this target network does not change very often, the targets stay constant for a long period of time, and the problem becomes more stationary.</p>
<p>The resulting algorithm is called <strong>Deep Q-Network (DQN)</strong>. It is summarized by the following pseudocode:</p>
<hr />
<ul>
<li>Initialize value network <span class="math inline">\(Q_{\theta}\)</span> with random weights.</li>
<li>Copy <span class="math inline">\(Q_{\theta}\)</span> to create the target network <span class="math inline">\(Q_{\theta&#39;}\)</span>.</li>
<li>Initialize experience replay memory <span class="math inline">\(\mathcal{D}\)</span> of maximal size <span class="math inline">\(N\)</span>.</li>
<li>Observe the initial state <span class="math inline">\(s_0\)</span>.</li>
<li>for <span class="math inline">\(t \in [0, T_\text{total}]\)</span>:
<ul>
<li>Select the action <span class="math inline">\(a_t\)</span> based on the behavior policy derived from <span class="math inline">\(Q_\theta(s_t, a)\)</span> (e.g. softmax).</li>
<li>Perform the action <span class="math inline">\(a_t\)</span> and observe the next state <span class="math inline">\(s_{t+1}\)</span> and the reward <span class="math inline">\(r_{t+1}\)</span>.</li>
<li>Store <span class="math inline">\((s_t, a_t, r_{t+1}, s_{t+1})\)</span> in the experience replay memory.</li>
<li>Every <span class="math inline">\(T_\text{train}\)</span> steps:
<ul>
<li>Sample a minibatch <span class="math inline">\(\mathcal{D}_s\)</span> randomly from <span class="math inline">\(\mathcal{D}\)</span>.</li>
<li>For each transition <span class="math inline">\((s, a, r, s&#39;)\)</span> in the minibatch:
<ul>
<li>Predict the Q-value of the greedy action in the next state <span class="math inline">\(\max_{a&#39;} Q_{\theta&#39;}(s&#39;, a&#39;)\)</span> using the target network.</li>
<li>Compute the target value <span class="math inline">\(y = r + \gamma \, \max_{a&#39;} Q_{\theta&#39;}(s&#39;, a&#39;)\)</span>.</li>
</ul></li>
<li>Train the value network <span class="math inline">\(Q_{\theta}\)</span> on <span class="math inline">\(\mathcal{D}_s\)</span> to minimize <span class="math inline">\(\mathcal{L}(\theta) = \mathbb{E}_{\mathcal{D}_s}[(y - Q_\theta(s, a))^2]\)</span></li>
</ul></li>
<li>Every <span class="math inline">\(T_\text{target}\)</span> steps:
<ul>
<li>Update the target network with the trained value network: <span class="math inline">\(\theta&#39; \leftarrow \theta\)</span></li>
</ul></li>
</ul></li>
</ul>
<hr />
<p>In this document, pseudocode will omit many details to simplify the explanations (for example here, the case where a state is terminal - the game ends - and the next state has to be chosen from the distribution of possible starting states). Refer to the original publication for more exact algorithms.</p>
<p>The first thing to notice is that experienced transitions are not immediately used for learning, but simply stored in the ERM to be sampled later. Due to the huge size of the ERM, it is even likely that the recently experienced transition will only be used for learning hundreds or thousands of steps later. Meanwhile, very old transitions, generated using an initially bad policy, can be used to train the network for a very long time.</p>
<p>The second thing is that the target network is not updated very often (<span class="math inline">\(T_\text{target}=10000\)</span>), so the target values are going to be wrong a long time. More recent algorithms such as DDPG (Section <a href="#sec:deep-deterministic-policy-gradient-ddpg">4.4.2</a>) use a a smoothed version of the current weights, as proposed in <span class="citation" data-cites="Lillicrap2015">Lillicrap et al. (<a href="#ref-Lillicrap2015" role="doc-biblioref">2015</a>)</span>:</p>
<p><span class="math display">\[
    \theta&#39; = \tau \, \theta + (1-\tau) \, \theta&#39;
\]</span></p>
<p>If this rule is applied after each step with a very small rate <span class="math inline">\(\tau\)</span>, the target network will slowly track the learned network, but never be the same.</p>
<p>These two facts make DQN extremely slow to learn: millions of transitions are needed to obtain a satisfying policy. This is called the <strong>sample complexity</strong>, i.e. the number of transitions needed to obtain a satisfying performance. DQN finds very good policies, but at the cost of a very long training time.</p>
<p>DQN was initially applied to solve various Atari 2600 games. Video frames were used as observations and the set of possible discrete actions was limited (left/right/up/down, shoot, etc). The CNN used is depicted on Fig. <a href="#fig:dqn">17</a>. It has two convolutional layers, no max-pooling, 2 fully-connected layer and one output layer representing the Q-value of all possible actions in the games.</p>
<figure>
<img src="img/dqn.png" alt="Figure 17: Architecture of the CNN used in the original DQN paper. Taken from Mnih et al. (2015)." id="fig:dqn" /><figcaption>Figure 17: Architecture of the CNN used in the original DQN paper. Taken from <span class="citation" data-cites="Mnih2015">Mnih et al. (<a href="#ref-Mnih2015" role="doc-biblioref">2015</a>)</span>.</figcaption>
</figure>
<p>The problem of partial observability is solved by concatenating the four last video frames into a single tensor used as input to the CNN. The convolutional layers become able through learning to extract the speed information from it. Some of the Atari games (Pinball, Breakout) were solved with a performance well above human level, especially when they are mostly reactive. Games necessitating more long-term planning (Montezuma’ Revenge) were still poorly learned, though.</p>
<p>Beside being able to learn using delayed and sparse rewards in highly dimensional input spaces, the true <em>tour de force</em> of DQN is that it was able to learn the 49 Atari games in a row, using the same architecture and hyperparameters, and without resetting the weights between two games: knowledge acquired in one game could be reused for the next game. This created great excitement, as the ability to reuse knowledge over different tasks is a fundamental property of true intelligence.</p>
<h2 id="sec:double-dqn"><span class="header-section-number">3.3</span> Double DQN</h2>
<p>In DQN, the experience replay memory and the target network were decisive in allowing the CNN to learn the tasks through RL. Their drawback is that they drastically slow down learning and increase the sample complexity. Additionally, DQN has stability issues: the same network may not converge the same way in different runs. One first improvement on DQN was proposed by <span class="citation" data-cites="vanHasselt2015">van Hasselt, Guez, and Silver (<a href="#ref-vanHasselt2015" role="doc-biblioref">2015</a>)</span> and called <strong>double DQN</strong>.</p>
<p>The idea is that the target value <span class="math inline">\(y = r(s, a, s&#39;) + \gamma \, \max_{a&#39;} Q_{\theta&#39;}(s&#39;, a&#39;)\)</span> is frequently over-estimating the true expected return because of the max operator. Especially at the beginning of learning when Q-values are far from being correct, if an action is over-estimated (<span class="math inline">\(Q_{\theta&#39;}(s&#39;, a)\)</span> is higher that its true value) and selected by the target network as the next greedy action, the learned Q-value <span class="math inline">\(Q_{\theta}(s, a)\)</span> will also become over-estimated, what will propagate to all previous actions on the long-term. <span class="citation" data-cites="vanHasselt2010">van Hasselt (<a href="#ref-vanHasselt2010" role="doc-biblioref">2010</a>)</span> showed that this over-estimation is inevitable in regular Q-learning and proposed <strong>double learning</strong>.</p>
<p>The idea is to train independently two value networks: one will be used to find the greedy action (the action with the maximal Q-value), the other to estimate the Q-value itself. Even if the first network choose an over-estimated action as the greedy action, the other might provide a less over-estimated value for it, solving the problem.</p>
<p>Applying double learning to DQN is particularly straightforward: there are already two value networks, the trained network and the target network. Instead of using the target network to both select the greedy action in the next state and estimate its Q-value, here the trained network <span class="math inline">\(\theta\)</span> is used to select the greedy action <span class="math inline">\(a^* = \text{argmax}_{a&#39;} Q_\theta (s&#39;, a&#39;)\)</span> while the target network only estimates its Q-value. The target value becomes:</p>
<p><span class="math display">\[
    y = r(s, a, s&#39;) + \gamma \, Q_{\theta&#39;}(s&#39;, \text{argmax}_{a&#39;} Q_\theta (s&#39;, a&#39;))
\]</span></p>
<p>This induces only a small modification of the DQN algorithm and significantly improves its performance and stability:</p>
<hr />
<ul>
<li>Every <span class="math inline">\(T_\text{train}\)</span> steps:
<ul>
<li>Sample a minibatch <span class="math inline">\(\mathcal{D}_s\)</span> randomly from <span class="math inline">\(\mathcal{D}\)</span>.</li>
<li>For each transition <span class="math inline">\((s, a, r, s&#39;)\)</span> in the minibatch:
<ul>
<li>Select the greedy action in the next state <span class="math inline">\(a^* = \text{argmax}_{a&#39;} Q_\theta (s&#39;, a&#39;)\)</span> using the trained network.</li>
<li>Predict its Q-value <span class="math inline">\(Q_{\theta&#39;}(s&#39;, a^*)\)</span> using the target network.</li>
<li>Compute the target value <span class="math inline">\(y = r + \gamma \, Q_{\theta&#39;}(s&#39;, a*)\)</span>.</li>
</ul></li>
</ul></li>
</ul>
<hr />
<h2 id="sec:prioritized-experience-replay"><span class="header-section-number">3.4</span> Prioritized experience replay</h2>
<p>Another drawback of the original DQN is that the experience replay memory is sampled uniformly. Novel and interesting transitions are selected with the same probability as old well-predicted transitions, what slows down learning. The main idea of <strong>prioritized experience replay</strong> <span class="citation" data-cites="Schaul2015">(Schaul et al. <a href="#ref-Schaul2015" role="doc-biblioref">2015</a>)</span> is to order the transitions in the experience replay memory in decreasing order of their TD error:</p>
<p><span class="math display">\[
    \delta = r(s, a, s&#39;) + \gamma \, Q_{\theta&#39;}(s&#39;, \text{argmax}_{a&#39;} Q_\theta (s&#39;, a&#39;)) - Q_\theta(s, a)
\]</span></p>
<p>and sample with a higher probability those surprising transitions to form a minibatch. However, non-surprising transitions might become relevant again after enough training, as the <span class="math inline">\(Q_\theta(s, a)\)</span> change, so prioritized replay has a softmax function over the TD error to ensure “exploration” of memorized transitions. This data structure has of course a non-negligible computational cost, but accelerates learning so much that it is worth it. See <a href="https://jaromiru.com/2016/11/07/lets-make-a-dqn-double-learning-and-prioritized-experience-replay/" class="uri">https://jaromiru.com/2016/11/07/lets-make-a-dqn-double-learning-and-prioritized-experience-replay/</a> for a presentation of double DQN with prioritized replay.</p>
<h2 id="sec:duelling-network"><span class="header-section-number">3.5</span> Duelling network</h2>
<p>The classical DQN architecture uses a single NN to predict directly the value of all possible actions <span class="math inline">\(Q_\theta(s, a)\)</span>. The value of an action depends on two factors:</p>
<ul>
<li>the value of the underlying state <span class="math inline">\(s\)</span>: in some states, all actions are bad, you lose whatever you do.</li>
<li>the interest of that action: some actions are better than others for a given state.</li>
</ul>
<p>This leads to the definition of the <strong>advantage</strong> <span class="math inline">\(A^\pi(s,a)\)</span> of an action:</p>
<p><span id="eq:advantagefunction"><span class="math display">\[
    A^\pi(s, a) = Q^\pi(s, a) - V^\pi(s)
\qquad(5)\]</span></span></p>
<p>The advantage of the optimal action in <span class="math inline">\(s\)</span> is equal to zero: the expected return in <span class="math inline">\(s\)</span> is the same as the expected return when being in <span class="math inline">\(s\)</span> and taking <span class="math inline">\(a\)</span>, as the optimal policy will choose <span class="math inline">\(a\)</span> in <span class="math inline">\(s\)</span> anyway. The advantage of all other actions is negative: they bring less reward than the optimal action (by definition), so they are less advantageous. Note that this is only true if your estimate of <span class="math inline">\(V^\pi(s)\)</span> is correct.</p>
<p><span class="citation" data-cites="Baird1993">Baird (<a href="#ref-Baird1993" role="doc-biblioref">1993</a>)</span> has shown that it is advantageous to decompose the Q-value of an action into the value of the state and the advantage of the action (<em>advantage updating</em>):</p>
<p><span class="math display">\[
    Q^\pi(s, a) = V^\pi(s) + A^\pi(s, a)
\]</span></p>
<p>If you already know that the value of a state is very low, you do not need to bother exploring and learning the value of all actions in that state, they will not bring much. Moreover, the advantage function has <strong>less variance</strong> than the Q-values, which is a very good property when using neural networks for function approximation. The variance of the Q-values comes from the fact that they are estimated based on other estimates, which themselves evolve during learning (non-stationarity of the targets) and can drastically change during exploration (stochastic policies). The advantages only track the <em>relative</em> change of the value of an action compared to its state, what is going to be much more stable over time.</p>
<p>The range of values taken by the advantages is also much smaller than the Q-values. Let’s suppose we have two states with values -10 and 10, and two actions with advantages 0 and -1 (it does not matter which one). The Q-values will vary between -11 (the worst action in the worst state) and 10 (the best action in the best state), while the advantage only varies between -1 and 0. It is therefore going to be much easier for a neural network to learn the advantages than the Q-values, as they are theoretically not bounded.</p>
<figure>
<img src="img/duelling.png" alt="Figure 18: Duelling network architecture. Top: classical feedforward architecture to predict Q-values. Bottom: Duelling networks predicting state values and advantage functions to form the Q-values. Taken from Wang et al. (2016)." id="fig:duelling" style="width:60.0%" /><figcaption>Figure 18: Duelling network architecture. Top: classical feedforward architecture to predict Q-values. Bottom: Duelling networks predicting state values and advantage functions to form the Q-values. Taken from <span class="citation" data-cites="Wang2016">Wang et al. (<a href="#ref-Wang2016" role="doc-biblioref">2016</a>)</span>.</figcaption>
</figure>
<p><span class="citation" data-cites="Wang2016">Wang et al. (<a href="#ref-Wang2016" role="doc-biblioref">2016</a>)</span> incorporated the idea of <em>advantage updating</em> in a double DQN architecture with prioritized replay (Fig. <a href="#fig:duelling">18</a>). As in DQN, the last layer represents the Q-values of the possible actions and has to minimize the mse loss:</p>
<p><span class="math display">\[
    \mathcal{L}(\theta) = \mathbb{E}_\pi([r(s, a, s&#39;) + \gamma \, Q_{\theta&#39;, \alpha&#39;, \beta&#39;}(s&#39;, \text{argmax}_{a&#39;} Q_{\theta, \alpha, \beta} (s&#39;, a&#39;)) - Q_{\theta, \alpha, \beta}(s, a)]^2)
\]</span></p>
<p>The difference is that the previous fully-connected layer is forced to represent the value of the input state <span class="math inline">\(V_{\theta, \beta}(s)\)</span> and the advantage of each action <span class="math inline">\(A_{\theta, \alpha}(s, a)\)</span> separately. There are two separate sets of weights in the network, <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>, to predict these two values, sharing representations from the early convolutional layers through weights <span class="math inline">\(\theta\)</span>. The output layer performs simply a parameter-less summation of both sub-networks:</p>
<p><span class="math display">\[
    Q_{\theta, \alpha, \beta}(s, a) = V_{\theta, \beta}(s) + A_{\theta, \alpha}(s, a)
\]</span></p>
<p>The issue with this formulation is that one could add a constant to <span class="math inline">\(V_{\theta, \beta}(s)\)</span> and substract it from <span class="math inline">\(A_{\theta, \alpha}(s, a)\)</span> while obtaining the same result. An easy way to constrain the summation is to normalize the advantages, so that the greedy action has an advantage of zero as expected:</p>
<p><span class="math display">\[
    Q_{\theta, \alpha, \beta}(s, a) = V_{\theta, \beta}(s) + (A_{\theta, \alpha}(s, a) - \max_a A_{\theta, \alpha}(s, a))
\]</span></p>
<p>By doing this, the advantages are still free, but the state value will have to take the correct value. <span class="citation" data-cites="Wang2016">Wang et al. (<a href="#ref-Wang2016" role="doc-biblioref">2016</a>)</span> found that it is actually better to replace the <span class="math inline">\(\max\)</span> operator by the mean of the advantages. In this case, the advantages only need to change as fast as their mean, instead of having to compensate quickly for any change in the greedy action as the policy improves:</p>
<p><span class="math display">\[
    Q_{\theta, \alpha, \beta}(s, a) = V_{\theta, \beta}(s) + (A_{\theta, \alpha}(s, a) - \frac{1}{|\mathcal{A}|} \sum_a A_{\theta, \alpha}(s, a))
\]</span></p>
<p>Apart from this specific output layer, everything works as usual, especially the gradient of the mse loss function can travel backwards using backpropagation to update the weights <span class="math inline">\(\theta\)</span>, <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>. The resulting architecture outperforms double DQN with prioritized replay on most Atari games, particularly games with repetitive actions.</p>
<h2 id="sec:distributed-dqn-gorila"><span class="header-section-number">3.6</span> Distributed DQN (GORILA)</h2>
<p>The main limitation of deep RL is the slowness of learning, which is mainly influenced by two factors:</p>
<ul>
<li>the <em>sample complexity</em>, i.e. the number of transitions needed to learn a satisfying policy.</li>
<li>the online interaction with the environment.</li>
</ul>
<p>The second factor is particularly critical in real-world applications like robotics: physical robots evolve in real time, so the acquisition speed of transitions will be limited. Even in simulation (video games, robot emulators), the simulator might turn out to be much slower than training the underlying neural network. Google Deepmind proposed the GORILA (General Reinforcement Learning Architecture) framework to speed up the training of DQN networks using distributed actors and learners <span class="citation" data-cites="Nair2015">(Nair et al. <a href="#ref-Nair2015" role="doc-biblioref">2015</a>)</span>. The framework is quite general and the distribution granularity can change depending on the task.</p>
<figure>
<img src="img/gorila-global.png" alt="Figure 19: GORILA architecture. Multiple actors interact with multiple copies of the environment and store their experiences in a (distributed) experience replay memory. Multiple DQN learners sample from the ERM and compute the gradient of the loss function w.r.t the parameters \theta. A master network (parameter server, possibly distributed) gathers the gradients, apply weight updates and synchronizes regularly both the actors and the learners with new parameters. Taken from Nair et al. (2015)." id="fig:gorila" style="width:90.0%" /><figcaption>Figure 19: GORILA architecture. Multiple actors interact with multiple copies of the environment and store their experiences in a (distributed) experience replay memory. Multiple DQN learners sample from the ERM and compute the gradient of the loss function w.r.t the parameters <span class="math inline">\(\theta\)</span>. A master network (parameter server, possibly distributed) gathers the gradients, apply weight updates and synchronizes regularly both the actors and the learners with new parameters. Taken from <span class="citation" data-cites="Nair2015">Nair et al. (<a href="#ref-Nair2015" role="doc-biblioref">2015</a>)</span>.</figcaption>
</figure>
<p>In GORILA, multiple actors interact with the environment to gather transitions. Each actor has an independent copy of the environment, so they can gather <span class="math inline">\(N\)</span> times more samples per second if there are <span class="math inline">\(N\)</span> actors. This is possible in simulation (starting <span class="math inline">\(N\)</span> instances of the same game in parallel) but much more complicated for real-world systems (but see <span class="citation" data-cites="Gu2017">Gu et al. (<a href="#ref-Gu2017" role="doc-biblioref">2017</a>)</span> for an example where multiple identical robots are used to gather experiences in parallel).</p>
<p>The experienced transitions are sent as in DQN to an experience replay memory, which may be distributed or centralized. Multiple DQN learners will then sample a minibatch from the ERM and compute the DQN loss on this minibatch (also using a target network). All learners start with the same parameters <span class="math inline">\(\theta\)</span> and simply compute the gradient of the loss function <span class="math inline">\(\frac{\partial \mathcal{L}(\theta)}{\partial \theta}\)</span> on the minibatch. The gradients are sent to a parameter server (a master network) which uses the gradients to apply the optimizer (e.g. SGD) and find new values for the parameters <span class="math inline">\(\theta\)</span>. Weight updates can also be applied in a distributed manner. This distributed method to train a network using multiple learners is now quite standard in deep learning: on multiple GPU systems, each GPU has a copy of the network and computes gradients on a different minibatch, while a master network integrates these gradients and updates the slaves.</p>
<p>The parameter server regularly updates the actors (to gather samples with the new policy) and the learners (to compute gradients w.r.t the new parameter values). Such a distributed system can greatly accelerate learning, but it can be quite tricky to find the optimum number of actors and learners (too many learners might degrade the stability) or their update rate (if the learners are not updated frequently enough, the gradients might not be correct). A similar idea is at the core of the A3C algorithm (Section <a href="#sec:asynchronous-advantage-actor-critic-a3c">4.2.2</a>).</p>
<h2 id="sec:deep-recurrent-q-learning-drqn"><span class="header-section-number">3.7</span> Deep Recurrent Q-learning (DRQN)</h2>
<p>The Atari games used as a benchmark for value-based methods are <strong>partially observable MDPs</strong> (POMDP), i.e. a single frame does not contain enough information to predict what is going to happen next (e.g. the speed and direction of the ball on the screen is not known). In DQN, partial observability is solved by stacking four consecutive frames and using the resulting tensor as an input to the CNN. if this approach worked well for most Atari games, it has several limitations (as explained in <a href="https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-6-partial-observability-and-deep-recurrent-q-68463e9aeefc" class="uri">https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-6-partial-observability-and-deep-recurrent-q-68463e9aeefc</a>):</p>
<ol type="1">
<li>It increases the size of the experience replay memory, as four video frames have to be stored for each transition.</li>
<li>It solves only short-term dependencies (instantaneous speeds). If the partial observability has long-term dependencies (an object has been hidden a long time ago but now becomes useful), the input to the neural network will not have that information. This is the main explanation why the original DQN performed so poorly on games necessitating long-term planning like Montezuma’s revenge.</li>
</ol>
<p>Building on previous ideas from the Schmidhuber’s group <span class="citation" data-cites="Bakker2001 Wierstra2007">(Bakker <a href="#ref-Bakker2001" role="doc-biblioref">2001</a>; Wierstra et al. <a href="#ref-Wierstra2007" role="doc-biblioref">2007</a>)</span>, <span class="citation" data-cites="Hausknecht2015">Hausknecht and Stone (<a href="#ref-Hausknecht2015" role="doc-biblioref">2015</a>)</span> replaced one of the fully-connected layers of the DQN network by a LSTM layer (see Section <a href="#sec:recurrent-neural-networks">2.2.3</a>) while using single frames as inputs. The resulting <strong>deep recurrent q-learning</strong> (DRQN) network became able to solve POMDPs thanks to the astonishing learning abilities of LSTMs: the LSTM layer learn to remember which part of the sensory information will be useful to take decisions later.</p>
<p>However, LSTMs are not a magical solution either. They are trained using <em>truncated BPTT</em>, i.e. on a limited history of states. Long-term dependencies exceeding the truncation horizon cannot be learned. Additionally, all states in that horizon (i.e. all frames) have to be stored in the ERM to train the network, increasing drastically its size. Despite these limitations, DRQN is a much more elegant solution to the partial observability problem, letting the network decide which horizon it needs to solve long-term dependencies.</p>
<h2 id="sec:other-variants-of-dqn"><span class="header-section-number">3.8</span> Other variants of DQN</h2>
<p>Double duelling DQN with prioritized replay is currently the state-of-the-art method for value-based deep RL (see <span class="citation" data-cites="Hessel2017">Hessel et al. (<a href="#ref-Hessel2017" role="doc-biblioref">2017</a>)</span> for an experimental study of the contribution of each mechanism and the corresponding <strong>Rainbow</strong> DQN network). Several improvements have been proposed since the corresponding milestone papers. This section provides some short explanations and links to the original papers (to be organized and extended).</p>
<p><strong>Average-DQN</strong> proposes to increase the stability and performance of DQN by replacing the single target network (a copy of the trained network) by an average of the last parameter values, in other words an average of many past target networks <span class="citation" data-cites="Anschel2016">(Anschel, Baram, and Shimkin <a href="#ref-Anschel2016" role="doc-biblioref">2016</a>)</span>.</p>
<p><span class="citation" data-cites="He2016">He et al. (<a href="#ref-He2016" role="doc-biblioref">2016</a>)</span> proposed <strong>fast reward propagation</strong> through optimality tightening to speedup learning: when rewards are sparse, they require a lot of episodes to propagate these rare rewards to all actions leading to it. Their method combines immediate rewards (single steps) with actual returns (as in Monte-Carlo) via a constrained optimization approach.</p>
<!--PAGEBREAK-->
<h1 id="sec:policy-gradient-methods"><span class="header-section-number">4</span> Policy Gradient methods</h1>
<p><strong>Policy search</strong> methods directly learn to estimate the policy <span class="math inline">\(\pi_\theta\)</span> with a parameterized function estimator. The goal of the neural network is to maximize an objective function representing the <em>return</em> (sum of rewards, noted <span class="math inline">\(R(\tau)\)</span> for simplicity) of the trajectories <span class="math inline">\(\tau = (s_0, a_0, s_1, a_1, \ldots, s_T, a_T)\)</span> selected by the policy <span class="math inline">\(\pi_\theta\)</span>:</p>
<p><span class="math display">\[
    J(\theta) = \mathbb{E}_{\tau \sim \rho_\theta}[R(\tau)] = \mathbb{E}_{\tau \sim \rho_\theta}[\sum_{t=0}^T \gamma^t \, r(s_t, a_t, s_{t+1}) ]
\]</span></p>
<p>To maximize this objective function, the policy <span class="math inline">\(\pi_\theta\)</span> should only generate trajectories <span class="math inline">\(\tau\)</span> associated with high expected returns <span class="math inline">\(R(\tau)\)</span> and avoid those with low expected return, which is exactly what we want.</p>
<p>The objective function uses the mathematical expectation of the expected return over all possible trajectories. The likelihood that a trajectory is generated by the policy <span class="math inline">\(\pi_\theta\)</span> is noted <span class="math inline">\(\rho_\theta(\tau)\)</span> and given by:</p>
<p><span id="eq:likelihood_trajectory"><span class="math display">\[
    \rho_\theta(\tau) = p_\theta(s_0, a_0, \ldots, s_T, a_T) = p_0 (s_0) \, \prod_{t=0}^T \pi_\theta(s_t, a_t) p(s_{t+1} | s_t, a_t)
\qquad(6)\]</span></span></p>
<p><span class="math inline">\(p_0 (s_0)\)</span> is the initial probability of starting in <span class="math inline">\(s_0\)</span> (independent from the policy) and <span class="math inline">\(p(s_{t+1} | s_t, a_t)\)</span> is the transition probability defining the MDP. Having the probability distribution of the trajectories, we can expand the mathematical expectation in the objective function:</p>
<p><span class="math display">\[
    J(\theta) = \int_\tau \rho_\theta (\tau) \, R(\tau) \, d\tau
\]</span></p>
<p>Monte-Carlo sampling could be used to estimate the objective function. One basically would have to sample multiple trajectories <span class="math inline">\(\{\tau_i\}\)</span> and average the obtained returns:</p>
<p><span class="math display">\[
    J(\theta) \approx \frac{1}{N} \, \sum_i \rho_\theta(\tau_i) \, R(\tau_i)
\]</span></p>
<p>However, this approach would suffer from several problems:</p>
<ol type="1">
<li>The trajectory space is extremely huge, so one would need a lot of sampled trajectories to have a correct estimate of the objective function (<strong>high variance</strong>).</li>
<li>For stability reasons, only small changes can be made to the policy at each iteration, so it would necessitate a lot of episodes (<strong>sample complexity</strong>).</li>
<li>The probability of a trajectory is difficult to estimate: the initial probability distribution <span class="math inline">\(p_0\)</span> has to be known, as well as the dynamics of the MDP (<span class="math inline">\(p(s_{t+1} | s_t, a_t)\)</span>). Those could be approximated or learned (<em>model-based RL</em>), but it would limit the applicability of the method and approximation errors would accumulate quickly over a trajectory.</li>
<li>For continuing tasks (<span class="math inline">\(T = \infty\)</span>), the return can not be estimated.</li>
</ol>
<p>The policy search methods presented in this section are called <strong>policy gradient methods</strong>. As we are going to apply gradient ascent on the weights <span class="math inline">\(\theta\)</span> in order to maximize <span class="math inline">\(J(\theta)\)</span>, all we actually need is the gradient <span class="math inline">\(\nabla_\theta J(\theta)\)</span> of the objective function w.r.t the weights:</p>
<p><span class="math display">\[
    \nabla_\theta J(\theta) = \frac{\partial J(\theta)}{\partial \theta}
\]</span></p>
<p>Once a suitable estimation of this <strong>policy gradient</strong> is obtained, gradient ascent is straightforward:</p>
<p><span class="math display">\[
    \theta \leftarrow \theta + \eta \, \nabla_\theta J(\theta)
\]</span></p>
<p>The rest of this section basically presents methods allowing to estimate the policy gradient (REINFORCE, DPG) and to improve the sample complexity. See <a href="http://www.scholarpedia.org/article/Policy_gradient_methods" class="uri">http://www.scholarpedia.org/article/Policy_gradient_methods</a> for an more detailed overview of policy gradient methods, <a href="https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html" class="uri">https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html</a> and <a href="http://karpathy.github.io/2016/05/31/rl/" class="uri">http://karpathy.github.io/2016/05/31/rl/</a> for excellent tutorials from Lilian Weng and Andrej Karpathy. The article by <span class="citation" data-cites="Peters2008">Peters and Schaal (<a href="#ref-Peters2008" role="doc-biblioref">2008</a>)</span> is also a good overview of policy gradient methods.</p>
<h2 id="sec:reinforce"><span class="header-section-number">4.1</span> REINFORCE</h2>
<h3 id="sec:estimating-the-policy-gradient"><span class="header-section-number">4.1.1</span> Estimating the policy gradient</h3>
<p><span class="citation" data-cites="Williams1992">Williams (<a href="#ref-Williams1992" role="doc-biblioref">1992</a>)</span> proposed a useful estimate of the policy gradient. Considering that the expected return <span class="math inline">\(R(\tau)\)</span> of a trajectory does not depend on the parameters <span class="math inline">\(\theta\)</span>, one can simplify the policy gradient in the following way:</p>
<p><span class="math display">\[
    \nabla_\theta J(\theta) = \nabla_\theta \int_\tau \rho_\theta (\tau) \, R(\tau) \, d\tau =  \int_\tau (\nabla_\theta \rho_\theta (\tau)) \, R(\tau) \, d\tau
\]</span></p>
<p>We now use the <strong>log-trick</strong>, a simple identity based on the fact that:</p>
<p><span class="math display">\[
    \frac{d \log f(x)}{dx} = \frac{f&#39;(x)}{f(x)}
\]</span></p>
<p>to rewrite the policy gradient of a single trajectory:</p>
<p><span class="math display">\[
    \nabla_\theta \rho_\theta (\tau) = \rho_\theta (\tau) \, \nabla_\theta \log \rho_\theta (\tau)
\]</span></p>
<p>The policy gradient becomes:</p>
<p><span class="math display">\[
    \nabla_\theta J(\theta) =  \int_\tau \rho_\theta (\tau) \, \nabla_\theta \log \rho_\theta (\tau) \, R(\tau) \, d\tau
\]</span></p>
<p>which now has the form of a mathematical expectation:</p>
<p><span class="math display">\[
    \nabla_\theta J(\theta) =  \mathbb{E}_{\tau \sim \rho_\theta}[ \nabla_\theta \log \rho_\theta (\tau) \, R(\tau) ]
\]</span></p>
<p>This means that we can obtain an estimate of the policy gradient by simply sampling different trajectories <span class="math inline">\(\{\tau_i\}\)</span> and averaging <span class="math inline">\(\nabla_\theta \log \rho_\theta (\tau_i) \, R(\tau_i)\)</span> (Monte-Carlo sampling).</p>
<p>Let’s now look further at how the gradient of the log-likelihood of a trajectory <span class="math inline">\(\log \pi_\theta (\tau)\)</span> look like. Through its definition (Eq. <a href="#eq:likelihood_trajectory">6</a>), the log-likelihood of a trajectory is:</p>
<p><span id="eq:loglikelihood_trajectory"><span class="math display">\[
    \log \rho_\theta(\tau) = \log p_0 (s_0) + \sum_{t=0}^T \log \pi_\theta(s_t, a_t) + \sum_{t=0}^T \log p(s_{t+1} | s_t, a_t)
\qquad(7)\]</span></span></p>
<p><span class="math inline">\(\log p_0 (s_0)\)</span> and <span class="math inline">\(\log p(s_{t+1} | s_t, a_t)\)</span> do not depend on the parameters <span class="math inline">\(\theta\)</span> (they are defined by the MDP), so the gradient of the log-likelihood is simply:</p>
<p><span id="eq:gradloglikelihood_trajectory"><span class="math display">\[
    \nabla_\theta \log \rho_\theta(\tau) = \sum_{t=0}^T \nabla_\theta \log \pi_\theta(s_t, a_t)
\qquad(8)\]</span></span></p>
<p><span class="math inline">\(\nabla_\theta \log \pi_\theta(s_t, a_t)\)</span> is called the <strong>score function</strong>.</p>
<p>This is the main reason why policy gradient algorithms are used: the gradient is independent from the MDP dynamics, allowing <strong>model-free</strong> learning. The policy gradient is then given by:</p>
<p><span class="math display">\[
    \nabla_\theta J(\theta) =  \mathbb{E}_{\tau \sim \rho_\theta}[\sum_{t=0}^T \nabla_\theta \log \pi_\theta(s_t, a_t) \, R(\tau) ] =  \mathbb{E}_{\tau \sim \rho_\theta}[ \sum_{t=0}^T \nabla_\theta \log \pi_\theta(s_t, a_t) \, (\sum_{t=0}^T \gamma^t r_{t+1})]
\]</span></p>
<p>Estimating the policy gradient now becomes straightforward using Monte-Carlo sampling. The resulting algorithm is called the <strong>REINFORCE</strong> algorithm <span class="citation" data-cites="Williams1992">(Williams <a href="#ref-Williams1992" role="doc-biblioref">1992</a>)</span>:</p>
<hr />
<ul>
<li><p>while not converged:</p>
<ul>
<li><p>Sample <span class="math inline">\(N\)</span> trajectories <span class="math inline">\(\{\tau_i\}\)</span> using the current policy <span class="math inline">\(\pi_\theta\)</span> and observe the returns <span class="math inline">\(\{R(\tau_i)\}\)</span>.</p></li>
<li><p>Estimate the policy gradient as an average over the trajectories:</p></li>
</ul>
<p><span class="math display">\[
     \nabla_\theta J(\theta) \approx \frac{1}{N} \sum_{i=1}^N \sum_{t=0}^T \nabla_\theta \log \pi_\theta(s_t, a_t) \, R(\tau_i)
  \]</span></p>
<ul>
<li>Update the policy using gradient ascent:</li>
</ul>
<p><span class="math display">\[
      \theta \leftarrow \theta + \eta \, \nabla_\theta J(\theta)
  \]</span></p></li>
</ul>
<hr />
<p>While very simple, the REINFORCE algorithm does not work very well in practice:</p>
<ol type="1">
<li>The returns <span class="math inline">\(\{R(\tau_i)\}\)</span> have a very high variance (as the Q-values in value-based methods), which is problematic for NNs (see Section <a href="#sec:reducing-the-variance">4.1.2</a>).</li>
<li>It requires a lot of episodes to converge (sample inefficient).</li>
<li>It only works with <strong>online</strong> learning: trajectories must be frequently sampled and immediately used to update the policy.</li>
<li>The problem must be episodic (<span class="math inline">\(T\)</span> finite).</li>
</ol>
<p>However, it has two main advantages:</p>
<ol type="1">
<li>It is a <strong>model-free</strong> method, i.e. one does not need to know anything about the MDP.</li>
<li>It also works on <strong>partially observable</strong> problems (POMDP): as the return is computed over complete trajectories, it does not matter if the states are not Markovian.</li>
</ol>
<p>The methods presented in this section basically try to solve the limitations of REINFORCE (high variance, sample efficiency, online learning) to produce efficient policy gradient algorithms.</p>
<h3 id="sec:reducing-the-variance"><span class="header-section-number">4.1.2</span> Reducing the variance</h3>
<p>The main problem with the REINFORCE algorithm is the <strong>high variance</strong> of the policy gradient. This variance comes from the fact that we learn stochastic policies (it is often unlikely to generate twice the exact same trajectory) in stochastic environments (rewards are stochastic, the same action in the same state may receive). Two trajectories which are identical at the beginning will be associated with different returns depending on the stochasticity of the policy, the transition probabilities and the probabilistic rewards.</p>
<p>Consider playing a game like chess with always the same opening, and then following a random policy. You may end up winning (<span class="math inline">\(R=1\)</span>) or losing (<span class="math inline">\(R=-1\)</span>) with some probability. The initial actions of the opening will receive a policy gradient which is sometimes positive, sometimes negative: were these actions good or bad? Should they be reinforced? In supervised learning, this would mean that the same image of a cat will be randomly associated to the labels “cat” or “dog” during training: the NN will not like it.</p>
<p>In supervised learning, there is no problem of variance in the outputs, as training sets are fixed. This is in contrary very hard to ensure in deep RL and constitutes one of its main limitations. The only direct solution is to sample enough trajectories and hope that the average will be able to smooth the variance. The problem is even worse in the following conditions:</p>
<ul>
<li>High-dimensional action spaces: it becomes difficult to sample the environment densely enough if many actions are possible.</li>
<li>Long horizons: the longer the trajectory, the more likely it will be unique.</li>
<li>Finite samples: if we cannot sample enough trajectories, the high variance can introduce a bias in the gradient, leading to poor convergence.</li>
</ul>
<p>See <a href="https://medium.com/mlreview/making-sense-of-the-bias-variance-trade-off-in-deep-reinforcement-learning-79cf1e83d565" class="uri">https://medium.com/mlreview/making-sense-of-the-bias-variance-trade-off-in-deep-reinforcement-learning-79cf1e83d565</a> for a nice explanation of the bias/variance trade-off in deep RL.</p>
<p>Another related problem is that the REINFORCE gradient is sensitive to <strong>reward scaling</strong>. Let’s consider a simple MDP where only two trajectories <span class="math inline">\(\tau_1\)</span> and <span class="math inline">\(\tau_2\)</span> are possible. Depending on the choice of the reward function, the returns may be different:</p>
<ol type="1">
<li><span class="math inline">\(R(\tau_1) = 1\)</span> and <span class="math inline">\(R(\tau_2) = -1\)</span></li>
<li><span class="math inline">\(R(\tau_1) = 3\)</span> and <span class="math inline">\(R(\tau_2) = 1\)</span></li>
</ol>
<p>In both cases, the policy should select the trajectory <span class="math inline">\(\tau_1\)</span>. However, the policy gradient for <span class="math inline">\(\tau_2\)</span> will change its sign between the two cases, although the problem is the same! What we want to do is to maximize the returns, regardless the absolute value of the rewards, but the returns are unbounded. Because of the non-stationarity of the problem (the agent becomes better with training, so the returns of the sampled trajectories will increase), the policy gradients will increase over time, what is linked to the variance problem. Value-based methods addressed this problem by using <strong>target networks</strong>, but it is not a perfect solution (the gradients become biased).</p>
<p>A first simple but effective idea to solve both problems would be to subtract the mean of the sampled returns from the returns:</p>
<hr />
<ul>
<li><p>while not converged:</p>
<ul>
<li>Sample <span class="math inline">\(N\)</span> trajectories <span class="math inline">\(\{\tau_i\}\)</span> using the current policy <span class="math inline">\(\pi_\theta\)</span> and observe the returns <span class="math inline">\(\{R(\tau_i)\}\)</span>.</li>
<li>Compute the mean return: <span class="math display">\[
  \hat{R} = \frac{1}{N} \sum_{i=1}^N R(\tau_i)
  \]</span></li>
<li>Estimate the policy gradient as an average over the trajectories: <span class="math display">\[
 \nabla_\theta J(\theta) \approx \frac{1}{N} \sum_{i=1}^N \sum_{t=0}^T \nabla_\theta \log \pi_\theta(s_t, a_t) \, ( R(\tau_i) - \hat{R})
  \]</span></li>
<li>Update the policy using gradient ascent: <span class="math display">\[
  \theta \leftarrow \theta + \eta \, \nabla_\theta J(\theta)
  \]</span></li>
</ul></li>
</ul>
<hr />
<p>This obviously solves the reward scaling problem, and reduces the variance of the gradients. But are we allowed to do this (i.e. does it introduce a bias to the gradient)? <span class="citation" data-cites="Williams1992">Williams (<a href="#ref-Williams1992" role="doc-biblioref">1992</a>)</span> showed that subtracting a constant <span class="math inline">\(b\)</span> from the returns still leads to an unbiased estimate of the gradient:</p>
<p><span class="math display">\[
    \nabla_\theta J(\theta) =  \mathbb{E}_{\tau \sim \rho_\theta}[\nabla_\theta \log \rho_\theta (\tau) \, (R(\tau) -b) ]
\]</span></p>
<p>The proof is actually quite simple:</p>
<p><span class="math display">\[
    \mathbb{E}_{\tau \sim \rho_\theta}[\nabla_\theta \log \rho_\theta (\tau) \, b ] = \int_\tau \rho_\theta (\tau) \nabla_\theta \log \rho_\theta (\tau) \, b \, d\tau = \int_\tau \nabla_\theta  \rho_\theta (\tau) \, b \, d\tau = b \, \nabla_\theta \int_\tau \rho_\theta (\tau) \, d\tau =  b \, \nabla_\theta 1 = 0
\]</span></p>
<p>As long as the constant <span class="math inline">\(b\)</span> does not depend on <span class="math inline">\(\theta\)</span>, the estimator is unbiased. The resulting algorithm is called <strong>REINFORCE with baseline</strong>. <span class="citation" data-cites="Williams1992">Williams (<a href="#ref-Williams1992" role="doc-biblioref">1992</a>)</span> has actually showed that the best baseline (the one which also reduces the variance) is the mean return weighted by the square of the gradient of the log-likelihood:</p>
<p><span class="math display">\[
    b = \frac{\mathbb{E}_{\tau \sim \rho_\theta}[(\nabla_\theta \log \rho_\theta (\tau))^2 \, R(\tau)]}{\mathbb{E}_{\tau \sim \rho_\theta}[(\nabla_\theta \log \rho_\theta (\tau))^2]}
\]</span></p>
<p>but the mean reward actually work quite well. Advantage actor-critic methods (Section <a href="#sec:advantage-actor-critic-methods">4.2</a>) replace the constant <span class="math inline">\(b\)</span> with an estimate of the value of each state <span class="math inline">\(\hat{V}(s_t)\)</span>.</p>
<h3 id="sec:policy-gradient-theorem"><span class="header-section-number">4.1.3</span> Policy Gradient theorem</h3>
<p>Let’s have another look at the REINFORCE estimate of the policy gradient after sampling:</p>
<p><span class="math display">\[
   \nabla_\theta J(\theta) \approx \frac{1}{N} \sum_{i=1}^N \sum_{t=0}^T \nabla_\theta \log \pi_\theta(s_t, a_t) \, R(\tau_i) = \frac{1}{N} \sum_{i=1}^N (\sum_{t=0}^T \nabla_\theta \log \pi_\theta(s_t, a_t) ) \, (\sum_{t&#39;=0}^T \gamma^{t&#39;} \, r(s_{t&#39;}, a_{t&#39;}, s_{t&#39;+1}) )
\]</span></p>
<p>For each transition <span class="math inline">\((s_t, a_t)\)</span>, the gradient of its log-likelihood (<em>score function</em>) <span class="math inline">\(\nabla_\theta \log \pi_\theta(s_t, a_t) )\)</span> is multiplied by the return of the whole episode <span class="math inline">\(R(\tau) = \sum_{t&#39;=0}^T \gamma^{t&#39;} \, r(s_{t&#39;}, a_{t&#39;}, s_{t&#39;+1})\)</span>. However, the <strong>causality principle</strong> dictates that the reward received at <span class="math inline">\(t=0\)</span> does not depend on actions taken in the future, so we can simplify the return for each transition:</p>
<p><span class="math display">\[
 \nabla_\theta J(\theta) \approx \frac{1}{N} \sum_{i=1}^N (\sum_{t=0}^T \nabla_\theta \log \pi_\theta(s_t, a_t)  \, \sum_{t&#39;=t}^T \gamma^{t&#39;-t} \, r(s_{t&#39;}, a_{t&#39;}, s_{t&#39;+1}) )
\]</span></p>
<p>The quantity <span class="math inline">\(\hat{Q}(s_t, a_t) = \sum_{t&#39;=t}^T \gamma^{t&#39;-t} \, r(s_{t&#39;}, a_{t&#39;}, s_{t&#39;+1})\)</span> is called the <strong>reward to-go</strong> from the transition <span class="math inline">\((s_t, a_t)\)</span>, i.e. the discounted sum of future rewards after that transition. Quite obviously, the Q-value of that action is the mathematical expectation of this reward to-go.</p>
<figure>
<img src="img/rewardtogo.png" alt="Figure 20: The reward to-go is the sum of rewards gathered during a single trajectory after a transition (s, a). The Q-value of the action (s, a) is the expectation of the reward to-go. Taken from S. Levine’s lecture http://rll.berkeley.edu/deeprlcourse/." id="fig:rewardtogo" style="width:30.0%" /><figcaption>Figure 20: The reward to-go is the sum of rewards gathered during a single trajectory after a transition <span class="math inline">\((s, a)\)</span>. The Q-value of the action <span class="math inline">\((s, a)\)</span> is the expectation of the reward to-go. Taken from S. Levine’s lecture <a href="http://rll.berkeley.edu/deeprlcourse/" class="uri">http://rll.berkeley.edu/deeprlcourse/</a>.</figcaption>
</figure>
<p><span class="math display">\[
 \nabla_\theta J(\theta) \approx \frac{1}{N} \sum_{i=1}^N \sum_{t=0}^T \nabla_\theta \log \pi_\theta(s_t, a_t) \, \hat{Q}(s_t, a_t)
\]</span></p>
<p><span class="citation" data-cites="Sutton1999">Sutton et al. (<a href="#ref-Sutton1999" role="doc-biblioref">1999</a>)</span> showed that the policy gradient can be estimated by replacing the return of the sampled trajectory with the Q-value of each action, what leads to the <strong>policy gradient theorem</strong> (Eq. <a href="#eq:policygradienttheorem">9</a>):</p>
<p><span id="eq:policygradienttheorem"><span class="math display">\[
    \nabla_\theta J(\theta) =  \mathbb{E}_{s \sim \rho_\theta, a \sim \pi_\theta}[\nabla_\theta \log \pi_\theta(s, a) \, Q^{\pi_\theta}(s, a)]
\qquad(9)\]</span></span></p>
<p>where <span class="math inline">\(\rho_\theta\)</span> is the distribution of states reachable under the policy <span class="math inline">\(\pi_\theta\)</span>. Because the actual return <span class="math inline">\(R(\tau)\)</span> is replaced by its expectation <span class="math inline">\(Q^{\pi_\theta}(s, a)\)</span>, the policy gradient is now a mathematical expectation over <strong>single transitions</strong> instead of complete trajectories, allowing <strong>bootstrapping</strong> as in temporal difference methods (Section <a href="#sec:temporal-difference">2.1.5</a>).</p>
<p>One clearly sees that REINFORCE is actually a special case of the policy gradient theorem, where the Q-value of an action replaces the return obtained during the corresponding trajectory.</p>
<p>The problem is of course that the true Q-value of the actions is as unknown as the policy. However, <span class="citation" data-cites="Sutton1999">Sutton et al. (<a href="#ref-Sutton1999" role="doc-biblioref">1999</a>)</span> showed that it is possible to estimate the Q-values with a function approximator <span class="math inline">\(Q_\varphi(s, a)\)</span> with parameters <span class="math inline">\(\varphi\)</span> and obtain an unbiased estimation:</p>
<p><span id="eq:policygradienttheoremapprox"><span class="math display">\[
    \nabla_\theta J(\theta) =  \mathbb{E}_{s \sim \rho_\theta, a \sim \pi_\theta}[\nabla_\theta \log \pi_\theta(s, a) \, Q_\varphi(s, a))]
\qquad(10)\]</span></span></p>
<p>Formally, the Q-value approximator must respect the Compatible Function Approximation Theorem, which states that the value approximator must be compatible with the policy (<span class="math inline">\(\nabla_\varphi Q_\varphi(s, a) = \nabla_\theta \log \pi_\theta(s, a)\)</span>) and minimize the mean-square error with the true Q-values <span class="math inline">\(\mathbb{E}_{s \sim \rho^\pi, a \sim \pi_\theta} [(Q^{\pi_\theta}(s, a) - Q_\varphi(s, a))^2]\)</span>. In the algorithms presented in this section, these conditions are either met or neglected.</p>
<p>The resulting algorithm belongs to the <strong>actor-critic</strong> class, in the sense that:</p>
<ul>
<li>The <strong>actor</strong> <span class="math inline">\(\pi_\theta(s, a)\)</span> learns to approximate the policy by maximizing Eq. <a href="#eq:policygradienttheoremapprox">10</a>.</li>
<li>The <strong>critic</strong> <span class="math inline">\(Q_\varphi(s, a)\)</span> learns to estimate the policy by minimizing the mse with the true Q-values.</li>
</ul>
<p>Fig. <a href="#fig:actorcriticpolicy">21</a> shows the architecture of the algorithm. The only problem left is to provide the critic with the true Q-values.</p>
<figure>
<img src="img/policygradient.png" alt="Figure 21: Architecture of the policy gradient (PG) method." id="fig:actorcriticpolicy" style="width:80.0%" /><figcaption>Figure 21: Architecture of the policy gradient (PG) method.</figcaption>
</figure>
<p>Most policy-gradient algorithms (A3C, DPPG, TRPO) are actor-critic architectures. Some remarks already:</p>
<ul>
<li>Trajectories now appear only implicitly in the policy gradient, one can even sample single transitions. It should therefore be possible (with modifications) to do <strong>off-policy learning</strong>, for example with using importance sampling (Section <a href="#sec:importance-sampling">4.3.1</a>) or a replay buffer of stored transitions as in DQN (see ACER Section <a href="#sec:actor-critic-with-experience-replay-acer">4.5.5</a>). REINFORCE works strictly on-policy.</li>
<li>The policy gradient theorem suffers from the same <strong>high variance</strong> problem as REINFORCE. The different algorithms presented later are principally attempts to solve this problem and reduce the sample complexity: advantages, deterministic policies, natural gradients…</li>
<li>The actor and the critic can be completely separated, or share some parameters.</li>
</ul>
<!--PAGEBREAK-->
<h2 id="sec:advantage-actor-critic-methods"><span class="header-section-number">4.2</span> Advantage Actor-Critic methods</h2>
<p>The <em>policy gradient theorem</em> provides an actor-critic architecture able to learn parameterized policies. In comparison to REINFORCE, the policy gradient depends on the Q-values of the actions taken during the trajectory rather than on the obtained returns <span class="math inline">\(R(\tau)\)</span>. Quite obviously, it will also suffer from the high variance of the gradient (Section <a href="#sec:reducing-the-variance">4.1.2</a>), requiring the use of baselines. In this section, the baseline is state-dependent and chosen equal to the value of the state <span class="math inline">\(V^\pi(s)\)</span>, so the factor multiplying the log-likelihood of the policy is:</p>
<p><span class="math display">\[
    A^{\pi}(s, a) = Q^{\pi}(s, a) - V^\pi(s)
\]</span></p>
<p>which is the <strong>advantage</strong> of the action <span class="math inline">\(a\)</span> in <span class="math inline">\(s\)</span>, as already seen in <em>duelling networks</em> (Section <a href="#sec:duelling-network">3.5</a>).</p>
<p>Now the problem is that the critic would have to approximate two functions: <span class="math inline">\(Q^{\pi}(s, a)\)</span> and <span class="math inline">\(V^{\pi}(s)\)</span>. <strong>Advantage actor-critic</strong> methods presented in this section (A2C, A3C, GAE) approximate the advantage of an action:</p>
<p><span class="math display">\[
    \nabla_\theta J(\theta) =  \mathbb{E}_{s \sim \rho^\pi, a \sim \pi_\theta}[\nabla_\theta \log \pi_\theta(s, a) \, A_\varphi(s, a)]
\]</span></p>
<p><span class="math inline">\(A_\varphi(s, a)\)</span> is called the <strong>advantage estimate</strong> and should be equal to the real advantage <em>in expectation</em>.</p>
<p>Different methods could be used to compute the advantage estimate:</p>
<ul>
<li><p><span class="math inline">\(A_\varphi(s, a) = R(s, a) - V_\varphi(s)\)</span> is the <strong>MC advantage estimate</strong>, the Q-value of the action being replaced by the actual return.</p></li>
<li><p><span class="math inline">\(A_\varphi(s, a) = r(s, a, s&#39;) + \gamma \, V_\varphi(s&#39;) - V_\varphi(s)\)</span> is the <strong>TD advantage estimate</strong> or TD error.</p></li>
<li><p><span class="math inline">\(A_\varphi(s, a) = \sum_{k=0}^{n-1} \gamma^{k} \, r_{t+k+1} + \gamma^n \, V_\varphi(s_{t+n+1}) - V_\varphi(s_t)\)</span> is the <strong>n-step advantage estimate</strong>.</p></li>
</ul>
<p>The most popular approach is the n-step advantage, which is at the core of the methods A2C and A3C, and can be understood as a trade-off between MC and TD. MC and TD advantages could be used as well, but come with the respective disadvantages of MC (need for finite episodes, slow updates) and TD (unstable). Generalized Advantage Estimation (GAE, Section <a href="#sec:generalized-advantage-estimation-gae">4.2.3</a>) takes another interesting approach to estimate the advantage.</p>
<p><em>Note:</em> A2C is actually derived from the A3C algorithm presented later, but it is simpler to explain it first. See <a href="https://blog.openai.com/baselines-acktr-a2c/" class="uri">https://blog.openai.com/baselines-acktr-a2c/</a> for an explanation of the reasons. A good explanation of A2C and A3C with Python code is available at <a href="https://cgnicholls.github.io/reinforcement-learning/2017/03/27/a3c.html" class="uri">https://cgnicholls.github.io/reinforcement-learning/2017/03/27/a3c.html</a>.</p>
<h3 id="sec:advantage-actor-critic-a2c"><span class="header-section-number">4.2.1</span> Advantage Actor-Critic (A2C)</h3>
<p>The first aspect of A2C is that it relies on n-step updating, which is a trade-off between MC and TD:</p>
<ul>
<li>MC waits until the end of an episode to update the value of an action using the reward to-go (sum of obtained rewards) <span class="math inline">\(R(s, a)\)</span>.</li>
<li>TD updates immediately the action using the immediate reward <span class="math inline">\(r(s, a, s&#39;)\)</span> and approximates the rest with the value of the next state <span class="math inline">\(V^\pi(s)\)</span>.</li>
<li>n-step uses the <span class="math inline">\(n\)</span> next immediate rewards and approximates the rest with the value of the state visited <span class="math inline">\(n\)</span> steps later.</li>
</ul>
<p><span id="eq:a2c"><span class="math display">\[
    \nabla_\theta J(\theta) =  \mathbb{E}_{s_t \sim \rho^\pi, a_t \sim \pi_\theta}[\nabla_\theta \log \pi_\theta(s_t, a_t) \, ( \sum_{k=0}^{n-1} \gamma^{k} \, r_{t+k+1} + \gamma^n \, V_\varphi(s_{t+n+1}) - V_\varphi(s_t))]
\qquad(11)\]</span></span></p>
<p>TD can be therefore be seen as a 1-step algorithm. For sparse rewards (mostly zero, +1 or -1 at the end of a game for example), this allows to update the <span class="math inline">\(n\)</span> last actions which lead to a win/loss, instead of only the last one in TD, speeding up learning. However, there is no need for finite episodes as in MC. In other words, n-step estimation ensures a trade-off between bias (wrong updates based on estimated values as in TD) and variance (variability of the obtained returns as in MC). An alternative to n-step updating is the use of <em>eligibility traces</em> (see Section <a href="#sec:eligibility-traces">2.1.6</a>, <span class="citation" data-cites="Sutton1998">Sutton and Barto (<a href="#ref-Sutton1998" role="doc-biblioref">1998</a>)</span>).</p>
<p>A2C has an actor-critic architecture (Fig. <a href="#fig:a3c">22</a>):</p>
<ul>
<li>The actor outputs the policy <span class="math inline">\(\pi_\theta\)</span> for a state <span class="math inline">\(s\)</span>, i.e. a vector of probabilities for each action.</li>
<li>The critic outputs the value <span class="math inline">\(V_\varphi(s)\)</span> of a state <span class="math inline">\(s\)</span>.</li>
</ul>
<figure>
<img src="img/a3c.png" alt="Figure 22: Advantage actor-critic architecture." id="fig:a3c" style="width:70.0%" /><figcaption>Figure 22: Advantage actor-critic architecture.</figcaption>
</figure>
<p>Having a computable formula for the policy gradient, the algorithm is rather simple:</p>
<ol type="1">
<li><p>Acquire a batch of transitions <span class="math inline">\((s, a, r, s&#39;)\)</span> using the current policy <span class="math inline">\(\pi_\theta\)</span> (either a finite episode or a truncated one).</p></li>
<li><p>For each state encountered, compute the discounted sum of the next <span class="math inline">\(n\)</span> rewards <span class="math inline">\(\sum_{k=0}^{n} \gamma^{k} \, r_{t+k+1}\)</span> and use the critic to estimate the value of the state encountered <span class="math inline">\(n\)</span> steps later <span class="math inline">\(V_\varphi(s_{t+n+1})\)</span>.</p></li>
</ol>
<p><span class="math display">\[
    R_t = \sum_{k=0}^{n-1} \gamma^{k} \, r_{t+k+1} + \gamma^n \, V_\varphi(s_{t+n+1})
\]</span></p>
<ol start="3" type="1">
<li>Update the actor using Eq. <a href="#eq:a2c">11</a>.</li>
</ol>
<p><span class="math display">\[
    \nabla_\theta J(\theta) =  \sum_t \nabla_\theta \log \pi_\theta(s_t, a_t) \, (R_t - V_\varphi(s_t))
\]</span></p>
<ol start="4" type="1">
<li>Update the critic to minimize the TD error between the estimated value of a state and its true value.</li>
</ol>
<p><span class="math display">\[
    \mathcal{L}(\varphi) = \sum_t (R_t - V_\varphi(s_t))^2
\]</span></p>
<ol start="5" type="1">
<li>Repeat.</li>
</ol>
<p>This is not very different in essence from REINFORCE (sample transitions, compute the return, update the policy), apart from the facts that episodes do not need to be finite and that a critic has to be learned in parallel. A more detailed pseudo-algorithm for a single A2C learner is the following:</p>
<hr />
<ul>
<li>Initialize the actor <span class="math inline">\(\pi_\theta\)</span> and the critic <span class="math inline">\(V_\varphi\)</span> with random weights.</li>
<li>Observe the initial state <span class="math inline">\(s_0\)</span>.</li>
<li>for <span class="math inline">\(t \in [0, T_\text{total}]\)</span>:
<ul>
<li><p>Initialize empty episode minibatch.</p></li>
<li><p>for <span class="math inline">\(k \in [0, n]\)</span>: # Sample episode</p>
<ul>
<li>Select a action <span class="math inline">\(a_k\)</span> using the actor <span class="math inline">\(\pi_\theta\)</span>.</li>
<li>Perform the action <span class="math inline">\(a_k\)</span> and observe the next state <span class="math inline">\(s_{k+1}\)</span> and the reward <span class="math inline">\(r_{k+1}\)</span>.</li>
<li>Store <span class="math inline">\((s_k, a_k, r_{k+1})\)</span> in the episode minibatch.</li>
</ul></li>
<li><p>if <span class="math inline">\(s_n\)</span> is not terminal: set <span class="math inline">\(R = V_\varphi(s_n)\)</span> with the critic, else <span class="math inline">\(R=0\)</span>.</p></li>
<li><p>Reset gradient <span class="math inline">\(d\theta\)</span> and <span class="math inline">\(d\varphi\)</span> to 0.</p></li>
<li><p>for <span class="math inline">\(k \in [n-1, 0]\)</span>: # Backwards iteration over the episode</p>
<ul>
<li>Update the discounted sum of rewards <span class="math inline">\(R = r_k + \gamma \, R\)</span></li>
<li>Accumulate the policy gradient using the critic: <span class="math display">\[
  d\theta \leftarrow d\theta + \nabla_\theta \log \pi_\theta(s_k, a_k) \, (R - V_\varphi(s_k))
  \]</span></li>
<li>Accumulate the critic gradient: <span class="math display">\[
  d\varphi \leftarrow d\varphi + \nabla_\varphi (R - V_\varphi(s_k))^2
  \]</span></li>
</ul></li>
<li><p>Update the actor and the critic with the accumulated gradients using gradient descent or similar: <span class="math display">\[
  \theta \leftarrow \theta + \eta \, d\theta \qquad \varphi \leftarrow \varphi + \eta \, d\varphi
  \]</span></p></li>
</ul></li>
</ul>
<hr />
<p>Note that not all states are updated with the same horizon <span class="math inline">\(n\)</span>: the last action encountered in the sampled episode will only use the last reward and the value of the final state (TD learning), while the very first action will use the <span class="math inline">\(n\)</span> accumulated rewards. In practice it does not really matter, but the choice of the discount rate <span class="math inline">\(\gamma\)</span> will have a significant influence on the results.</p>
<p>As many actor-critic methods, A2C performs online learning: a couple of transitions are explored using the current policy, which is immediately updated. As for value-based networks (e.g. DQN, Section <a href="#sec:deep-q-network-dqn">3.2</a>), the underlying NN will be affected by the correlated inputs and outputs: a single batch contains similar states and action (e.g. consecutive frames of a video game). The solution retained in A2C and A3C does not depend on an <em>experience replay memory</em> as DQN, but rather on the use of <strong>multiple parallel actors and learners</strong>.</p>
<p>The idea is depicted on Fig. <a href="#fig:a3carchi">23</a> (actually for A3C, but works with A2C). The actor and critic are stored in a global network. Multiple instances of the environment are created in different parallel threads (the <strong>workers</strong> or <strong>actor-learners</strong>). At the beginning of an episode, each worker receives a copy of the actor and critic weights from the global network. Each worker samples an episode (starting from different initial states, so the episodes are uncorrelated), computes the accumulated gradients and sends them back to the global network. The global networks merges the gradients and uses them to update the parameters of the policy and critic networks. The new parameters are send to each worker again, until it converges.</p>
<hr />
<ul>
<li>Initialize the actor <span class="math inline">\(\pi_\theta\)</span> and the critic <span class="math inline">\(V_\varphi\)</span> in the global network.</li>
<li>repeat:
<ul>
<li>for each worker <span class="math inline">\(i\)</span> in <strong>parallel</strong>:
<ul>
<li>Get a copy of the global actor <span class="math inline">\(\pi_\theta\)</span> and critic <span class="math inline">\(V_\varphi\)</span>.</li>
<li>Sample an episode of <span class="math inline">\(n\)</span> steps.</li>
<li>Return the accumulated gradients <span class="math inline">\(d\theta_i\)</span> and <span class="math inline">\(d\varphi_i\)</span>.</li>
</ul></li>
<li>Wait for all workers to terminate.</li>
<li>Merge all accumulated gradients into <span class="math inline">\(d\theta\)</span> and <span class="math inline">\(d\varphi\)</span>.</li>
<li>Update the global actor and critic networks.</li>
</ul></li>
</ul>
<hr />
<p>This solves the problem of correlated inputs and outputs, as each worker explores different regions of the environment (one can set different initial states in each worker, vary the exploration rate, etc), so the final batch of transitions used for training the global networks is much less correlated. The only drawback of this approach is that it has to be possible to explore multiple environments in parallel. This is easy to achieve in simulated environments (e.g. video games) but much harder in real-world systems like robots. A brute-force solution for robotics is simply to buy enough robots and let them learn in parallel <span class="citation" data-cites="Gu2017">(Gu et al. <a href="#ref-Gu2017" role="doc-biblioref">2017</a>)</span>.</p>
<h3 id="sec:asynchronous-advantage-actor-critic-a3c"><span class="header-section-number">4.2.2</span> Asynchronous Advantage Actor-Critic (A3C)</h3>
<figure>
<img src="img/A3C_architecture.png" alt="Figure 23: Architecture of A3C. A master network interacts asynchronously with several workers, each having a copy of the network and interacting with a separate environment. At the end of an episode, the accumulated gradients are sent back to the master network, and a new value of the parameters is sent to the workers. Taken from https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2." id="fig:a3carchi" style="width:70.0%" /><figcaption>Figure 23: Architecture of A3C. A master network interacts asynchronously with several workers, each having a copy of the network and interacting with a separate environment. At the end of an episode, the accumulated gradients are sent back to the master network, and a new value of the parameters is sent to the workers. Taken from <a href="https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2" class="uri">https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2</a>.</figcaption>
</figure>
<p>Asynchronous Advantage Actor-Critic <span class="citation" data-cites="Mnih2016">(A3C, Mnih et al. <a href="#ref-Mnih2016" role="doc-biblioref">2016</a>)</span> extends the approach of A2C by removing the need of synchronization between the workers at the end of each episode before applying the gradients. The rationale behind this is that each worker may need different times to complete its task, so they need to be synchronized. Some workers might then be idle most of the time, what is a waste of resources. Gradient merging and parameter updates are sequential operations, so no significant speedup is to be expected even if one increases the number of workers.</p>
<p>The solution retained in A3C is to simply skip the synchronization step: each worker reads and writes the network parameters whenever it wants. Without synchronization barriers, there is of course a risk that one worker tries to read the network parameters while another writes them: the obtained parameters would be a mix of two different networks. Surprisingly, it does not matter: if the learning rate is small enough, there is anyway not a big difference between two successive versions of the network parameters. This kind of “dirty” parameter sharing is called <em>HogWild!</em> updating <span class="citation" data-cites="Niu2011">(Niu et al. <a href="#ref-Niu2011" role="doc-biblioref">2011</a>)</span> and has been proven to work under certain conditions which are met here.</p>
<p>The resulting A3C pseudocode is summarized here:</p>
<hr />
<ul>
<li><p>Initialize the actor <span class="math inline">\(\pi_\theta\)</span> and the critic <span class="math inline">\(V_\varphi\)</span> in the global network.</p></li>
<li><p>for each worker <span class="math inline">\(i\)</span> in <strong>parallel</strong>:</p>
<ul>
<li>repeat:
<ul>
<li>Get a copy of the global actor <span class="math inline">\(\pi_\theta\)</span> and critic <span class="math inline">\(V_\varphi\)</span>.</li>
<li>Sample an episode of <span class="math inline">\(n\)</span> steps.</li>
<li>Compute the accumulated gradients <span class="math inline">\(d\theta_i\)</span> and <span class="math inline">\(d\varphi_i\)</span>.</li>
<li>Update the global actor and critic networks asynchronously (HogWild!).</li>
</ul></li>
</ul></li>
</ul>
<hr />
<p>The workers are fully independent: their only communication is through the <strong>asynchronous</strong> updating of the global networks. This can lead to very efficient parallel implementations: in the original A3C paper <span class="citation" data-cites="Mnih2016">(Mnih et al. <a href="#ref-Mnih2016" role="doc-biblioref">2016</a>)</span>, they solved the same Atari games than DQN using 16 CPU cores instead of a powerful GPU, while achieving a better performance in less training time (1 day instead of 8). The speedup is almost linear: the more workers, the faster the computations, the better the performance (as the policy updates are less correlated).</p>
<h4 id="sec:entropy-regularization" class="unnumbered">Entropy regularization</h4>
<p>An interesting addition in A3C is the way they enforce exploration during learning. In actor-critic methods, exploration classically relies on the fact that the learned policies are stochastic (<strong>on-policy</strong>): <span class="math inline">\(\pi(s, a)\)</span> describes the probability of taking the action <span class="math inline">\(a\)</span> in the state <span class="math inline">\(s\)</span>. In discrete action spaces, the output of the actor is usually a softmax layer, ensuring that all actions get a non-zero probability of being selected during training. In continuous action spaces, the executed action is sampled from the output probability distribution. However, this is often not sufficient and hard to control.</p>
<p>In A3C, the authors added an <strong>entropy regularization</strong> term <span class="citation" data-cites="Williams1991">(Williams and Peng <a href="#ref-Williams1991" role="doc-biblioref">1991</a>)</span> to the policy gradient update:</p>
<p><span id="eq:a3c_entropy"><span class="math display">\[
    \nabla_\theta J(\theta) =  \mathbb{E}_{s_t \sim \rho^\pi, a_t \sim \pi_\theta}[\nabla_\theta \log \pi_\theta(s_t, a_t) \, ( R_t - V_\varphi(s_t)) + \beta \, \nabla_\theta H(\pi_\theta(s_t))]
\qquad(12)\]</span></span></p>
<p>For discrete actions, the entropy of the policy for a state <span class="math inline">\(s_t\)</span> is simple to compute: <span class="math inline">\(H(\pi_\theta(s_t)) = - \sum_a \pi_\theta(s_t, a) \, \log \pi_\theta(s_t, a)\)</span>. For continuous actions, replace the sum with an integral. It measures the “randomness” of the policy: if the policy is fully deterministic (the same action is systematically selected), the entropy is zero as it carries no information. If the policy is completely random, the entropy is maximal. Maximizing the entropy at the same time as the returns improves exploration by forcing the policy to be as non-deterministic as possible.</p>
<p>The parameter <span class="math inline">\(\beta\)</span> controls the level of regularization: we do not want the entropy to dominate either, as a purely random policy does not bring much reward. If <span class="math inline">\(\beta\)</span> is chosen too low, the entropy won’t play a significant role in the optimization and we may obtain a suboptimal deterministic policy early during training as there was not enough exploration. If <span class="math inline">\(\beta\)</span> is too high, the policy will be random. Entropy regularization adds yet another hyperparameter to the problem, but can be really useful for convergence when adequately chosen.</p>
<h4 id="sec:comparison-between-a3c-and-dqn" class="unnumbered">Comparison between A3C and DQN</h4>
<ol type="1">
<li>DQN uses an experience replay memory to solve the correlation of inputs/outputs problem, while A3C uses parallel actor-learners. If multiple copies of the environment are available, A3C should be preferred because the ERM slows down learning (very old transitions are still used for learning) and requires a lot of memory.</li>
<li>A3C is on-policy: the learned policy must be used to explore the environment. DQN is off-policy: a behavior policy can be used for exploration, allowing to guide externally which regions of the state-action space should be explored. Off-policy are often preferred when expert knowledge is available.</li>
<li>DQN has to use target networks to fight the non-stationarity of the Q-values. A3C uses state-values and advantages, which are much more stable over time than Q-values, so there is no need for target networks.</li>
<li>A3C can deal with continuous action spaces easily, as it uses a parameterized policy. DQN has to be strongly modified to deal with this problem.</li>
<li>Both can deal with POMDP by using LSTMs in the actor network: A3C <span class="citation" data-cites="Mnih2016 Mirowski2016">(Mnih et al. <a href="#ref-Mnih2016" role="doc-biblioref">2016</a>; Mirowski et al. <a href="#ref-Mirowski2016" role="doc-biblioref">2016</a>)</span>, DQN <span class="citation" data-cites="Hausknecht2015">(Hausknecht and Stone <a href="#ref-Hausknecht2015" role="doc-biblioref">2015</a>, see Section <a href="#sec:deep-recurrent-q-learning-drqn" role="doc-biblioref">3.7</a>)</span>.</li>
</ol>
<h3 id="sec:generalized-advantage-estimation-gae"><span class="header-section-number">4.2.3</span> Generalized Advantage Estimation (GAE)</h3>
<p>The different versions of the policy gradient seen so far take the form:</p>
<p><span class="math display">\[
    \nabla_\theta J(\theta) =  \mathbb{E}_{s_t \sim \rho^\pi, a_t \sim \pi_\theta}[\nabla_\theta \log \pi_\theta (s_t, a_t) \, \psi_t ]
\]</span></p>
<p>where:</p>
<ul>
<li><p><span class="math inline">\(\psi_t = R_t\)</span> is the <em>REINFORCE</em> algorithm (MC sampling).</p></li>
<li><p><span class="math inline">\(\psi_t = R_t - b\)</span> is the <em>REINFORCE with baseline</em> algorithm.</p></li>
<li><p><span class="math inline">\(\psi_t = Q^\pi(s_t, a_t)\)</span> is the <em>policy gradient theorem</em>.</p></li>
<li><p><span class="math inline">\(\psi_t = A^\pi(s_t, a_t)\)</span> is the <em>advantage actor critic</em>.</p></li>
<li><p><span class="math inline">\(\psi_t = r_{t+1} + \gamma \, V^\pi(s_{t+1}) - V^\pi(s_t)\)</span> is the <em>TD actor critic</em>.</p></li>
<li><p><span class="math inline">\(\psi_t = \sum_{k=0}^{n-1} \gamma^{k} \, r_{t+k+1} + \gamma^n \, V^\pi(s_{t+n+1}) - V^\pi(s_t)\)</span> is the <em>n-step algorithm</em> (A2C).</p></li>
</ul>
<p>Generally speaking:</p>
<ul>
<li>the more <span class="math inline">\(\psi_t\)</span> relies on real rewards (e.g. <span class="math inline">\(R_t\)</span>), the more the gradient will be correct on average (small bias), but the more it will vary (high variance). This increases the sample complexity: we need to average more samples to correctly estimate the gradient.</li>
<li>the more <span class="math inline">\(\psi_t\)</span> relies on estimations (e.g. the TD error), the more stable the gradient (small variance), but the more incorrect it is (high bias). This can lead to suboptimal policies, i.e. local optima of the objective function.</li>
</ul>
<p>This is the classical bias/variance trade-off in machine learning (see Section <a href="#sec:reducing-the-variance">4.1.2</a>). The n-step algorithm used in A2C is an attempt to mitigate between these extrema. <span class="citation" data-cites="Schulman2015a">Schulman, Moritz, et al. (<a href="#ref-Schulman2015a" role="doc-biblioref">2015</a>)</span> proposed the <strong>Generalized Advantage Estimate</strong> (GAE) to further control the bias/variance trade-off.</p>
<p>Let’s define the n-step advantage:</p>
<p><span class="math display">\[
    A^{n}_t = \sum_{k=0}^{n-1} \gamma^{k} \, r_{t+k+1} + \gamma^n \, V^\pi(s_{t+n+1}) - V^\pi(s_t)
\]</span></p>
<p>It is easy to show recursively that it depends on the TD error <span class="math inline">\(\delta_t = r_{t+1} + \gamma \, V^\pi(s_{t+1}) - V^\pi(s_t)\)</span> of the <span class="math inline">\(n\)</span> next steps:</p>
<p><span class="math display">\[
    A^{n}_t = \sum_{l=0}^{n-1} \gamma^l \, \delta_{t+l}
\]</span></p>
<p>In other words, the prediction error over <span class="math inline">\(n\)</span> steps is the (discounted) sum of the prediction errors between two successive steps. Now, what is the optimal value of <span class="math inline">\(n\)</span>? GAE decides not to choose and to simply average all n-step advantages and to weight them with a discount parameter <span class="math inline">\(\lambda\)</span>. This defines the <strong>Generalized Advantage Estimator</strong> <span class="math inline">\(A^{\text{GAE}(\gamma, \lambda)}_t\)</span>:</p>
<p><span class="math display">\[
    A^{\text{GAE}(\gamma, \lambda)}_t = (1-\lambda) \, \sum_{l=0}^\infty \lambda^l A^l_t = \sum_{l=0}^\infty (\gamma \lambda)^l \delta_{t+l}
\]</span></p>
<p>The GAE is the discounted sum of all n-step advantages. When <span class="math inline">\(\lambda=0\)</span>, we have <span class="math inline">\(A^{\text{GAE}(\gamma, 0)}_t = A^{0}_t = \delta_t\)</span>, i.e. the TD advantage (high bias, low variance). When <span class="math inline">\(\lambda=1\)</span>, we have (at the limit) <span class="math inline">\(A^{\text{GAE}(\gamma, 1)}_t = R_t\)</span>, i.e. the MC advantage (low bias, high variance). Choosing the right value of <span class="math inline">\(\lambda\)</span> between 0 and 1 allows to control the bias/variance trade-off.</p>
<p><span class="math inline">\(\gamma\)</span> and <span class="math inline">\(\lambda\)</span> play different roles in GAE: <span class="math inline">\(\gamma\)</span> determines the scale or horizon of the value functions: how much future rewards rewards are to be taken into account. The higher <span class="math inline">\(\gamma &lt;1\)</span>, the smaller the bias, but the higher the variance. Empirically, <span class="citation" data-cites="Schulman2015a">Schulman, Moritz, et al. (<a href="#ref-Schulman2015a" role="doc-biblioref">2015</a>)</span> found that small <span class="math inline">\(\lambda\)</span> values introduce less bias than <span class="math inline">\(\gamma\)</span>, so <span class="math inline">\(\lambda\)</span> can be chosen smaller than <span class="math inline">\(\gamma\)</span> (which is typically 0.99).</p>
<p>The policy gradient for Generalized Advantage Estimation is therefore:</p>
<p><span class="math display">\[
    \nabla_\theta J(\theta) =  \mathbb{E}_{s_t \sim \rho^\pi, a_t \sim \pi_\theta}[\nabla_\theta \log \pi_\theta (s_t, a_t) \, \sum_{l=0}^\infty (\gamma \lambda)^l \delta_{t+l} ]
\]</span></p>
<p>Note that <span class="citation" data-cites="Schulman2015a">Schulman, Moritz, et al. (<a href="#ref-Schulman2015a" role="doc-biblioref">2015</a>)</span> additionally use <em>trust region optimization</em> to stabilize learning and further reduce the bias (see Section <a href="#sec:trust-region-policy-optimization-trpo">4.5.3</a>), for now just consider it is a better optimization method than gradient descent. The GAE algorithm is summarized here:</p>
<hr />
<ul>
<li>Initialize the actor <span class="math inline">\(\pi_\theta\)</span> and the critic <span class="math inline">\(V_\varphi\)</span> with random weights.</li>
<li>for <span class="math inline">\(t \in [0, T_\text{total}]\)</span>:
<ul>
<li><p>Initialize empty minibatch.</p></li>
<li><p>for <span class="math inline">\(k \in [0, n]\)</span>:</p>
<ul>
<li>Select a action <span class="math inline">\(a_k\)</span> using the actor <span class="math inline">\(\pi_\theta\)</span>.</li>
<li>Perform the action <span class="math inline">\(a_k\)</span> and observe the next state <span class="math inline">\(s_{k+1}\)</span> and the reward <span class="math inline">\(r_{k+1}\)</span>.</li>
<li>Store <span class="math inline">\((s_k, a_k, r_{k+1})\)</span> in the minibatch.</li>
</ul></li>
<li><p>for <span class="math inline">\(k \in [0, n]\)</span>:</p>
<ul>
<li>Compute the TD error <span class="math inline">\(\delta_k = r_{k+1} + \gamma \, V_\varphi(s_{k+1}) - V_\varphi(s_k)\)</span></li>
</ul></li>
<li><p>for <span class="math inline">\(k \in [0, n]\)</span>:</p>
<ul>
<li>Compute the GAE advantage <span class="math inline">\(A^{\text{GAE}(\gamma, \lambda)}_k = \sum_{l=0}^\infty (\gamma \lambda)^l \delta_{k+l}\)</span></li>
</ul></li>
<li><p>Update the actor using the GAE advantage and natural gradients (TRPO).</p></li>
<li><p>Update the critic using natural gradients (TRPO)</p></li>
</ul></li>
</ul>
<hr />
<h3 id="sec:stochastic-actor-critic-for-continuous-action-spaces"><span class="header-section-number">4.2.4</span> Stochastic actor-critic for continuous action spaces</h3>
<p>The actor-critic method presented above use <strong>stochastic policies</strong> <span class="math inline">\(\pi_\theta(s, a)\)</span> assigning parameterized probabilities of being selecting to each <span class="math inline">\((s, a)\)</span> pair.</p>
<ul>
<li><p>When the action space is discrete, the output layer of the actor is simply a <em>softmax</em> layer with as many neurons as possible actions in each state, making sure the probabilities sum to one. It is then straightforward to sample an action from this layer.</p></li>
<li><p>When the action space is continuous (e.g. the different joints of a robotic arm), one has to make an assumption on the underlying distribution. The actor learns the parameters of the distribution (for example the mean and variance of a Gaussian distribution) and the executed action is simply sampled from the parameterized distribution.</p></li>
</ul>
<p>The most used distribution is the Gaussian distribution, leading to <strong>Gaussian policies</strong>. In this case, the output of the actor is a mean vector <span class="math inline">\(\mu_\theta(s)\)</span> and possibly a variance vector <span class="math inline">\(\sigma_\theta(s)\)</span>. The policy is then simply defined as:</p>
<p><span class="math display">\[
    \pi_\theta(s, a) = \frac{1}{\sqrt{2\pi\sigma_\theta(s)}} \, \exp -\frac{(a - \mu_\theta(s))^2}{2\sigma_\theta(s)^2}
\]</span></p>
<p>In order to use backpropagation on the policy gradient (i.e. getting an analytical form of the score function <span class="math inline">\(\nabla_\theta \log \pi_\theta (s, a)\)</span>), one can use the <em>re-parameterization trick</em> <span class="citation" data-cites="Heess2015">(Heess et al. <a href="#ref-Heess2015" role="doc-biblioref">2015</a>)</span> by rewriting the policy as:</p>
<p><span class="math display">\[
    a = \mu_\theta(s) + \sigma_\theta(s) \times \xi \qquad \text{where} \qquad \xi \sim \mathcal{N}(0,1)
\]</span></p>
<p>To select an action, we only need to sample <span class="math inline">\(\xi\)</span> from the unit normal distribution, multiply it by the standard deviation and add the mean. To compute the score function, we use the following partial derivatives:</p>
<p><span class="math display">\[
    \nabla_\mu \log \pi_\theta (s, a) = \frac{a - \mu_\theta(s)}{\sigma_\theta(s)^2} \qquad \nabla_\sigma \log \pi_\theta (s, a) = \frac{(a - \mu_\theta(s))^2}{\sigma_\theta(s)^3} - \frac{1}{\sigma_\theta(s)}
\]</span></p>
<p>and use the chain rule to obtain the score function. The <em>re-parameterization trick</em> is a cool trick to apply backpropagation on stochastic problems: it is for example used in the variational auto-encoders <span class="citation" data-cites="Kingma2013">(VAR; Kingma and Welling <a href="#ref-Kingma2013" role="doc-biblioref">2013</a>)</span>.</p>
<p>Depending on the problem, one could use: 1) a fixed <span class="math inline">\(\sigma\)</span> for the whole action space, 2) a fixed <span class="math inline">\(\sigma\)</span> per DoF, 3) a learnable <span class="math inline">\(\sigma\)</span> per DoF (assuming all action dimensions to be mutually independent) or even 4) a covariance matrix <span class="math inline">\(\Sigma\)</span> when the action dimensions are dependent.</p>
<p>One limitation of Gaussian policies is that their support is infinite: even with a small variance, samples actions can deviate a lot (albeit rarely) from the mean. This is particularly a problem when action must have a limited range: the torque of an effector, the linear or angular speed of a car, etc. Clipping the sampled action to minimal and maximal values introduces a bias which can impair learning. <span class="citation" data-cites="Chou2017">Chou, Maturana, and Scherer (<a href="#ref-Chou2017" role="doc-biblioref">2017</a>)</span> proposed to use <strong>beta-distributions</strong> instead of Gaussian ones in the actor. Sampled values have a <span class="math inline">\([0,1]\)</span> support, which can rescaled to <span class="math inline">\([v_\text{min},v_\text{max}]\)</span> easily. They show that beta policies have less bias than Gaussian policies in most continuous problems.</p>
<!--PAGEBREAK-->
<h2 id="sec:off-policy-actor-critic"><span class="header-section-number">4.3</span> Off-policy Actor-Critic</h2>
<p>Actor-critic architectures are generally <strong>on-policy</strong> algorithms: the actions used to explore the environment must have been generated by the actor, otherwise the feedback provided by the critic (the advantage) will introduce a huge bias (i.e. an error) in the policy gradient. This comes from the definition of the policy gradient theorem (Section <a href="#sec:policy-gradient-theorem">4.1.3</a>, Eq. <a href="#eq:policygradienttheorem">9</a>):</p>
<p><span class="math display">\[
    \nabla_\theta J(\theta) =  \mathbb{E}_{s \sim \rho^\pi, a \sim \pi_\theta}[\nabla_\theta \log \pi_\theta(s, a) \, Q^{\pi_\theta}(s, a)]
\]</span></p>
<p>The state distribution <span class="math inline">\(\rho^\pi\)</span> defines the ensemble of states that can be visited using the actor policy <span class="math inline">\(\pi_\theta\)</span>. If, during Monte-Carlo sampling of the policy gradient, the states <span class="math inline">\(s\)</span> do not come from this state distribution, the approximated policy gradient will be wrong (high bias) and the resulting policy will be suboptimal.</p>
<p>The major drawback of on-policy methods is their sample complexity: it is difficult to ensure that the <em>“interesting”</em> regions of the policy are actually discovered by the actor (see Fig. <a href="#fig:onlinepolicyexploration">24</a>). If the actor is initialized in a flat region of the reward space (where there is not a lot of rewards), policy gradient updates will only change slightly the policy and it may take a lot of iterations until interesting policies are discovered and fine-tuned.</p>
<figure>
<img src="img/onpolicyexploration.png" alt="Figure 24: Illustration of the sample complexity inherent to on-policy methods, where the actor has only two parameters \theta_1 and \theta_2. If only very small regions of the actor parameters are associated with high rewards, the policy might wander randomly for a very long time before “hitting”the interesting regions." id="fig:onlinepolicyexploration" style="width:50.0%" /><figcaption>Figure 24: Illustration of the sample complexity inherent to on-policy methods, where the actor has only two parameters <span class="math inline">\(\theta_1\)</span> and <span class="math inline">\(\theta_2\)</span>. If only very small regions of the actor parameters are associated with high rewards, the policy might wander randomly for a very long time before “hitting”the interesting regions.</figcaption>
</figure>
<p>The problem becomes even worse when the state or action spaces are highly dimensional, or when rewards are sparse. Imagine the scenario where you are searching for your lost keys at home (a sparse reward is delivered only once you find them): you could spend hours trying randomly each action at your disposal (looking in your jackets, on your counter, but also jumping around, cooking something, watching TV…) until finally you explore the action “look behind the curtains” and find them. (Note: with deep RL, you would even have to do that one million times in order to allow gradient descent to train your brain…). If you had somebody telling you “if I were you, I would first search in your jackets, then on your counter and finally behind the curtains, but forget about watching TV, you will never find anything by doing that”, this would certainly reduce your exploration time.</p>
<p>This is somehow the idea behind <strong>off-policy</strong> algorithms: they use a <strong>behavior policy</strong> <span class="math inline">\(b(s, a)\)</span> to explore the environment and train the <strong>target policy</strong> <span class="math inline">\(\pi(s, a)\)</span> to reproduce the best ones by estimating how good they are. This does not come without caveats: if the behavior policy does not explore the optimal actions, the target policy will likely not be able to find it by itself, except by chance. But if the behavior policy is good enough, this can drastically reduce the amount of exploration needed to obtain a satisfying policy. <span class="citation" data-cites="Sutton2017">Sutton and Barto (<a href="#ref-Sutton2017" role="doc-biblioref">2017</a>)</span> noted that:</p>
<p><em>“On-policy methods are generally simpler and are considered first. Off-policy methods require additional concepts and notation, and because the data is due to a different policy, off-policy methods are often of greater variance and are slower to converge.”</em></p>
<p>The most famous off-policy method is Q-learning (Section <a href="#sec:temporal-difference">2.1.5</a>). The reason why it is off-policy is that it does not use the next executed action (<span class="math inline">\(a_{t+1}\)</span>) to update the value of an action, but the greedy action in the next state, which is independent from exploration:</p>
<p><span class="math display">\[
    \delta = r(s, a, s&#39;) + \gamma \, \max_{a&#39;} Q^\pi(s&#39;, a&#39;) - Q^\pi(s, a)
\]</span></p>
<p>The only condition for Q-learning to work (in the tabular case) is that the behavior policy <span class="math inline">\(b(s,a)\)</span> must be able to explore actions which are selected by the target policy:</p>
<p><span id="eq:qlearningcondition"><span class="math display">\[
    \pi(s, a) &gt; 0 \rightarrow b(s, a) &gt; 0
\qquad(13)\]</span></span></p>
<p>Actions which would be selected by the target policy should be selected at least from time to time by the behavior policy in order to allow their update: if the target policy thinks this action should be executed, the behavior policy should try it to confirm or infirm this assumption. In mathematical terms, there is an assumption of <em>coverage</em> of <span class="math inline">\(\pi\)</span> by <span class="math inline">\(b\)</span> (the support of <span class="math inline">\(b\)</span> includes the one of <span class="math inline">\(\pi\)</span>).</p>
<p>There are mostly two ways to create the behavior policy:</p>
<ol type="1">
<li><p><em>Use expert knowledge / human demonstrations</em>. Not all available actions should be explored: the programmer already knows they do not belong to the optimal policy. When an agent learns to play chess, for example, the behavior policy could consist of the moves typically played by human experts: if chess masters play this move, it is likely to be a good action, so it should be tried out, valued and possibly incorporated into the target policy (if it is indeed a good action, experts might be wrong). A similar idea was used to bootstrap early versions of AlphaGo <span class="citation" data-cites="Silver2016">(Silver et al. <a href="#ref-Silver2016" role="doc-biblioref">2016</a>)</span>. In robotics, one could for example use “classical” engineering methods to control the exploration of the robot, while learning (hopefully) a better policy. It is also possible to perform <em>imitation learning</em>, where the agent learns from human demonstrations (e.g. <span class="citation" data-cites="Levine2013">Levine and Koltun (<a href="#ref-Levine2013" role="doc-biblioref">2013</a>)</span>).</p></li>
<li><p><em>Derive it from the target policy</em>. In Q-learning, the target policy can be <strong>deterministic</strong>, i.e. always select the greedy action (with the maximum Q-value). The behavior policy can be derived from the target policy by making it <em><span class="math inline">\(\epsilon\)</span>-soft</em>, for example using a <span class="math inline">\(\epsilon\)</span>-greedy or softmax action selection scheme on the Q-values learned by the target policy (see Section <a href="#sec:monte-carlo-sampling">2.1.4</a>).</p></li>
</ol>
<p>The second option allows to control the level of exploration during learning (by controlling <span class="math inline">\(\epsilon\)</span> or the softmax temperature) while making sure that the target policy (the one used in production) is deterministic and optimal. It furthermore makes sure that Eq. <a href="#eq:qlearningcondition">13</a> is respected: the greedy action of the target policy always has a non-zero probability of being selected by an <span class="math inline">\(\epsilon\)</span>-greedy or softmax action selection. This is harder to ensure using expert knowledge.</p>
<p>Q-learning methods such as DQN use this second option. The target policy in DQN is actually a greedy policy with respect to the Q-values (i.e. the action with the maximum Q-value will be deterministically chosen), but an <span class="math inline">\(\epsilon\)</span>-soft behavior policy is derived from it to ensure exploration. This explains now the following comment in the description of the DQN algorithm (Section <a href="#sec:deep-q-network-dqn">3.2</a>):</p>
<ul>
<li>Select the action <span class="math inline">\(a_t\)</span> based on the behavior policy derived from <span class="math inline">\(Q_\theta(s_t, a)\)</span> (e.g. softmax).</li>
</ul>
<p>Off-policy learning furthermore allows the use of an <strong>experience replay memory</strong>: in this case, the transitions used for training the target policy were generated by an older version of it (sometimes much older). Only off-policy methods can work with replay buffers. A3C is for example on-policy: it relies on multiple parallel learners to fight against the correlation of inputs and outputs.</p>
<h3 id="sec:importance-sampling"><span class="header-section-number">4.3.1</span> Importance sampling</h3>
<p>Off-policy methods learn a target policy <span class="math inline">\(\pi(s,a)\)</span> while exploring with a behavior policy <span class="math inline">\(b(s,a)\)</span>. The environment is sampled using the behavior policy to form estimates of the state or action values (for value-based methods) or of the policy gradient (for policy gradient methods). But is it mathematically correct?</p>
<p>In policy gradient methods, we want to maximize the expected return of trajectories:</p>
<p><span class="math display">\[
    J(\theta) = \mathbb{E}_{\tau \sim \rho_\theta}[R(\tau)] = \int_\tau \rho_\theta(\tau) \, R(\tau) \, d\tau \approx \frac{1}{N} \sum_{i=1}^N R(\tau_i)
\]</span></p>
<p>where <span class="math inline">\(\rho_\theta\)</span> is the distribution of trajectories <span class="math inline">\(\tau\)</span> generated by the <strong>target</strong> policy <span class="math inline">\(\pi_\theta\)</span>. Mathematical expectations can be approximating by an average of enough samples of the estimator (Monte-Carlo). In policy gradient, we estimate the gradient, but let’s consider we sample the objective function for now. If we use a behavior policy to generate the trajectories, what we are actually estimating is:</p>
<p><span class="math display">\[
    \hat{J}(\theta) = \mathbb{E}_{\tau \sim \rho_b}[R(\tau)] = \int_\tau \rho_b(\tau) \, R(\tau) \, d\tau
\]</span></p>
<p>where <span class="math inline">\(\rho_b\)</span> is the distribution of trajectories generated by the <strong>behavior</strong> policy. In the general case, there is no reason why <span class="math inline">\(\hat{J}(\theta)\)</span> should be close from <span class="math inline">\(J(\theta)\)</span>, even when taking their gradient.</p>
<p><strong>Importance sampling</strong> is a classical statistical method used to estimate properties of a distribution (here the expected return of the trajectories of the target policy) while only having samples generated from a different distribution (here the trajectories of the behavior policy). See for example <a href="https://statweb.stanford.edu/~owen/mc/Ch-var-is.pdf" class="uri">https://statweb.stanford.edu/~owen/mc/Ch-var-is.pdf</a> and <a href="http://timvieira.github.io/blog/post/2014/12/21/importance-sampling" class="uri">http://timvieira.github.io/blog/post/2014/12/21/importance-sampling</a> for more generic explanations.</p>
<p>The trick is simply to rewrite the objective function as:</p>
<p><span class="math display">\[
\begin{aligned}
    J(\theta) &amp; = \mathbb{E}_{\tau \sim \rho_\theta}[R(\tau)]  \\
              &amp; = \int_\tau \rho_\theta(\tau) \, R(\tau) \, d\tau \\
              &amp; = \int_\tau \frac{\rho_b(\tau)}{\rho_b(\tau)} \, \rho_\theta(\tau) \, R(\tau) \, d\tau \\
              &amp; = \int_\tau \rho_b(\tau) \frac{\rho_\theta(\tau)}{\rho_b(\tau)} \, R(\tau) \, d\tau \\
              &amp; = \mathbb{E}_{\tau \sim \rho_b}[\frac{\rho_\theta(\tau)}{\rho_b(\tau)} \, R(\tau)]  \\
\end{aligned}
\]</span></p>
<p>The ratio <span class="math inline">\(\frac{\rho_\theta(\tau)}{\rho_b(\tau)}\)</span> is called the <strong>importance sampling weight</strong> for the trajectory. If a trajectory generated by <span class="math inline">\(b\)</span> is associated with a lot of rewards <span class="math inline">\(R(\tau)\)</span> (with <span class="math inline">\(\rho_b(\tau)\)</span> significantly high), the actor should learn to reproduce that trajectory with a high probability <span class="math inline">\(\rho_\theta(\tau)\)</span>, as its goal is to maximize <span class="math inline">\(J(\theta)\)</span>. Conversely, if the associated reward is low (<span class="math inline">\(R(\tau)\approx 0\)</span>), the target policy can forget about it (by setting <span class="math inline">\(\rho_\theta(\tau) = 0\)</span>), even though the behavior policy still generates it!</p>
<p>The problem is now to estimate the importance sampling weight. Using the definition of the likelihood of a trajectory, the importance sampling weight only depends on the policies, not the dynamics of the environment (they cancel out):</p>
<p><span class="math display">\[
    \frac{\rho_\theta(\tau)}{\rho_b(\tau)} = \frac{p_0 (s_0) \, \prod_{t=0}^T \pi_\theta(s_t, a_t) p(s_{t+1} | s_t, a_t)}{p_0 (s_0) \, \prod_{t=0}^T b(s_t, a_t) p(s_{t+1} | s_t, a_t)} = \frac{\prod_{t=0}^T \pi_\theta(s_t, a_t)}{\prod_{t=0}^T b(s_t, a_t)} = \prod_{t=0}^T \frac{\pi_\theta(s_t, a_t)}{b(s_t, a_t)}
\]</span></p>
<p>This allows to estimate the objective function <span class="math inline">\(J(\theta)\)</span> using Monte Carlo sampling <span class="citation" data-cites="Meuleau2000 Peshkin2002">(Meuleau et al. <a href="#ref-Meuleau2000" role="doc-biblioref">2000</a>; Peshkin and Shelton <a href="#ref-Peshkin2002" role="doc-biblioref">2002</a>)</span>:</p>
<p><span class="math display">\[
  J(\theta) \approx \frac{1}{m} \, \sum_{i=1}^m \frac{\rho_\theta(\tau_i)}{\rho_b(\tau_i)} \, R(\tau_i)
\]</span></p>
<p>All one needs to do is to repeatedly apply the following algorithm:</p>
<hr />
<ol type="1">
<li><p>Generate <span class="math inline">\(m\)</span> trajectories <span class="math inline">\(\tau_i\)</span> using the behavior policy:</p>
<ul>
<li><p>For each transition <span class="math inline">\((s_t, a_t, s_{t+1})\)</span> of each trajectory, store:</p>
<ol type="1">
<li><p>The received reward <span class="math inline">\(r_{t+1}\)</span>.</p></li>
<li><p>The probability <span class="math inline">\(b(s_t, a_t)\)</span> that the behavior policy generates this transition.</p></li>
<li><p>The probability <span class="math inline">\(\pi_\theta(s_t, a_t)\)</span> that the target policy generates this transition.</p></li>
</ol></li>
</ul></li>
<li><p>Estimate the objective function with:</p></li>
</ol>
<p><span class="math display">\[
  \hat{J}(\theta) = \frac{1}{m} \, \sum_{i=1}^m \left(\prod_{t=0}^T \frac{\pi_\theta(s_t, a_t)}{b(s_t, a_t)} \right) \, \left(\sum_{t=0}^T \gamma^t \, r_{t+1} \right)
\]</span></p>
<ol start="3" type="1">
<li>Update the target policy to maximize <span class="math inline">\(\hat{J}(\theta)\)</span>.</li>
</ol>
<hr />
<p><span class="citation" data-cites="Tang2010">Tang and Abbeel (<a href="#ref-Tang2010" role="doc-biblioref">2010</a>)</span> showed that the same idea can be applied to the policy gradient, under assumptions often met in practice:</p>
<p><span class="math display">\[
    \nabla_\theta J(\theta) =  \mathbb{E}_{\tau \sim \rho_b}[ \nabla_\theta \log \rho_\theta(\tau) \, \frac{\rho_\theta(\tau)}{\rho_b(\tau)} \, R(\tau)]
\]</span></p>
<p>When decomposing the policy gradient for each state encountered, one can also use the <strong>causality</strong> principle to simplify the terms:</p>
<ol type="1">
<li><p>The return after being in a state <span class="math inline">\(s_t\)</span> only depends on future states.</p></li>
<li><p>The importance sampling weight (relative probability of arriving in <span class="math inline">\(s_t\)</span> using the behavior and target policies) only depends on the past weights.</p></li>
</ol>
<p>This gives the following approximation of the policy gradient, used for example in Guided policy search <span class="citation" data-cites="Levine2013">(Levine and Koltun <a href="#ref-Levine2013" role="doc-biblioref">2013</a>)</span>:</p>
<p><span class="math display">\[
    \nabla_\theta J(\theta) =  \mathbb{E}_{\tau \sim \rho_b}[ \sum_{t=0}^T \nabla_\theta \log \pi_\theta(s_t, a_t) \, \left(\prod_{t&#39;=0}^t \frac{\pi_\theta(s_{t&#39;}, a_{t&#39;})}{b(s_{t&#39;}, a_{t&#39;})} \right) \, \left(\sum_{t&#39;=t}^T \gamma^{t&#39;-t} \, r(s_{t&#39;}, a_{t&#39;}) \right)]
\]</span></p>
<h3 id="sec:linear-off-policy-actor-critic-off-pac"><span class="header-section-number">4.3.2</span> Linear Off-Policy Actor-Critic (Off-PAC)</h3>
<p>The first off-policy actor-critic method was proposed by <span class="citation" data-cites="Degris2012">Degris, White, and Sutton (<a href="#ref-Degris2012" role="doc-biblioref">2012</a>)</span> for linear approximators. Another way to express the objective function in policy search is by using the Bellman equation (here in the off-policy setting):</p>
<p><span class="math display">\[
    J(\theta) = \mathbb{E}_{s \sim \rho_b} [V^{\pi_\theta}(s)] = \mathbb{E}_{s \sim \rho_b} [\sum_{a\in\mathcal{A}} \pi(s, a) \, Q^{\pi_\theta}(s, a)]
\]</span></p>
<p>Maximizing the value of all states reachable by the policy is the same as finding the optimal policy: the encoutered states bring the maximum return. The policy gradient becomes:</p>
<p><span class="math display">\[
    \nabla_\theta J(\theta) = \mathbb{E}_{s \sim \rho_b} [\sum_{a\in\mathcal{A}} \nabla_\theta  (\pi_\theta(s, a) \, Q^{\pi_\theta}(s, a))]
\]</span></p>
<p>Because both <span class="math inline">\(\pi(s, a)\)</span> and <span class="math inline">\(Q^\pi(s, a)\)</span> depend on the target policy <span class="math inline">\(\pi_\theta\)</span> (hence its parameters <span class="math inline">\(\theta\)</span>), one should normally write:</p>
<p><span class="math display">\[
    \nabla_\theta  (\pi_\theta(s, a) \, Q^{\pi_\theta}(s, a)) = Q^{\pi_\theta}(s, a) \, \nabla_\theta  \pi_\theta(s, a) + \pi_\theta(s, a) \, \nabla_\theta Q^{\pi_\theta}(s, a)
\]</span></p>
<p>The second term depends on <span class="math inline">\(\nabla_\theta Q^{\pi_\theta}(s, a)\)</span>, which is very difficult to estimate. <span class="citation" data-cites="Degris2012">Degris, White, and Sutton (<a href="#ref-Degris2012" role="doc-biblioref">2012</a>)</span> showed that when the Q-values are estimated by an unbiased <strong>critic</strong> <span class="math inline">\(Q_\varphi(s, a)\)</span>, this second term can be omitted. Using the log-trick and importance sampling, the policy gradient can be expressed as:</p>
<p><span class="math display">\[
\begin{aligned}
    \nabla_\theta J(\theta) &amp; = \mathbb{E}_{s \sim \rho_b} [\sum_{a\in\mathcal{A}} Q_\varphi(s, a) \, \nabla_\theta \pi_\theta(s, a)] \\
                            &amp; = \mathbb{E}_{s \sim \rho_b} [\sum_{a\in\mathcal{A}} b(s, a) \, \frac{\pi_\theta(s, a)}{b(s, a)} \, Q_\varphi(s, a) \, \frac{\nabla_\theta \pi_\theta(s, a)}{\pi_\theta(s, a)}] \\
                            &amp; = \mathbb{E}_{s,a \sim \rho_b} [\frac{\pi_\theta(s, a)}{b(s, a)} \, Q_\varphi(s, a) \, \nabla_\theta \log \pi_\theta(s, a)] \\
\end{aligned}
\]</span></p>
<p>We now have an <strong>actor-critic</strong> architecture (actor <span class="math inline">\(\pi_\theta(s, a)\)</span>, critic <span class="math inline">\(Q_\varphi(s, a)\)</span>) able to learn from single transitions <span class="math inline">\((s,a)\)</span> (<strong>online update</strong> instead of complete trajectories) generated <strong>off-policy</strong> (behavior policy <span class="math inline">\(b(s,a)\)</span> and importance sampling weight <span class="math inline">\(\frac{\pi_\theta(s, a)}{b(s, a)}\)</span>). The off-policy actor-critic (Off-PAC) algorithm of <span class="citation" data-cites="Degris2012">Degris, White, and Sutton (<a href="#ref-Degris2012" role="doc-biblioref">2012</a>)</span> furthermore uses <strong>eligibility traces</strong> to stabilize learning. However, it was limited to linear function approximators because its variance is too high to train deep neural networks.</p>
<h3 id="sec:retrace"><span class="header-section-number">4.3.3</span> Retrace</h3>
<p>For a good deep RL algorithm, we need the two following properties:</p>
<ol type="1">
<li><p><strong>Off-policy</strong> learning: it allows to learn from transitions stored in a replay buffer (i.e. generated with an older policy). As NN need many iterations to converge, it is important to be able to re-use old transitions for its training, instead of constantly sampling new ones (sample complexity). Multiple parallel actors as in A3C allow to mitigate this problem, but it is still too complex.</p></li>
<li><p><strong>Multi-step returns</strong>: the two extremes of RL are TD (using a single “real” reward for the update, the rest is estimated) and Monte-Carlo (use only “real” rewards, no estimation). TD has a smaller variance, but a high bias (errors in estimates propagate to all other values), while MC has a small bias but a high variance (learns from many real rewards, but the returns may vary a lot between two almost identical episodes). Eligibility traces and n-step returns (used in A3C) are the most common trade-off between TD and MC.</p></li>
</ol>
<p>The <strong>Retrace</strong> algorithm <span class="citation" data-cites="Munos2016">(Munos et al. <a href="#ref-Munos2016" role="doc-biblioref">2016</a>)</span> is designed to exhibit both properties when learning Q-values. It can therefore be used to train the critic (instead of classical Q-learning) and provide the actor with safe, efficient and low-variance values.</p>
<p>In the generic form, Q-learning updates the Q-value of a transition <span class="math inline">\((s_t, a_t)\)</span> using the TD error:</p>
<p><span class="math display">\[
    \Delta Q^\pi(s_t, a_t) = \alpha \, \delta_t = \alpha \, (r_{t+1} + \gamma \, \max_a Q^\pi(s_{t+1}, a_{t+1}) - Q^\pi(s_t, a_t))
\]</span></p>
<p>When using eligibility traces in the forward view (Section <a href="#sec:eligibility-traces">2.1.6</a>), the change in Q-value depends also on the TD error of future transitions at times <span class="math inline">\(t&#39; &gt; t\)</span>. A parameter <span class="math inline">\(\lambda\)</span> ensures the stability of the update:</p>
<p><span class="math display">\[
    \Delta Q^\pi(s_t, a_t) = \alpha \, \sum_{t&#39;=t}^T (\gamma \lambda)^{t&#39;-t} \delta_{t&#39;}
\]</span></p>
<p>The Retrace algorithm proposes to generalize this formula using a parameter <span class="math inline">\(c_s\)</span> for each time step between <span class="math inline">\(t\)</span> and <span class="math inline">\(t&#39;\)</span>:</p>
<p><span class="math display">\[
    \Delta Q^\pi(s_t, a_t) = \alpha \, \sum_{t&#39;=t}^T (\gamma)^{t&#39;-t} \left(\prod_{s=t+1}^{t&#39;} c_s \right) \, \delta_{t&#39;}
\]</span></p>
<p>Depending on the choice of <span class="math inline">\(c_s\)</span>, the formula covers different existing methods:</p>
<ol type="1">
<li><span class="math inline">\(c_s = \lambda\)</span> is the classical <strong>eligibility trace</strong> mechanism (<span class="math inline">\(Q(\lambda)\)</span>) in its forward view, which is not safe: the behavior policy <span class="math inline">\(b\)</span> must be very close from the target policy <span class="math inline">\(\tau\)</span>:</li>
</ol>
<p><span class="math display">\[
    || \pi - b ||_1 \leq \frac{1 - \gamma}{\lambda \gamma}
\]</span></p>
<p>As <span class="math inline">\(\gamma\)</span> is typically chosen very close from 1 (e.g. 0.99), this does not leave much room for the target policy to differ from the behavior policy <span class="citation" data-cites="Harutyunyan2016">(see Harutyunyan et al. <a href="#ref-Harutyunyan2016" role="doc-biblioref">2016</a> for the proof)</span>.</p>
<ol start="2" type="1">
<li><p><span class="math inline">\(c_s = \frac{\pi(s_s, a_s)}{b(s_s, a_s)}\)</span> is the importance sampling weight. As shown in Section <a href="#sec:importance-sampling">4.3.1</a>, importance sampling is unbiased in off-policy settings, but can have a very large variance: the product of ratios <span class="math inline">\(\prod_{s=t+1}^{t&#39;} \frac{\pi(s_s, a_s)}{b(s_s, a_s)}\)</span> can quickly vary between two episodes.</p></li>
<li><p><span class="math inline">\(c_s = \pi(s_s, a_s)\)</span> corresponds to the tree-backup algorithm <span class="math inline">\(TB(\lambda)\)</span> <span class="citation" data-cites="Precup2000">(Precup, Sutton, and Singh <a href="#ref-Precup2000" role="doc-biblioref">2000</a>)</span>. It has the advantage to work for arbitrary policies <span class="math inline">\(\pi\)</span> and <span class="math inline">\(b\)</span>, but the product of such probabilities decays very fast to zero when the time difference <span class="math inline">\(t&#39; - t\)</span> increases: TD errors will be efficiently shared over a couple of steps only.</p></li>
</ol>
<p>For Retrace, <span class="citation" data-cites="Munos2016">Munos et al. (<a href="#ref-Munos2016" role="doc-biblioref">2016</a>)</span> showed that a much better value for <span class="math inline">\(c_s\)</span> is:</p>
<p><span class="math display">\[
    c_s = \lambda \min (1, \frac{\pi(s_s, a_s)}{b(s_s, a_s)})
\]</span></p>
<p>The importance sampling weight is clipped to 1, and decays exponentially with the parameter <span class="math inline">\(\lambda\)</span>. It can be seen as a trade-off between importance sampling and eligibility traces. The authors showed that Retrace(<span class="math inline">\(\lambda\)</span>) has a low variance (as it uses multiple returns), is safe (works for all <span class="math inline">\(\pi\)</span> and <span class="math inline">\(b\)</span>) and efficient (it can propagate rewards over many time steps). They used retrace to learn Atari games and compared it positively with DQN, both in terms of optimality and speed of learning. These properties make Retrace particularly suited for deep RL and actor-critic architectures: it is for example used in ACER (Section <a href="#sec:actor-critic-with-experience-replay-acer">4.5.5</a>) and the Reactor (Section <a href="#sec:the-reactor">4.7.1</a>).</p>
<p>Rémi Munos uploaded some slides explaining Retrace in a simpler manner than in the original paper: <a href="https://ewrl.files.wordpress.com/2016/12/munos.pdf" class="uri">https://ewrl.files.wordpress.com/2016/12/munos.pdf</a>.</p>
<h3 id="sec:self-imitation-learning-sil"><span class="header-section-number">4.3.4</span> Self-Imitation Learning (SIL)</h3>
<p>We have discussed or now only strictly on-policy or off-policy methods. Off-policy methods are much more stable and efficient, but they learn generally a deterministic policy, what can be problematic in stochastic environments (e.g. two players games: being predictable is clearly an issue). Hybrid methods combining on- and off-policy mechanisms have clearly a great potential.</p>
<p><span class="citation" data-cites="Oh2018">Oh et al. (<a href="#ref-Oh2018" role="doc-biblioref">2018</a>)</span> proposed a <strong>Self-Imitation Learning</strong> (SIL) method that can extend on-policy actor-critic algorithms (e.g. A2C, Section <a href="#sec:advantage-actor-critic-a2c">4.2.1</a>) with a replay buffer able to feed past <em>good</em> experiences to the NN to speed up learning.</p>
<p>The main idea is to use <strong>prioritized experience replay</strong> (<span class="citation" data-cites="Schaul2015">Schaul et al. (<a href="#ref-Schaul2015" role="doc-biblioref">2015</a>)</span>, see Section <a href="#sec:prioritized-experience-replay">3.4</a>) to select only transitions whose actual return is higher than their current expected value. This defines two additional losses for the actor and the critic:</p>
<p><span class="math display">\[
    \mathcal{L}^\text{SIL}_\text{actor}(\theta) = \mathbb{E}_{s, a \in \mathcal{D}}[\log \pi_\theta(s, a) \, (R(s, a) - V_\varphi(s))^+]
\]</span> <span class="math display">\[
    \mathcal{L}^\text{SIL}_\text{critic}(\varphi) = \mathbb{E}_{s, a \in \mathcal{D}}[((R(s, a) - V_\varphi(s))^+)^2]
\]</span></p>
<p>where <span class="math inline">\((x)^+ = \max(0, x)\)</span> is the positive function. Transitions sampled from the replay buffer will participate to the off-policy learning only if their return is higher that the current value of the state, i.e. if they are good experiences compared to what is currently known (<span class="math inline">\(V_\varphi(s)\)</span>). The pseudo-algorithm is actually quite simple and simply extends the A2C procedure:</p>
<hr />
<ul>
<li><p>Initialize the actor <span class="math inline">\(\pi_\theta\)</span> and the critic <span class="math inline">\(V_\varphi\)</span> with random weights.</p></li>
<li><p>Initialize the prioritized experience replay buffer <span class="math inline">\(\mathcal{D}\)</span>.</p></li>
<li><p>Observe the initial state <span class="math inline">\(s_0\)</span>.</p></li>
<li><p>for <span class="math inline">\(t \in [0, T_\text{total}]\)</span>:</p>
<ul>
<li><p>Initialize empty episode minibatch.</p></li>
<li><p>for <span class="math inline">\(k \in [0, n]\)</span>: # Sample episode</p>
<ul>
<li><p>Select a action <span class="math inline">\(a_k\)</span> using the actor <span class="math inline">\(\pi_\theta\)</span>.</p></li>
<li><p>Perform the action <span class="math inline">\(a_k\)</span> and observe the next state <span class="math inline">\(s_{k+1}\)</span> and the reward <span class="math inline">\(r_{k+1}\)</span>.</p></li>
<li><p>Store <span class="math inline">\((s_k, a_k, r_{k+1})\)</span> in the episode minibatch.</p></li>
</ul></li>
<li><p>if <span class="math inline">\(s_n\)</span> is not terminal: set <span class="math inline">\(R_n = V_\varphi(s_n)\)</span> with the critic, else <span class="math inline">\(R_n=0\)</span>.</p></li>
<li><p>for <span class="math inline">\(k \in [n-1, 0]\)</span>: # Backwards iteration over the episode</p>
<ul>
<li>Update the discounted sum of rewards <span class="math inline">\(R_k = r_k + \gamma \, R_{k+1}\)</span> <strong>and store it in the replay buffer <span class="math inline">\(\mathcal{D}\)</span></strong>.</li>
</ul></li>
<li><p>Update the actor and the critic <strong>on-policy</strong> with the episode:</p></li>
</ul>
<p><span class="math display">\[
      \theta \leftarrow \theta + \eta \, \sum_k \nabla_\theta \log \pi_\theta(s_k, a_k) \, (R_k - V_\varphi(s_k))
  \]</span></p>
<p><span class="math display">\[
      \varphi \leftarrow \varphi + \eta \, \sum_k \nabla_\varphi (R - V_\varphi(s_k))^2
  \]</span></p>
<ul>
<li><p>for <span class="math inline">\(m \in [0, M]\)</span>:</p>
<ul>
<li><p>Sample a minibatch of K transitions <span class="math inline">\((s_k, a_k, R_k)\)</span> from the replay buffer <span class="math inline">\(\mathcal{D}\)</span> prioritized with high <span class="math inline">\((R_k - V_\varphi(s_k))\)</span>.</p></li>
<li><p>Update the actor and the critic <strong>off-policy</strong> with self-imitation.</p></li>
</ul>
<p><span class="math display">\[
      \theta \leftarrow \theta + \eta \, \sum_k \nabla_\theta \log \pi_\theta(s_k, a_k) \, (R_k - V_\varphi(s_k))^+
  \]</span></p>
<p><span class="math display">\[
      \varphi \leftarrow \varphi + \eta \, \sum_k \nabla_\varphi ((R_k - V_\varphi(s_k))^+)^2
  \]</span></p></li>
</ul></li>
</ul>
<hr />
<p>In the paper, they furthermore used entropy regularization as in A3C. They showed that A2C+SIL has a better performance both on Atari games and continuous control problems (Mujoco) than state-of-the art methods (A3C, TRPO, Reactor, PPO). It shows that self-imitation learning can be very useful in problems where exploration is hard: a proper level of exploitation of past experiences actually fosters a deeper exploration of environment.</p>
<!--PAGEBREAK-->
<h2 id="sec:deterministic-policy-gradient-dpg"><span class="header-section-number">4.4</span> Deterministic Policy Gradient (DPG)</h2>
<p>So far, the actor produces a stochastic policy <span class="math inline">\(\pi_\theta(s)\)</span> assigning probabilities to each discrete action or necessitating sampling in some distribution for continuous actions (see Section <a href="#sec:stochastic-actor-critic-for-continuous-action-spaces">4.2.4</a>). The main advantage is that stochastic policies ensure <strong>exploration</strong> of the state-action space: as most actions have a non-zero probability of being selected, we should not miss any important reward which should be ignored if the greedy action is always selected (exploration/exploitation dilemma).</p>
<p>There are however two drawbacks:</p>
<ol type="1">
<li>The policy gradient theorem only works <strong>on-policy</strong>: the value of an action estimated by the critic must have been produced recently by the actor, otherwise the bias would increase dramatically (but see importance sampling, Section <a href="#sec:off-policy-actor-critic">4.3</a>). This prevents the use of an experience replay memory as in DQN to stabilize learning. Importance sampling can help, but is unstable for long trajectories.</li>
<li>Because of the stochasticity of the policy, the returns may vary considerably between two episodes generated by the same optimal policy. This induces a lot of <strong>variance</strong> in the policy gradient, which explains why policy gradient methods have a worse <strong>sample complexity</strong> than value-based methods: they need more samples to get rid of this variance.</li>
</ol>
<p>Successful value-based methods such as DQN produce a <strong>deterministic policy</strong>, where the action to be executed after learning is simply the greedy action <span class="math inline">\(a^*_t = \text{argmax}_a Q_\theta(s_t, a)\)</span>. Exploration is enforced by forcing the behavior policy (the one used to generate the sample) to be stochastic (<span class="math inline">\(\epsilon\)</span>-greedy), but the learned policy is itself deterministic. This is <strong>off-policy</strong> learning, allowing to use a different policy than the learned one to explore. When using an experience replay memory, the behavior policy is simply an older version of the learning policy (as samples stored in the ERM were generated by an older version of the actor).</p>
<p>In this section, we will see the now state-of-the-art method DDPG (Deep Deterministic Policy Gradient), which tries to combine the advantages of policy gradient methods (actor-critic, continuous or highly dimensional outputs, stability) with those of value-based methods (sample efficiency, off-policy).</p>
<h3 id="sec:deterministic-policy-gradient-theorem"><span class="header-section-number">4.4.1</span> Deterministic policy gradient theorem</h3>
<p>We now assume that we want to learn a parameterized <strong>deterministic policy</strong> <span class="math inline">\(\mu_\theta(s)\)</span>. As for the stochastic policy gradient theorem, the goal is to maximize the expectation over all states reachable by the policy of the reward to-go (return) after each action:</p>
<p><span class="math display">\[
    J(\theta) =  \mathbb{E}_{s \sim \rho_\mu}[R(s, \mu_\theta(s))]
\]</span></p>
<p>As in the stochastic case, the distribution of states reachable by the policy <span class="math inline">\(\rho_\mu\)</span> is impossible to estimate, so we will have to perform approximations. Building on <span class="citation" data-cites="Hafner2011">Hafner and Riedmiller (<a href="#ref-Hafner2011" role="doc-biblioref">2011</a>)</span>, <span class="citation" data-cites="Silver2014">Silver et al. (<a href="#ref-Silver2014" role="doc-biblioref">2014</a>)</span> showed how to obtain a usable gradient for the objective function when the policy is deterministic.</p>
<p>Considering that the Q-value of an action is the expectation of the reward to-go after that action <span class="math inline">\(Q^\pi(s, a) = \mathbb{E}_\pi[R(s, a)]\)</span>, maximizing the returns or maximizing the true Q-value of all actions leads to the same optimal policy. This is the basic idea behind dynamic programming (see Section <a href="#sec:dynamic-programming">2.1.3</a>). where <em>policy evaluation</em> first finds the true Q-value of all state-action pairs and <em>policy improvement</em> changes the policy by selecting the action with the maximal Q-value <span class="math inline">\(a^*_t = \text{argmax}_a Q_\theta(s_t, a)\)</span>.</p>
<p>In the continuous case, we will simply state that the gradient of the objective function is the same as the gradient of the Q-value. Supposing we have an unbiased estimate <span class="math inline">\(Q^\mu(s, a)\)</span> of the value of any action in <span class="math inline">\(s\)</span>, changing the policy <span class="math inline">\(\mu_\theta(s)\)</span> in the direction of <span class="math inline">\(\nabla_\theta Q^\mu(s, a)\)</span> leads to an action with a higher Q-value, therefore with a higher associated return:</p>
<p><span class="math display">\[
    \nabla_\theta J(\theta) = \mathbb{E}_{s \sim \rho_\mu}[\nabla_\theta Q^\mu(s, a) |_{a = \mu_\theta(s)}]
\]</span></p>
<p>This notation means that the gradient w.r.t <span class="math inline">\(a\)</span> of the Q-value is taken at <span class="math inline">\(a = \mu_\theta(s)\)</span>. We now use the chain rule to expand the gradient of the Q-value:</p>
<p><span class="math display">\[
    \nabla_\theta J(\theta) = \mathbb{E}_{s \sim \rho_\mu}[\nabla_\theta \mu_\theta(s) \times \nabla_a Q^\mu(s, a) |_{a = \mu_\theta(s)}]
\]</span></p>
<p>It is perhaps clearer using partial derivatives and simplifying the notations:</p>
<p><span class="math display">\[
    \frac{\partial Q(s,a)}{\partial \theta} = \frac{\partial Q(s,a)}{\partial a} \times \frac{\partial a}{\partial \theta}
\]</span></p>
<p>The first term defines of the Q-value of an action changes when one varies slightly the action (if I move my joint a bit more to the right, do I get a higher Q-value, hence more reward?), the second term defines how the action changes when the parameters <span class="math inline">\(\theta\)</span> of the actor change (which weights should be changed in order to produce that action with a slightly higher Q-value?).</p>
<p>We already see an <strong>actor-critic</strong> architecture emerging from this equation: <span class="math inline">\(\nabla_\theta \mu_\theta(s)\)</span> only depends on the parameterized actor, while <span class="math inline">\(\nabla_a Q^\mu(s, a)\)</span> is a sort of critic, telling the actor in which direction to change its policy: towards actions associated with more reward.</p>
<p>As in the stochastic policy gradient theorem, the question is now how to obtain an unbiased estimate of the Q-value of any action and compute its gradient. <span class="citation" data-cites="Silver2014">Silver et al. (<a href="#ref-Silver2014" role="doc-biblioref">2014</a>)</span> showed that it is possible to use a function approximator <span class="math inline">\(Q_\varphi(s, a)\)</span> as long as it is compatible and minimize the quadratic error with the true Q-values:</p>
<p><span class="math display">\[
    \nabla_\theta J(\theta) = \mathbb{E}_{s \sim \rho_\mu}[\nabla_\theta \mu_\theta(s) \times \nabla_a Q_\varphi(s, a) |_{a = \mu_\theta(s)}]
\]</span> <span class="math display">\[
    J(\varphi) = \mathbb{E}_{s \sim \rho_\mu}[(Q^\mu(s, \mu_\theta(s)) - Q_\varphi(s, \mu_\theta(s)))^2]
\]</span></p>
<p>Fig. <a href="#fig:dpg">25</a> outlines the actor-critic architecture of the DPG (deterministic policy gradient) method, to compare with the actor-critic architecture of the stochastic policy gradient (Fig. <a href="#fig:actorcriticpolicy">21</a>).</p>
<figure>
<img src="img/dpg.png" alt="Figure 25: Architecture of the DPG (deterministic policy gradient) method." id="fig:dpg" style="width:65.0%" /><figcaption>Figure 25: Architecture of the DPG (deterministic policy gradient) method.</figcaption>
</figure>
<p><span class="citation" data-cites="Silver2014">Silver et al. (<a href="#ref-Silver2014" role="doc-biblioref">2014</a>)</span> investigated the performance of DPG using linear function approximators and showed that it compared positively to stochastic algorithms in high-dimensional or continuous action spaces. However, non-linear function approximators such as deep NN would not work yet.</p>
<h3 id="sec:deep-deterministic-policy-gradient-ddpg"><span class="header-section-number">4.4.2</span> Deep Deterministic Policy Gradient (DDPG)</h3>
<p><span class="citation" data-cites="Lillicrap2015">Lillicrap et al. (<a href="#ref-Lillicrap2015" role="doc-biblioref">2015</a>)</span> extended the DPG approach to work with non-linear function approximators. In fact, they combined ideas from DQN and DPG to create a very successful algorithm able to solve continuous problems off-policy, the <strong>deep deterministic policy gradient</strong> (DDPG) algorithm..</p>
<p>The key ideas borrowed from DQN are:</p>
<ul>
<li>Using an <strong>experience replay memory</strong> to store past transitions and learn off-policy.</li>
<li>Using <strong>target networks</strong> to stabilize learning.</li>
</ul>
<p>They modified the update frequency of the target networks originally used in DQN. In DQN, the target networks are updated with the parameters of the trained networks every couple of thousands of steps. The target networks therefore change a lot between two updates, but not very often. <span class="citation" data-cites="Lillicrap2015">Lillicrap et al. (<a href="#ref-Lillicrap2015" role="doc-biblioref">2015</a>)</span> found that it is actually better to make the target networks slowly track the trained networks, by updating their parameters after each update of the trained network using a sliding average for both the actor and the critic:</p>
<p><span class="math display">\[
    \theta&#39; = \tau \, \theta + (1-\tau) \, \theta&#39;
\]</span></p>
<p>with <span class="math inline">\(\tau &lt;&lt;1\)</span>. Using this update rule, the target networks are always “late” with respect to the trained networks, providing more stability to the learning of Q-values.</p>
<p>The key idea borrowed from DPG is the policy gradient for the actor. The critic is learned using regular Q-learning and target networks:</p>
<p><span class="math display">\[
    J(\varphi) = \mathbb{E}_{s \sim \rho_\mu}[(r(s, a, s&#39;) + \gamma \, Q_{\varphi&#39;}(s&#39;, \mu_{\theta&#39;}(s&#39;)) - Q_\varphi(s, a))^2]
\]</span></p>
<p>One remaining issue is <strong>exploration</strong>: as the policy is deterministic, it can very quickly produce always the same actions, missing perhaps more rewarding options. Some environments are naturally noisy, enforcing exploration by itself, but this cannot be assumed in the general case. The solution retained in DDPG is an <strong>additive noise</strong> added to the deterministic action to explore the environment:</p>
<p><span class="math display">\[
    a_t = \mu_\theta(s_t) + \xi
\]</span></p>
<p>This additive noise could be anything, but the most practical choice is to use an <strong>Ornstein-Uhlenbeck</strong> process <span class="citation" data-cites="Uhlenbeck1930">(Uhlenbeck and Ornstein <a href="#ref-Uhlenbeck1930" role="doc-biblioref">1930</a>)</span> to generate temporally correlated noise with zero mean. Ornstein-Uhlenbeck processes are used in physics to model the velocity of Brownian particles with friction. It updates a variable <span class="math inline">\(x_t\)</span> using a stochastic differential equation (SDE):</p>
<p><span class="math display">\[ dx_t = \theta (\mu - x_t) dt + \sigma dW_t \qquad \text{with} \qquad dW_t = \mathcal{N}(0, dt)\]</span></p>
<p><span class="math inline">\(\mu\)</span> is the mean of the process (usually 0), <span class="math inline">\(\theta\)</span> is the friction (how fast it varies with noise) and <span class="math inline">\(\sigma\)</span> controls the amount of noise. Fig. <a href="#fig:OU">26</a> shows three independent runs of a Ornstein-Uhlenbeck process: successive values of the variable <span class="math inline">\(x_t\)</span> vary randomly but coherently over time.</p>
<figure>
<img src="img/OU.png" alt="Figure 26: Three independent runs of an Ornstein-Uhlenbeck process with \mu=0, \sigma=0.3, \theta=0.15 and dt=0.1. The code is adapted from https://gist.github.com/jimfleming/9a62b2f7ed047ff78e95b5398e955b9e" id="fig:OU" style="width:80.0%" /><figcaption>Figure 26: Three independent runs of an Ornstein-Uhlenbeck process with <span class="math inline">\(\mu=0\)</span>, <span class="math inline">\(\sigma=0.3\)</span>, <span class="math inline">\(\theta=0.15\)</span> and <span class="math inline">\(dt=0.1\)</span>. The code is adapted from <a href="https://gist.github.com/jimfleming/9a62b2f7ed047ff78e95b5398e955b9e" class="uri">https://gist.github.com/jimfleming/9a62b2f7ed047ff78e95b5398e955b9e</a></figcaption>
</figure>
<p>The architecture of the DDPG algorithm is depicted on Fig. <a href="#fig:ddpg">27</a>.</p>
<figure>
<img src="img/ddpg.png" alt="Figure 27: Architecture of the DDPG (deep deterministic policy gradient) algorithm." id="fig:ddpg" style="width:70.0%" /><figcaption>Figure 27: Architecture of the DDPG (deep deterministic policy gradient) algorithm.</figcaption>
</figure>
<p>The pseudo-algorithm is as follows:</p>
<hr />
<ul>
<li><p>Initialize actor network <span class="math inline">\(\mu_{\theta}\)</span> and critic <span class="math inline">\(Q_\varphi\)</span> with random weights.</p></li>
<li><p>Create the target networks <span class="math inline">\(\mu_{\theta&#39;}\)</span> and <span class="math inline">\(Q_{\varphi&#39;}\)</span>.</p></li>
<li><p>Initialize experience replay memory <span class="math inline">\(\mathcal{D}\)</span> of maximal size <span class="math inline">\(N\)</span>.</p></li>
<li><p>for episode <span class="math inline">\(\in [1, M]\)</span>:</p>
<ul>
<li>Initialize random process <span class="math inline">\(\xi\)</span>.</li>
<li>Observe the initial state <span class="math inline">\(s_0\)</span>.</li>
<li>for <span class="math inline">\(t \in [0, T_\text{max}]\)</span>:
<ul>
<li><p>Select the action <span class="math inline">\(a_t = \mu_\theta(s_t) + \xi\)</span> according to the current policy and the noise.</p></li>
<li><p>Perform the action <span class="math inline">\(a_t\)</span> and observe the next state <span class="math inline">\(s_{t+1}\)</span> and the reward <span class="math inline">\(r_{t+1}\)</span>.</p></li>
<li><p>Store <span class="math inline">\((s_t, a_t, r_{t+1}, s_{t+1})\)</span> in the experience replay memory.</p></li>
<li><p>Sample a minibatch of <span class="math inline">\(N\)</span> transitions randomly from <span class="math inline">\(\mathcal{D}\)</span>.</p></li>
<li><p>For each transition <span class="math inline">\((s_k, a_k, r_k, s&#39;_k)\)</span> in the minibatch:</p>
<ul>
<li>Compute the target value using target networks <span class="math inline">\(y_k = r_k + \gamma \, Q_{\varphi&#39;}(s&#39;_k, \mu_{\theta&#39;}(s&#39;_k))\)</span>.</li>
</ul></li>
<li><p>Update the critic by minimizing: <span class="math display">\[
  \mathcal{L}(\varphi) = \frac{1}{N} \sum_k (y_k - Q_\varphi(s_k, a_k))^2
  \]</span></p></li>
<li><p>Update the actor using the sampled policy gradient: <span class="math display">\[
  \nabla_\theta J(\theta) = \frac{1}{N} \sum_k \nabla_\theta \mu_\theta(s_k) \times \nabla_a Q_\varphi(s_k, a) |_{a = \mu_\theta(s_k)}
  \]</span></p></li>
<li><p>Update the target networks: <span class="math display">\[\theta&#39; \leftarrow \tau \theta + (1-\tau) \, \theta&#39;\]</span> <span class="math display">\[\varphi&#39; \leftarrow \tau \varphi + (1-\tau) \, \varphi&#39;\]</span></p></li>
</ul></li>
</ul></li>
</ul>
<hr />
<p>The question that arises is how to obtain the gradient of the Q-value w.r.t the action <span class="math inline">\(\nabla_a Q_\varphi(s, a)\)</span>, when the critic only outputs the Q-value <span class="math inline">\(Q_\varphi(s, a)\)</span>. Fortunately, deep neural networks are simulated using automatic differentiation libraries such as tensorflow, theano, pytorch and co, which can automatically output this gradient. If not available, one could simply use the finite difference method (Euler) to approximate this gradient. One has to evaluate the Q-value in <span class="math inline">\(a +da\)</span>, where <span class="math inline">\(da\)</span> is a very small change of the executed action, and estimate the gradient using:</p>
<p><span class="math display">\[
    \nabla_a Q_\varphi(s, a) \approx \frac{Q_\varphi(s, a + da) - Q_\varphi(s, a)}{da}
\]</span></p>
<p>Note that the DDPG algorithm is <strong>off-policy</strong>: the samples used to train the actor come from the replay buffer, i.e. were generated by an older version of the target policy. DDPG does not rely on importance sampling: as the policy is deterministic (we maximize <span class="math inline">\(\mathbb{E}_{s}[Q(s, \mu_\theta(s))]\)</span>), there is no need to balance the probabilities of the behavior and target policies (with stochastic policies, one should maximize <span class="math inline">\(\mathbb{E}_{s}[\sum_{a\in\mathcal{A}} \pi(s, a) Q(s, a)]\)</span>). In other words, the importance sampling weight can safely be set to 1 for deterministic policies.</p>
<p>DDPG has rapidly become the state-of-the-art model-free method for continuous action spaces (although now PPO is preferred). It is able to learn efficent policies on most contiuous problems, either pixel-based or using individual state variables. In the original DDPG paper, they showed that <em>batch normalization</em> <span class="citation" data-cites="Ioffe2015">(Ioffe and Szegedy <a href="#ref-Ioffe2015" role="doc-biblioref">2015</a>)</span> is crucial in stabilizing the training of deep networks on such problems. Its main limitation is its high sample complexity. Distributed versions of DDPG have been proposed to speed up learning, similarly to the parallel actor learners of A3C <span class="citation" data-cites="Lotzsch2017 Popov2017 Barth-Maron2018">(Lötzsch, Vitay, and Hamker <a href="#ref-Lotzsch2017" role="doc-biblioref">2017</a>; Popov et al. <a href="#ref-Popov2017" role="doc-biblioref">2017</a>; Barth-Maron et al. <a href="#ref-Barth-Maron2018" role="doc-biblioref">2018</a>)</span>.</p>
<p><strong>Additional references:</strong> see <a href="http://pemami4911.github.io/blog/2016/08/21/ddpg-rl.html" class="uri">http://pemami4911.github.io/blog/2016/08/21/ddpg-rl.html</a> for additional explanations and step-by-step tensorflow code and <a href="https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html" class="uri">https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html</a> for contextual explanations.</p>
<h3 id="sec:distributed-distributional-ddpg-d4pg"><span class="header-section-number">4.4.3</span> Distributed Distributional DDPG (D4PG)</h3>
<p>Distributed Distributional DDPG <span class="citation" data-cites="Barth-Maron2018">(D4PG, Barth-Maron et al. <a href="#ref-Barth-Maron2018" role="doc-biblioref">2018</a>)</span> proposed recently several improvements on DDPG to make it more efficient:</p>
<ol type="1">
<li><p>The critic is trained using <strong>distributional learning</strong> <span class="citation" data-cites="Bellemare2017">(Bellemare, Dabney, and Munos <a href="#ref-Bellemare2017" role="doc-biblioref">2017</a>)</span> instead of classical Q-learning to improve the stability of learning in the actor (less variance). See Section <a href="#sec:distributional-learning">4.7</a> for more explanations.</p></li>
<li><p>The critic uses <strong>n-step</strong> returns instead of simple one-step TD returns as in A3C (Section <a href="#sec:advantage-actor-critic-methods">4.2</a>, <span class="citation" data-cites="Mnih2016">Mnih et al. (<a href="#ref-Mnih2016" role="doc-biblioref">2016</a>)</span>).</p></li>
<li><p><strong>Multiple Distributed Parallel Actors</strong> gather <span class="math inline">\((s, a, r, s&#39;)\)</span> transitions in parallel and write them to the same replay buffer.</p></li>
<li><p>The replay buffer uses <strong>Prioritized Experience Replay</strong> <span class="citation" data-cites="Schaul2015">(Schaul et al. <a href="#ref-Schaul2015" role="doc-biblioref">2015</a>)</span> to sample transitions based the information gain.</p></li>
</ol>
<!--PAGEBREAK-->
<h2 id="sec:natural-gradients"><span class="header-section-number">4.5</span> Natural Gradients</h2>
<p>The deep networks used as function approximators in the methods presented until now were all optimized (trained) using <strong>stochastic gradient descent</strong> (SGD) or any of its variants (RMSProp, Adam, etc). The basic idea is to change the parameters <span class="math inline">\(\theta\)</span> in the opposite direction of the gradient of the loss function (or the same direction as the policy gradient, in which case it is called gradient ascent), proportionally to a small learning rate <span class="math inline">\(\eta\)</span>:</p>
<p><span class="math display">\[
    \Delta \theta = - \eta \, \nabla_\theta \mathcal{L}(\theta)
\]</span></p>
<p>SGD is also called a <strong>steepest descent method</strong>: one searches for the smallest parameter change <span class="math inline">\(\Delta \theta\)</span> inducing the biggest negative change of the loss function. In classical supervised learning, this is what we want: we want to minimize the loss function as fast as possible, while keeping weight changes as small as possible, otherwise learning might become unstable (weight changes computed for a single minibatch might erase the changes made on previous minibatches). The main difficulty of supervised learning is to choose the right value for the learning rate: too high and learning is unstable; too low and learning takes forever.</p>
<p>In deep RL, we have an additional problem: the problem is not stationary (see Section <a href="#sec:limitations-of-deep-neural-networks-for-function-approximation">3.1</a>). In Q-learning, the target <span class="math inline">\(r(s, a, s&#39;) + \gamma \, \max_{a&#39;} Q_\theta(S&#39;, a&#39;)\)</span> is changing with <span class="math inline">\(\theta\)</span>. If the Q-values change a lot between two minibatches, the network will not get any stable target signal to learn from, and the policy will end up suboptimal. The trick is to use <strong>target networks</strong> to compute the target, which can be either an old copy of the current network (vanilla DQN), or a smoothed version of it (DDPG). Obviously, this introduces a bias (the targets are always wrong during training), but this bias converges to zero (after sufficient training, the targets will be almost correct), at the cost of a huge sample complexity.</p>
<p>Target networks cannot be used in <strong>on-policy</strong> methods, especially actor-critic architectures. As seen in Section <a href="#sec:off-policy-actor-critic">4.3</a>, the critic must learn from transitions recently generated by the actor (although importance sampling and the Retrace algorithm might help). The problem with on-policy methods is that they waste a lot of data: they always need fresh samples to learn from and never reuse past experiences. The policy gradient theorem shows why:</p>
<p><span class="math display">\[
    \nabla_\theta J(\theta) =  \mathbb{E}_{s \sim \rho_\theta, a \sim \pi_\theta}[\nabla_\theta \log \pi_\theta(s, a) \, Q^{\pi_\theta}(s, a))] \approx  \mathbb{E}_{s \sim \rho_\theta, a \sim \pi_\theta}[\nabla_\theta \log \pi_\theta(s, a) \, Q_\varphi(s, a))]
\]</span></p>
<p>If the policy <span class="math inline">\(\pi_\theta\)</span> changes a lot between two updates, the estimated Q-value <span class="math inline">\(Q_\varphi(s, a)\)</span> will represent the value of the action for a totally different policy, not the true Q-value <span class="math inline">\(Q^{\pi_\theta}(s, a)\)</span>. The estimated policy gradient will then be strongly biased and learning will be suboptimal. In other words, the actor should not change much faster than the critic, and vice versa. A naive solution would be to use a very small learning rate for the actor, but this just slows down learning (adding to the sample complexity) without solving the problem.</p>
<p>To solve the problem, we should actually do the opposite of the steepest descent: <em>search for the biggest parameter change <span class="math inline">\(\Delta \theta\)</span> inducing the smallest change in the policy, but in the right direction</em>. If the parameter change is high, the actor will learn a lot internally from each experience. But if the policy change is small between two updates (although the parameters have changed a lot), we might be able to reuse past experiences, as the targets will not be that wrong.</p>
<p>This is where <strong>natural gradients</strong> come into play, which are originally a statistical method to optimize over spaces of probability distributions, for example for variational inference. The idea to use natural gradients to train neural networks comes from <span class="citation" data-cites="Amari1998">Amari (<a href="#ref-Amari1998" role="doc-biblioref">1998</a>)</span>. <span class="citation" data-cites="Kakade2001">Kakade (<a href="#ref-Kakade2001" role="doc-biblioref">2001</a>)</span> applied natural gradients to policy gradient methods, while <span class="citation" data-cites="Peters2008">Peters and Schaal (<a href="#ref-Peters2008" role="doc-biblioref">2008</a>)</span> proposed a natural actor-critic algorithm for linear function approximators. The idea was adapted to deep RL by Schulman and colleagues, with Trust Region Policy Optimization <span class="citation" data-cites="Schulman2015">(TRPO, Schulman, Levine, et al. <a href="#ref-Schulman2015" role="doc-biblioref">2015</a>)</span> and Proximal Policy Optimization <span class="citation" data-cites="Schulman2017">(PPO, Schulman et al. <a href="#ref-Schulman2017" role="doc-biblioref">2017</a>)</span>, the latter gaining momentum over DDPG as the go-to method for continuous RL problems, particularly because of its smaller sample complexity and its robustness to hyperparameters.</p>
<h3 id="sec:principle-of-natural-gradients"><span class="header-section-number">4.5.1</span> Principle of natural gradients</h3>
<figure>
<img src="img/naturalgradient.png" alt="Figure 28: Euclidian distances in the parameter space do not represent well the statistical distance between probability distributions. The two Gaussians on the left (\mathcal{N}(0, 0.2) and \mathcal{N}(1, 0.2)) have the same Euclidian distance in the parameter space (d = \sqrt{(\mu_0 - \mu_1)^2+(\sigma_0 - \sigma_1)^2}) than the two Gaussians on the right (\mathcal{N}(0, 10) and \mathcal{N}(1, 10)). However, the Gaussians on the right are much more similar than the two on the left: if you have a single sample, you could not say from which distribution it comes for the Gaussians on the right, while it is obvious for the Gaussians on the left." id="fig:naturalgradient" style="width:80.0%" /><figcaption>Figure 28: Euclidian distances in the parameter space do not represent well the statistical distance between probability distributions. The two Gaussians on the left (<span class="math inline">\(\mathcal{N}(0, 0.2)\)</span> and <span class="math inline">\(\mathcal{N}(1, 0.2)\)</span>) have the same Euclidian distance in the parameter space (<span class="math inline">\(d = \sqrt{(\mu_0 - \mu_1)^2+(\sigma_0 - \sigma_1)^2}\)</span>) than the two Gaussians on the right (<span class="math inline">\(\mathcal{N}(0, 10)\)</span> and <span class="math inline">\(\mathcal{N}(1, 10)\)</span>). However, the Gaussians on the right are much more similar than the two on the left: if you have a single sample, you could not say from which distribution it comes for the Gaussians on the right, while it is obvious for the Gaussians on the left.</figcaption>
</figure>
<p>Consider the two Gaussian distributions in the left part of Fig. <a href="#fig:naturalgradient">28</a> (<span class="math inline">\(\mathcal{N}(0, 0.2)\)</span> and <span class="math inline">\(\mathcal{N}(1, 0.2)\)</span>) and the two on the right (<span class="math inline">\(\mathcal{N}(0, 10)\)</span> and <span class="math inline">\(\mathcal{N}(1, 10)\)</span>). In both cases, the distance in the Euclidian space of parameters <span class="math inline">\(d = \sqrt{(\mu_0 - \mu_1)^2+(\sigma_0 - \sigma_1)^2}\)</span> is the same between the two Gaussians. Obviously, the two distributions on the left are however further away from each other than the two on the the right. This indicates that the Euclidian distance in the parameter space (which is what <em>regular</em> gradients act on) is not a correct measurement of the statistical distance between two distributions (which what we want to minimize between two iterations of PG).</p>
<p>In statistics, a common measurement of the statistical distance between two distributions <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span> is the <strong>Kullback-Leibler (KL) divergence</strong> <span class="math inline">\(D_{KL}(p||q)\)</span>, also called relative entropy or information gain. It is defined as:</p>
<p><span class="math display">\[
    D_{KL}(p || q) = \mathbb{E}_{x \sim p} [\log \frac{p(x)}{q(x)}]  = \int p(x) \, \log \frac{p(x)}{q(x)} \, dx
\]</span></p>
<p>Its minimum is 0 when <span class="math inline">\(p=q\)</span> (as <span class="math inline">\(\log \frac{p(x)}{q(x)}\)</span> is then 0) and is positive otherwise. Minimizing the KL divergence is equivalent to “matching” two distributions. Note that supervised methods in machine learning can all be interpreted as a minimization of the KL divergence: if <span class="math inline">\(p(x)\)</span> represents the distribution of the data (the label of a sample <span class="math inline">\(x\)</span>) and <span class="math inline">\(q(x)\)</span> the one of the model (the prediction of a neural network for the same sample <span class="math inline">\(x\)</span>), supervised methods want the output distribution of the model to match the distribution of the data, i.e. make predictions that are the same as the labels. For generative models, this is for example at the core of <em>generative adversarial networks</em> <span class="citation" data-cites="Goodfellow2014 Arjovsky2017">(Goodfellow et al. <a href="#ref-Goodfellow2014" role="doc-biblioref">2014</a>; Arjovsky, Chintala, and Bottou <a href="#ref-Arjovsky2017" role="doc-biblioref">2017</a>)</span> or <em>variational autoencoders</em> <span class="citation" data-cites="Kingma2013">(Kingma and Welling <a href="#ref-Kingma2013" role="doc-biblioref">2013</a>)</span>.</p>
<p>The KL divergence is however not symmetrical (<span class="math inline">\(D_{KL}(p || q) \neq D_{KL}(q || p)\)</span>), so a more useful divergence is the symmetric KL divergence, also known as Jensen-Shannon (JS) divergence:</p>
<p><span class="math display">\[
    D_{JS}(p || q) = \frac{D_{KL}(p || q) + D_{KL}(q || p)}{2}
\]</span></p>
<p>Other forms of divergence measurements exist, such as the Wasserstein distance which improves generative adversarial networks <span class="citation" data-cites="Arjovsky2017">(Arjovsky, Chintala, and Bottou <a href="#ref-Arjovsky2017" role="doc-biblioref">2017</a>)</span>, but they are not relevant here. See <a href="https://www.alexirpan.com/2017/02/22/wasserstein-gan.html" class="uri">https://www.alexirpan.com/2017/02/22/wasserstein-gan.html</a> for more explanations.</p>
<p>We now have a global measurement of the similarity between two distributions on the whole input space, but which is hard to compute. How can we use it anyway in our optimization problem? As mentioned above, we search for the biggest parameter change <span class="math inline">\(\Delta \theta\)</span> inducing the smallest change in the policy. We need a metric linking changes in the parameters of the distribution (the weights of the network) to changes in the distribution itself. In other terms, we will apply gradient descent on the statistical manifold defined by the parameters rather than on the parameters themselves.</p>
<figure>
<img src="img/riemannian.png" alt="Figure 29: Naive illustration of the Riemannian metric. The Euclidian distance between p(x; \theta) and p(x; \theta + \Delta \theta) depends on the Euclidian distance between \theta and \theta + \Delta\theta, i.e. \Delta \theta. Riemannian metrics follow the geometry of the manifold to compute that distance, depending on its curvature. This figure is only for illustration: Riemannian metrics are purely local, \Delta \theta should be much smaller." id="fig:riemannian" style="width:50.0%" /><figcaption>Figure 29: Naive illustration of the Riemannian metric. The Euclidian distance between <span class="math inline">\(p(x; \theta)\)</span> and <span class="math inline">\(p(x; \theta + \Delta \theta)\)</span> depends on the Euclidian distance between <span class="math inline">\(\theta\)</span> and <span class="math inline">\(\theta + \Delta\theta\)</span>, i.e. <span class="math inline">\(\Delta \theta\)</span>. Riemannian metrics follow the geometry of the manifold to compute that distance, depending on its curvature. This figure is only for illustration: Riemannian metrics are purely local, <span class="math inline">\(\Delta \theta\)</span> should be much smaller.</figcaption>
</figure>
<p>Let’s consider a parameterized distribution <span class="math inline">\(p(x; \theta)\)</span> and its new value <span class="math inline">\(p(x; \theta + \Delta \theta)\)</span> after applying a small parameter change <span class="math inline">\(\Delta \theta\)</span>. As depicted on Fig. <a href="#fig:riemannian">29</a>, the Euclidian metric in the parameter space (<span class="math inline">\(||\theta + \Delta \theta - \theta||^2\)</span>) does not take the structure of the statistical manifold into account. We need to define a <strong>Riemannian metric</strong> which accounts locally for the curvature of the manifold between <span class="math inline">\(\theta\)</span> and <span class="math inline">\(\theta + \Delta \theta\)</span>. The Riemannian distance is defined by the dot product:</p>
<p><span class="math display">\[
    ||\Delta \theta||^2 = &lt; \Delta \theta , F(\theta) \, \Delta \theta &gt;
\]</span></p>
<p>where <span class="math inline">\(F(\theta)\)</span> is called the Riemannian metric tensor and is an inner product on the tangent space of the manifold at the point <span class="math inline">\(\theta\)</span>.</p>
<p>When using the symmetric KL divergence to measure the distance between two distributions, the corresponding Riemannian metric is the <strong>Fisher Information Matrix</strong> (FIM), defined as the Hessian matrix of the KL divergence around <span class="math inline">\(\theta\)</span>, i.e. the matrix of second order derivatives w.r.t the elements of <span class="math inline">\(\theta\)</span>. See <a href="https://stats.stackexchange.com/questions/51185/connection-between-fisher-metric-and-the-relative-entropy" class="uri">https://stats.stackexchange.com/questions/51185/connection-between-fisher-metric-and-the-relative-entropy</a> and <a href="https://wiseodd.github.io/techblog/2018/03/14/natural-gradient/" class="uri">https://wiseodd.github.io/techblog/2018/03/14/natural-gradient/</a> for an explanation of the link between the Fisher matrix and KL divergence.</p>
<p>The Fisher information matrix is defined as the Hessian of the KL divergence around <span class="math inline">\(\theta\)</span>, i.e. how the manifold locally changes around <span class="math inline">\(\theta\)</span>:</p>
<p><span class="math display">\[
    F(\theta) = \nabla^2 D_{JS}(p(x; \theta) || p(x; \theta + \Delta \theta))|_{\Delta \theta = 0}
\]</span></p>
<p>which necessitates to compute second order derivatives which are very complex and slow to obtain, especially when there are many parameters <span class="math inline">\(\theta\)</span> (the weights of the NN). Fortunately, it also has a simpler form which only depends on the outer product between the gradients of the log-likelihoods:</p>
<p><span class="math display">\[
    F(\theta) = \mathbb{E}_{x \sim p(x, \theta)}[ \nabla \log p(x; \theta)  (\nabla \log p(x; \theta))^T]
\]</span></p>
<p>which is something we can easily sample and compute.</p>
<p>Why is it useful? The Fisher Information matrix allows to <strong>locally</strong> approximate (for small <span class="math inline">\(\Delta \theta\)</span>) the KL divergence between the two close distributions (using a second-order Taylor series expansion):</p>
<p><span class="math display">\[
    D_{JS}(p(x; \theta) || p(x; \theta + \Delta \theta)) \approx \Delta \theta^T \, F(\theta) \, \Delta \theta
\]</span></p>
<p>The KL divergence is then locally quadratic, which means that the update rules obtained when minimizing the KL divergence with gradient descent will be linear. Suppose we want to minimize a loss function <span class="math inline">\(L\)</span> parameterized by <span class="math inline">\(\theta\)</span> and depending on the distribution <span class="math inline">\(p\)</span>. <strong>Natural gradient descent</strong> <span class="citation" data-cites="Amari1998">(Amari <a href="#ref-Amari1998" role="doc-biblioref">1998</a>)</span> attempts to move along the statistical manifold defined by <span class="math inline">\(p\)</span> by correcting the gradient of <span class="math inline">\(L(\theta)\)</span> using the local curvature of the KL-divergence surface, i.e. moving some given distance in the direction <span class="math inline">\(\tilde{\nabla_\theta} L(\theta)\)</span>:</p>
<p><span class="math display">\[
    \tilde{\nabla_\theta} L(\theta) = F(\theta)^{-1} \, \nabla_\theta L(\theta)
\]</span></p>
<p><span class="math inline">\(\tilde{\nabla_\theta} L(\theta)\)</span> is the <strong>natural gradient</strong> of <span class="math inline">\(L(\theta)\)</span>. Natural gradient descent simply takes steps in this direction:</p>
<p><span class="math display">\[
    \Delta \theta = - \eta \, \tilde{\nabla_\theta} L(\theta)
\]</span></p>
<p>When the manifold is not curved (<span class="math inline">\(F(\theta)\)</span> is the identity matrix), natural gradient descent is the regular gradient descent.</p>
<p>But what is the advantage of natural gradients? The problem with regular gradient descent is that it relies on a fixed learning rate. In regions where the loss function is flat (a plateau), the gradient will be almost zero, leading to very slow improvements. Because the natural gradient depends on the inverse of the curvature (Fisher), the magnitude of the gradient will be higher in flat regions, leading to bigger steps, and smaller in very steep regions (around minima). Natural GD therefore converges faster and better than regular GD.</p>
<p>Natural gradient descent is a generic optimization method, it can for example be used to train more efficiently deep networks in supervised learning <span class="citation" data-cites="Pascanu2013">(Pascanu and Bengio <a href="#ref-Pascanu2013" role="doc-biblioref">2013</a>)</span>. Its main drawback is the necessity to inverse the Fisher information matrix, whose size depends on the number of free parameters (if you have N weights in the NN, you need to inverse a NxN matrix). Several approximations allows to remediate to this problem, for example Conjugate Gradients or Kronecker-Factored Approximate Curvature (K-FAC).</p>
<p><strong>Additional resources</strong> to understand natural gradients:</p>
<ul>
<li><a href="http://andymiller.github.io/2016/10/02/natural_gradient_bbvi.html" class="uri">http://andymiller.github.io/2016/10/02/natural_gradient_bbvi.html</a></li>
<li><a href="https://hips.seas.harvard.edu/blog/2013/01/25/the-natural-gradient/" class="uri">https://hips.seas.harvard.edu/blog/2013/01/25/the-natural-gradient/</a></li>
<li><a href="http://kvfrans.com/what-is-the-natural-gradient-and-where-does-it-appear-in-trust-region-policy-optimization" class="uri">http://kvfrans.com/what-is-the-natural-gradient-and-where-does-it-appear-in-trust-region-policy-optimization</a></li>
<li><a href="https://wiseodd.github.io/techblog/2018/03/14/natural-gradient/" class="uri">https://wiseodd.github.io/techblog/2018/03/14/natural-gradient/</a></li>
<li>A tutorial by John Schulman (OpenAI) <a href="https://www.youtube.com/watch?v=xvRrgxcpaHY" class="uri">https://www.youtube.com/watch?v=xvRrgxcpaHY</a></li>
<li>A blog post on the related Hessian-free optimization and conjuguate gradients <a href="http://andrew.gibiansky.com/blog/machine-learning/hessian-free-optimization/" class="uri">http://andrew.gibiansky.com/blog/machine-learning/hessian-free-optimization/</a></li>
<li>K-FAC: <a href="https://syncedreview.com/2017/03/25/optimizing-neural-networks-using-structured-probabilistic-models/" class="uri">https://syncedreview.com/2017/03/25/optimizing-neural-networks-using-structured-probabilistic-models/</a></li>
<li>Conjugate gradients: <a href="https://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf" class="uri">https://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf</a></li>
</ul>
<p><strong>Note:</strong> Natural gradients can also be used to train DQN architectures, resulting in more efficient and stable learning behaviors <span class="citation" data-cites="Knight2018">(Knight and Lerner <a href="#ref-Knight2018" role="doc-biblioref">2018</a>)</span>.</p>
<h3 id="sec:natural-policy-gradient-and-natural-actor-critic-nac"><span class="header-section-number">4.5.2</span> Natural policy gradient and Natural Actor Critic (NAC)</h3>
<p><span class="citation" data-cites="Kakade2001">Kakade (<a href="#ref-Kakade2001" role="doc-biblioref">2001</a>)</span> applied the principle of natural gradients proposed by <span class="citation" data-cites="Amari1998">Amari (<a href="#ref-Amari1998" role="doc-biblioref">1998</a>)</span> to the policy gradient theorem:</p>
<p><span class="math display">\[
    \nabla_\theta J(\theta) =  \mathbb{E}_{s \sim \rho_\theta, a \sim \pi_\theta}[\nabla_\theta \log \pi_\theta(s, a) \, Q^{\pi_\theta}(s, a)]
\]</span></p>
<p>This <em>regular</em> gradient does not take into account the underlying structure of the policy distribution <span class="math inline">\(\pi(s, a)\)</span>. The Fisher information matrix for the policy is defined by:</p>
<p><span class="math display">\[
    F(\theta) = \mathbb{E}_{s \sim \rho_\theta, a \sim \pi_\theta}[ \nabla \log \pi_\theta(s, a)  (\nabla \log \pi_\theta(s, a))^T]
\]</span></p>
<p>The natural policy gradient is simply:</p>
<p><span class="math display">\[
    \tilde{\nabla}_\theta J(\theta) = F(\theta)^{-1} \, \nabla_\theta J(\theta)  = \mathbb{E}_{s \sim \rho_\theta, a \sim \pi_\theta}[ F(\theta)^{-1} \,  \nabla_\theta \log \pi_\theta(s, a) \, Q^{\pi_\theta}(s, a)]
\]</span></p>
<p><span class="citation" data-cites="Kakade2001">Kakade (<a href="#ref-Kakade2001" role="doc-biblioref">2001</a>)</span> also shows that you can replace the true Q-value <span class="math inline">\(Q^{\pi_\theta}(s, a)\)</span> with a compatible approximation <span class="math inline">\(Q_\varphi(s, a)\)</span> (as long as it minimizes the quadratic error) and still obtained an unbiased natural gradient. An important theoretical result is that policy improvement is guaranteed with natural gradients: the new policy after an update is always better (more expected returns) than before. He experimented this new rule on various simple MDPs and observed drastic improvements over vanilla PG.</p>
<p><span class="citation" data-cites="Peters2008">Peters and Schaal (<a href="#ref-Peters2008" role="doc-biblioref">2008</a>)</span> extended on the work of <span class="citation" data-cites="Kakade2001">Kakade (<a href="#ref-Kakade2001" role="doc-biblioref">2001</a>)</span> to propose the natural actor-critic (NAC). The exact derivations would be too complex to summarize here, but the article is an interesting read. He particularly reviews the progress at that time on policy gradient for its use in robotics. He showed that the <span class="math inline">\(F(\theta)\)</span>) is a true Fisher information matrix even when using sampled episodes, and derived a baseline <span class="math inline">\(b\)</span> to reduce the variance of the natural policy gradient. He demonstrated the power of this algorithm by letting a robot learning motor primitives for baseball.</p>
<h3 id="sec:trust-region-policy-optimization-trpo"><span class="header-section-number">4.5.3</span> Trust Region Policy Optimization (TRPO)</h3>
<h4 id="sec:principle" class="unnumbered">Principle</h4>
<p><span class="citation" data-cites="Schulman2015">Schulman, Levine, et al. (<a href="#ref-Schulman2015" role="doc-biblioref">2015</a>)</span> extended the idea of natural gradients to allow their use for non-linear function approximators (e.g. deep networks), as the previous algorithms only worked efficiently for linear approximators. The proposed algorithm, Trust Region Policy Optimization (TRPO), has now been replaced in practice by Proximal Policy Optimization (PPO, see next section) but its novel ideas are important to understand already.</p>
<p>Let’s note the expected return of a policy <span class="math inline">\(\pi\)</span> as:</p>
<p><span class="math display">\[
    \eta(\pi) = \mathbb{E}_{s \sim \rho_\pi, a \sim \pi}[\sum_{t=0}^\infty \gamma^t \, r(s_t, a_t, s_{t+1})]
\]</span></p>
<p>where <span class="math inline">\(\rho_\pi\)</span> is the discounted visitation frequency distribution (the probability that a state <span class="math inline">\(s\)</span> will be visited at some point in time by the policy <span class="math inline">\(\pi\)</span>):</p>
<p><span class="math display">\[
    \rho_\pi(s) = P(s_0=s) + \gamma \, P(s_1=s) + \gamma^2 \, P(s_2=s) + \ldots
\]</span></p>
<p><span class="citation" data-cites="Kakade2002">Kakade and Langford (<a href="#ref-Kakade2002" role="doc-biblioref">2002</a>)</span> had shown that it is possible to relate the expected return of two policies <span class="math inline">\(\pi_\theta\)</span> and <span class="math inline">\(\pi_{\theta_\text{old}}\)</span> using advantages (omitting <span class="math inline">\(\pi\)</span> in the notations):</p>
<p><span class="math display">\[
    \eta(\theta) = \eta(\theta_\text{old}) + \mathbb{E}_{s \sim \rho_{\pi_\theta}, a \sim \pi_\theta} [A_{\pi_{\theta_\text{old}}}(s, a)]
\]</span></p>
<p>The advantage <span class="math inline">\(A_{\pi_{\theta_\text{old}}}(s, a)\)</span> denotes the change in the expected return obtained after <span class="math inline">\((s, a)\)</span> when using the new policy <span class="math inline">\(\pi_\theta\)</span>, in comparison to the one obtained with the old policy <span class="math inline">\(\pi_{\theta_\text{old}}\)</span>. While this formula seems interesting (it measures how good the new policy is with regard to the average performance of the old policy, so we could optimize directly), it is difficult to estimate as the mathematical expectation depends on state-action pairs generated by the new policy : <span class="math inline">\(s \sim \rho_{\pi_\theta}, a \sim \pi_\theta\)</span>.</p>
<p><span class="citation" data-cites="Schulman2015">Schulman, Levine, et al. (<a href="#ref-Schulman2015" role="doc-biblioref">2015</a>)</span> propose an approximation to this formula, by considering that if the two policies <span class="math inline">\(\pi_\theta\)</span> and <span class="math inline">\(\pi_{\theta_\text{old}}\)</span> are not very different from another, one can sample the states from the old distribution:</p>
<p><span class="math display">\[
    \eta(\theta) \approx \eta(\theta_\text{old}) + \mathbb{E}_{s \sim \rho_{\pi_{\theta_\text{old}}}, a \sim \pi_\theta} [A_{\pi_{\theta_\text{old}}}(s, a)]
\]</span></p>
<p>One can already recognize the main motivation behind natural gradients: finding a weight update that moves the policy in the right direction (getting more rewards) while keeping the change in the policy distribution as small as possible (to keep the assumption correct).</p>
<p>Let’s now define the following objective function:</p>
<p><span class="math display">\[
    J_{\theta_\text{old}}(\theta) = \eta(\theta_\text{old}) + \mathbb{E}_{s \sim \rho_{\pi_{\theta_\text{old}}}, a \sim \pi_\theta} [A_{\pi_{\theta_\text{old}}}(s, a)]
\]</span></p>
<p>It is easy to see that <span class="math inline">\(J_{\theta_\text{old}}(\theta_\text{old}) = \eta(\theta_\text{old})\)</span> by definition of the advantage, and that its gradient w.r.t to <span class="math inline">\(\theta\)</span> taken in <span class="math inline">\(\theta_\text{old}\)</span> is the same as the one of <span class="math inline">\(\eta(\theta_\text{old})\)</span>:</p>
<p><span class="math display">\[
    \nabla_\theta J_{\theta_\text{old}}(\theta)|_{\theta = \theta_\text{old}} = \nabla_\theta \eta(\theta)|_{\theta = \theta_\text{old}}
\]</span></p>
<p>This means that, at least locally, one maximization step of <span class="math inline">\(J_{\theta_\text{old}}(\theta)\)</span> goes in the same direction as maximizing <span class="math inline">\(\eta(\theta)\)</span> if we do not go too far. <span class="math inline">\(J\)</span> is called a <strong>surrogate objective function</strong>: it is not what we want to optimize, but it leads to the same result. TRPO belongs to the class of <strong>minorization-majorization</strong> algorithms (MM, we first find a local lower bound and then maximize it, iteratively).</p>
<p>Let’s now suppose that we can find its maximum, i.e. a policy <span class="math inline">\(\pi&#39;\)</span> that maximizes the advantage of each state-action pair over <span class="math inline">\(\pi_{\theta_\text{old}}\)</span>. There would be no guarantee that <span class="math inline">\(\pi&#39;\)</span> and <span class="math inline">\(\pi_{\theta_\text{old}}\)</span> are close enough so that the assumption stands. We could therefore make only a small step in its direction and hope for the best:</p>
<p><span id="eq:linesearch"><span class="math display">\[
    \pi_\theta(s, a) = (1-\alpha) \, \pi_{\theta_\text{old}}(s, a) + \alpha \, \pi&#39;(s,a)
\qquad(14)\]</span></span></p>
<p>This is the conservative policy iteration method of <span class="citation" data-cites="Kakade2002">Kakade and Langford (<a href="#ref-Kakade2002" role="doc-biblioref">2002</a>)</span>, where a bound on the difference between <span class="math inline">\(\eta(\pi_{\theta_\text{old}})\)</span> and <span class="math inline">\(J_{\theta_\text{old}}(\theta)\)</span> can be derived.</p>
<p><span class="citation" data-cites="Schulman2015">Schulman, Levine, et al. (<a href="#ref-Schulman2015" role="doc-biblioref">2015</a>)</span> propose to penalize instead the objective function by the KL divergence between the new and old policies. There are basically two ways to penalize an optimization problem:</p>
<ol type="1">
<li>Adding a hard constraint on the KL divergence, leading to a constrained optimization problem (where Lagrange methods can be applied):</li>
</ol>
<p><span class="math display">\[
    \text{maximize}_\theta \qquad J_{\theta_\text{old}}(\theta) \\
    \qquad \text{subject to} \qquad D_{KL}(\pi_{\theta_\text{old}} || \pi_\theta) \leq \delta
\]</span></p>
<ol start="2" type="1">
<li>Regularizing the objective function with the KL divergence:</li>
</ol>
<p><span class="math display">\[
    \text{maximize}_\theta \qquad L(\theta) = J_{\theta_\text{old}}(\pi_\theta) - C \,  D_{KL}(\pi_{\theta_\text{old}} || \pi_\theta)
\]</span></p>
<p>In the first case, we force the KL divergence to stay below a certain threshold. In the second case, we penalize solutions that would maximize <span class="math inline">\(J_{\theta_\text{old}}(\theta)\)</span> but would be too different from the previous policy. In both cases, we want to find a policy <span class="math inline">\(\pi_\theta\)</span> maximizing the expected return (the objective), but which is still close (in terms of KL divergence) from the current one. Both methods are however sensible to the choice of the parameters <span class="math inline">\(\delta\)</span> and <span class="math inline">\(C\)</span>.</p>
<p>Formally, the KL divergence <span class="math inline">\(D_{KL}(\pi_{\theta_\text{old}} || \pi_\theta)\)</span> should be the maximum KL divergence over the state space:</p>
<p><span class="math display">\[
    D^\text{max}_{KL}(\pi_{\theta_\text{old}} || \pi_\theta) = \max_s D_{KL}(\pi_{\theta_\text{old}}(s, .) || \pi_\theta(s, .))
\]</span></p>
<p>This maximum KL divergence over the state space would be very hard to compute. Empirical evaluations showed however that it is safe to use the mean KL divergence, or even to sample it:</p>
<p><span class="math display">\[
    \bar{D}_{KL}(\pi_{\theta_\text{old}} || \pi_\theta) = \mathbb{E}_s [D_{KL}(\pi_{\theta_\text{old}}(s, .) || \pi_\theta(s, .))] \approx \frac{1}{N} \sum_{i=1}^N D_{KL}(\pi_{\theta_\text{old}}(s_i, .) || \pi_\theta(s_i, .))
\]</span></p>
<h4 id="sec:trust-regions" class="unnumbered">Trust regions</h4>
<figure>
<img src="img/trustregion.png" alt="Figure 30: Graphical illustration of trust regions. From the current parameters \theta_\text{old}, we search for the maximum \theta^* of the real objective \eta(\theta). The unconstrained objective J_{\theta_\text{old}}(\theta) is locally similar to \eta(\theta) but quickly diverge as \pi_\theta and \pi_{\theta_\text{old}} become very different. The surrogate objective L(\theta) = J_{\theta_\text{old}} (\theta) - C \, D_{KL}(\pi_{\theta_\text{old}} || \pi_\theta) is always smaller than \eta(\theta) and has a maximum close to \theta^* which keeps \pi_\theta and \pi_{\theta_\text{old}} close from each other in terms of KL divergence. The region around \theta_\text{old} where big optimization steps can be taken without changing the policy too much is called the trust region." id="fig:trustregion" style="width:70.0%" /><figcaption>Figure 30: Graphical illustration of trust regions. From the current parameters <span class="math inline">\(\theta_\text{old}\)</span>, we search for the maximum <span class="math inline">\(\theta^*\)</span> of the real objective <span class="math inline">\(\eta(\theta)\)</span>. The unconstrained objective <span class="math inline">\(J_{\theta_\text{old}}(\theta)\)</span> is locally similar to <span class="math inline">\(\eta(\theta)\)</span> but quickly diverge as <span class="math inline">\(\pi_\theta\)</span> and <span class="math inline">\(\pi_{\theta_\text{old}}\)</span> become very different. The surrogate objective <span class="math inline">\(L(\theta) = J_{\theta_\text{old}} (\theta) - C \, D_{KL}(\pi_{\theta_\text{old}} || \pi_\theta)\)</span> is always smaller than <span class="math inline">\(\eta(\theta)\)</span> and has a maximum close to <span class="math inline">\(\theta^*\)</span> which keeps <span class="math inline">\(\pi_\theta\)</span> and <span class="math inline">\(\pi_{\theta_\text{old}}\)</span> close from each other in terms of KL divergence. The region around <span class="math inline">\(\theta_\text{old}\)</span> where big optimization steps can be taken without changing the policy too much is called the trust region.</figcaption>
</figure>
<p>Before diving further into how these optimization problems can be solved, let’s wonder why the algorithm is called <strong>trust region policy optimization</strong> using the regularized objective. Fig. <a href="#fig:trustregion">30</a> illustrates the idea. The “real” objective function <span class="math inline">\(\eta(\theta)\)</span> should be maximized (with gradient descent or similar) starting from the parameters <span class="math inline">\(\theta_\text{old}\)</span>. We cannot estimate the objective function directly, so we build a surrogate objective function <span class="math inline">\(L(\theta) = J_{\theta_\text{old}} (\theta) - C \, D_{KL}(\pi_{\theta_\text{old}} || \pi_\theta)\)</span>. We know that:</p>
<ol type="1">
<li>The two objectives have the same value in <span class="math inline">\(\theta_\text{old}\)</span>: <span class="math display">\[L(\theta_\text{old}) = J_{\theta_\text{old}}(\theta_\text{old}) - C \,  D_{KL}(\pi_{\theta_\text{old}} || \pi_{\theta_\text{old}}) = \eta(\theta_\text{old})\]</span></li>
<li>Their gradient w.r.t <span class="math inline">\(\theta\)</span> are the same in <span class="math inline">\(\theta_\text{old}\)</span>: <span class="math display">\[\nabla_\theta L(\theta)|_{\theta = \theta_\text{old}} = \nabla_\theta \eta(\theta)|_{\theta = \theta_\text{old}}\]</span></li>
<li>The surrogate objective is always smaller than the real objective, as the KL divergence is positive: <span class="math display">\[\eta(\theta) \geq J_{\theta_\text{old}}(\theta) - C \,  D_{KL}(\pi_{\theta_\text{old}} || \pi_\theta)\]</span>.</li>
</ol>
<p>Under these conditions, the surrogate objective is also called a <strong>lower bound</strong> of the primary objective. The interesting fact is that the value of <span class="math inline">\(\theta\)</span> that maximizes <span class="math inline">\(L(\theta)\)</span> is at the same time:</p>
<ul>
<li>A big step in the parameter space towards the maximum of <span class="math inline">\(\eta(\theta)\)</span>, as <span class="math inline">\(\theta\)</span> and <span class="math inline">\(\theta_\text{old}\)</span> can be very different.</li>
<li>A small step in the policy distribution space, as the KL divergence between the previous and the new policies is kept small.</li>
</ul>
<p>Exactly what we needed! The parameter region around <span class="math inline">\(\theta_\text{old}\)</span> where the KL divergence s kept small is called the <strong>trust region</strong>. This means that we can safely take big optimization steps (e.g. with a high learning rate or even analytically) without risking to violate the initial assumptions.</p>
<h4 id="sec:sample-based-formulation" class="unnumbered">Sample-based formulation</h4>
<p>Although the theoretical proofs in <span class="citation" data-cites="Schulman2015">Schulman, Levine, et al. (<a href="#ref-Schulman2015" role="doc-biblioref">2015</a>)</span> used the regularized optimization method, the practical implementation uses the constrained optimization problem:</p>
<p><span class="math display">\[
    \text{maximize}_\theta \qquad J_{\theta_\text{old}}(\theta) = \eta(\theta_\text{old}) + \mathbb{E}_{s \sim \rho_{\pi_{\theta_\text{old}}}, a \sim \pi_\theta} [A_{\pi_{\theta_\text{old}}}(s, a)] \\
    \qquad \text{subject to} \qquad D_{KL}(\pi_{\theta_\text{old}} || \pi_\theta) \leq \delta
\]</span></p>
<p>The first thing to notice is that <span class="math inline">\(\eta(\theta_\text{old})\)</span> does not depend on <span class="math inline">\(\theta\)</span>, so it is constant in the optimization problem. We only need to maximize the advantage of the actions taken by <span class="math inline">\(\pi_\theta\)</span> in each state visited by <span class="math inline">\(\pi_{\theta_\text{old}}\)</span>. The problem is that <span class="math inline">\(\pi_\theta\)</span> is what we search, so we can not sample actions from it. The solution is to use <strong>importance sampling</strong> (see Section <a href="#sec:importance-sampling">4.3.1</a>) to allow sampling actions from <span class="math inline">\(\pi_{\theta_\text{old}}\)</span>. This is possible as long as we correct the objective with the importance sampling weight:</p>
<p><span class="math display">\[
    \text{maximize}_\theta \qquad \mathbb{E}_{s \sim \rho_{\pi_{\theta_\text{old}}}, a \sim \pi_{\theta_\text{old}}} [\frac{\pi_\theta(s, a)}{\pi_{\theta_\text{old}}(s, a)} \, A_{\pi_{\theta_\text{old}}}(s, a)] \\
    \qquad \text{subject to} \qquad D_{KL}(\pi_{\theta_\text{old}} || \pi_\theta) \leq \delta
\]</span></p>
<p>Now that states and actions are generated by the old policy, we can safely sample many trajectories using <span class="math inline">\(\pi_{\theta_\text{old}}\)</span> (<span class="citation" data-cites="Schulman2015">Schulman, Levine, et al. (<a href="#ref-Schulman2015" role="doc-biblioref">2015</a>)</span> proposes two methods called single path and Vine, but we ignore it here), compute the advantages of all state-action pairs (using real rewards along the trajectories), form the surrogate objective function and optimize it using second-order optimization methods.</p>
<p>One last thing to notice is that the advantages <span class="math inline">\(A_{\pi_{\theta_\text{old}}}(s, a) = Q_{\pi_{\theta_\text{old}}}(s, a) - V_{\pi_{\theta_\text{old}}}(s)\)</span> depend on the value of the states encountered by <span class="math inline">\(\pi_{\theta_\text{old}}\)</span>. The state values do not depend on the policies, they are constant for each optimization step, so they can also be safely removed:</p>
<p><span class="math display">\[
    \text{maximize}_\theta \qquad \mathbb{E}_{s \sim \rho_{\pi_{\theta_\text{old}}}, a \sim \pi_{\theta_\text{old}}} [\frac{\pi_\theta(s, a)}{\pi_{\theta_\text{old}}(s, a)} \, Q_{\pi_{\theta_\text{old}}}(s, a)] \\
    \qquad \text{subject to} \qquad D_{KL}(\pi_{\theta_\text{old}} || \pi_\theta) \leq \delta
\]</span></p>
<p>Here we go, that’s TRPO. It could seem a bit disappointing to come up with such a simple formulation (find a policy which maximizes the Q-value of sampled actions while being not too different from the previous one) after so many mathematical steps, but that is also the beauty of it: not only it works, but it is guaranteed to work. With TRPO, each optimization step brings the policy closer from an optimum, what is called <strong>monotonic improvement</strong>.</p>
<h4 id="sec:practical-implementation" class="unnumbered">Practical implementation</h4>
<p>Now, how do we solve the constrained optimization problem? And what is the link with natural gradients?</p>
<p>To solve constrained optimization problems, we can form a Lagrange function with an additional parameter <span class="math inline">\(\lambda\)</span> and search for its maximum:</p>
<p><span class="math display">\[
    \mathcal{L}(\theta, \lambda) = J_{\theta_\text{old}}(\theta)  - \lambda \, (D_{KL}(\pi_{\theta_\text{old}} || \pi_\theta) - \delta)
\]</span></p>
<p>Notice how close the Lagrange function is from the regularized problem used in the theory. We can form a first-order approximation of <span class="math inline">\(J_{\theta_\text{old}}(\theta)\)</span>:</p>
<p><span class="math display">\[
    J_{\theta_\text{old}}(\theta) = \nabla_\theta J_{\theta_\text{old}}(\theta) \, (\theta- \theta_\text{old})
\]</span></p>
<p>as <span class="math inline">\(J_{\theta_\text{old}}(\theta_\text{old}) = 0\)</span>. <span class="math inline">\(g = \nabla_\theta J_{\theta_\text{old}}(\theta)\)</span> is the now familiar <strong>policy gradient</strong> with importance sampling. Higher-order terms do not matter, as they are going to be dominated by the KL divergence term.</p>
<p>As in Section <a href="#sec:principle-of-natural-gradients">4.5.1</a>, we will use a second-order approximation of the KL divergence term using the Fisher Information Matrix:</p>
<p><span class="math display">\[
    D_{KL}(\pi_{\theta_\text{old}} || \pi_\theta) = (\theta- \theta_\text{old})^T \, F(\theta_\text{old}) \,  (\theta- \theta_\text{old})
\]</span></p>
<p>We get the following Lagrangian function:</p>
<p><span class="math display">\[
    \mathcal{L}(\theta, \lambda) = \nabla_\theta J_{\theta_\text{old}}(\theta) \, (\theta- \theta_\text{old})  - \lambda \, (\theta- \theta_\text{old})^T \, F(\theta_\text{old}) \,  (\theta- \theta_\text{old})
\]</span></p>
<p>which is quadratic in <span class="math inline">\(\Delta \theta = \theta- \theta_\text{old}\)</span>. It has therefore a unique maximum, characterized by a first-order derivative equal to 0:</p>
<p><span class="math display">\[
    \nabla_\theta J_{\theta_\text{old}}(\theta) = \lambda \, F(\theta_\text{old}) \,  \Delta \theta
\]</span></p>
<p>or:</p>
<p><span class="math display">\[
    \Delta \theta  = \frac{1}{\lambda} \, F(\theta_\text{old})^{-1} \,  \nabla_\theta J_{\theta_\text{old}}(\theta)
\]</span></p>
<p>which is the <strong>natural gradient descent</strong>! The size of the step <span class="math inline">\(\frac{1}{\lambda}\)</span> still has to be determined, but it can also be replaced by a fixed hyperparameter.</p>
<p>The main problem is now to compute and inverse the Fisher information matrix, which is quadratic with the number of parameters <span class="math inline">\(\theta\)</span>, i.e. with the number of weights in the NN. <span class="citation" data-cites="Schulman2015">Schulman, Levine, et al. (<a href="#ref-Schulman2015" role="doc-biblioref">2015</a>)</span> proposes to used <strong>conjugate gradients</strong> to iteratively approximate the Fisher, a second-order method which will not be presented here (see <a href="https://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf" class="uri">https://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf</a> for a detailed introduction). After the conjugate gradient optimization step, the constraint <span class="math inline">\(D_{KL}(\pi_{\theta_\text{old}} || \pi_\theta) \leq \delta\)</span> is however not ensured anymore, so a line search is made as in Eq. <a href="#eq:linesearch">14</a> until that criteria is met.</p>
<h4 id="sec:summary" class="unnumbered">Summary</h4>
<p>TRPO is a policy gradient method using natural gradients to monotonically improve the expected return associated to the policy. As a minorization-maximization (MM) method, it uses a surrogate objective function (a lower bound on the expected return) to iteratively change the parameters of the policy using large steps, but without changing the policy too much (as measured by the KL divergence). Its main advantage over DDPG is that it is much less sensible to the choice of the learning rate.</p>
<p>However, it has several limitations:</p>
<ul>
<li>It is hard to use with neural networks having multiple outputs (e.g. the policy and the value function, as in actor-critic methods) as natural gradients are dependent on the policy distribution and its relationship with the parameters.</li>
<li>It works well when the NN has only fully-connected layers, but empirically performs poorly on tasks requiring convolutional layers or recurrent layers.</li>
<li>The use of conjugate gradients makes the implementation much more complicated and less flexible than regular SGD.</li>
</ul>
<p><strong>Additional resources</strong></p>
<ul>
<li><a href="http://178.79.149.207/posts/trpo.html" class="uri">http://178.79.149.207/posts/trpo.html</a></li>
<li><a href="https://towardsdatascience.com/introduction-to-various-reinforcement-learning-algorithms-part-ii-trpo-ppo-87f2c5919bb9" class="uri">https://towardsdatascience.com/introduction-to-various-reinforcement-learning-algorithms-part-ii-trpo-ppo-87f2c5919bb9</a></li>
<li><a href="https://medium.com/@sanketgujar95/trust-region-policy-optimization-trpo-and-proximal-policy-optimization-ppo-e6e7075f39ed" class="uri">https://medium.com/@sanketgujar95/trust-region-policy-optimization-trpo-and-proximal-policy-optimization-ppo-e6e7075f39ed</a></li>
<li><a href="https://www.depthfirstlearning.com/2018/TRPO" class="uri">https://www.depthfirstlearning.com/2018/TRPO</a></li>
<li><a href="http://rll.berkeley.edu/deeprlcoursesp17/docs/lec5.pdf" class="uri">http://rll.berkeley.edu/deeprlcoursesp17/docs/lec5.pdf</a></li>
</ul>
<h3 id="sec:proximal-policy-optimization-ppo"><span class="header-section-number">4.5.4</span> Proximal Policy Optimization (PPO)</h3>
<p>Proximal Policy Optimization (PPO) was proposed by <span class="citation" data-cites="Schulman2017">Schulman et al. (<a href="#ref-Schulman2017" role="doc-biblioref">2017</a>)</span> to overcome the problems of TRPO (complexity, inability to share parameters or to use complex NN architectures) while increasing the range of tasks learnable by the system (compared to DQN) and improving the sample complexity (compared to online PG methods, which perform only one update per step).</p>
<p>For that, they investigated various surrogate objectives (lower bounds) that could be solved using first-order optimization techniques (gradient descent). Let’s rewrite the surrogate loss of TRPO in the following manner:</p>
<p><span class="math display">\[
    L^\text{CPI}(\theta) = \mathbb{E}_{t} [\frac{\pi_\theta(s_t, a_t)}{\pi_{\theta_\text{old}}(s_t, a_t)} \, A_{\pi_{\theta_\text{old}}}(s_t, a_t)] = \mathbb{E}_{t} [\rho_t(\theta) \, A_{\pi_{\theta_\text{old}}}(s_t, a_t)]
\]</span></p>
<p>by making the dependency over time explicit and noting the importance sampling weight <span class="math inline">\(\rho_t(\theta)\)</span>. The superscript CPI refers to conservative policy iteration <span class="citation" data-cites="Kakade2002">(Kakade and Langford <a href="#ref-Kakade2002" role="doc-biblioref">2002</a>)</span>. Without a constraint, the maximization of <span class="math inline">\(L^\text{CPI}\)</span> would lead to an excessively large policy updates. The authors searched how to modify the objective, in order to penalize changes to the policy that make <span class="math inline">\(\rho_t(\theta)\)</span> very different from 1, i.e. where the KL divergence between the new and old policies would become high. They ended up with the following surrogate loss:</p>
<p><span class="math display">\[
    L^\text{CLIP}(\theta) = \mathbb{E}_{t} [ \min (\rho_t(\theta) \, A_{\pi_{\theta_\text{old}}}(s_t, a_t), \text{clip}(\rho_t(\theta) , 1- \epsilon, 1+\epsilon) \,  A_{\pi_{\theta_\text{old}}}(s_t, a_t)]
\]</span></p>
<p>The left part of the min operator is the surrogate objective of TRPO <span class="math inline">\(L^\text{CPI}(\theta)\)</span>. The right part restricts the importance sampling weight between <span class="math inline">\(1-\epsilon\)</span> and <span class="math inline">\(1 +\epsilon\)</span>. Let’s consider two cases (depicted on Fig. <a href="#fig:ppo">31</a>):</p>
<figure>
<img src="img/ppo.png" alt="Figure 31: Illustration of the effect of clipping the importance sampling weight. Taken from Schulman et al. (2017)." id="fig:ppo" style="width:80.0%" /><figcaption>Figure 31: Illustration of the effect of clipping the importance sampling weight. Taken from <span class="citation" data-cites="Schulman2017">Schulman et al. (<a href="#ref-Schulman2017" role="doc-biblioref">2017</a>)</span>.</figcaption>
</figure>
<ol type="1">
<li><p>the transition <span class="math inline">\(s_t, a_t\)</span> has a positive advantage, i.e. it is a better action than expected. The probability of selecting that action again should be increased, i.e. <span class="math inline">\(\pi_\theta(s_t, a_t) &gt; \pi_{\theta_\text{old}}(s_t, a_t)\)</span>. However, the importance sampling weight could become very high (a change from 0.01 to 0.05 is a ration of <span class="math inline">\(\rho_t(\theta) = 5\)</span>). In that case, <span class="math inline">\(\rho_t(\theta)\)</span> will be clipped to <span class="math inline">\(1+\epsilon\)</span>, for example 1.2. As a consequence, the parameters <span class="math inline">\(\theta\)</span> will move in the right direction, but the distance between the new and the old policies will stay small.</p></li>
<li><p>the transition <span class="math inline">\(s_t, a_t\)</span> has a negative advantage, i.e. it is a worse action than expected. Its probability will be decreased and the importance sampling weight might become much smaller than 1. Clipping it to <span class="math inline">\(1-\epsilon\)</span> avoids drastic changes to the policy, while still going in the right direction.</p></li>
</ol>
<p>Finally, they take the minimum of the clipped and unclipped objective, so that the final objective is a lower bound of the unclipped objective. In the original paper, they use <strong>generalized advantage estimation</strong> (GAE, Section <a href="#sec:generalized-advantage-estimation-gae">4.2.3</a>) to estimate <span class="math inline">\(A_{\pi_{\theta_\text{old}}}(s_t, a_t)\)</span>, but anything could be used (n-steps, etc). Transitions are sampled by multiple actors in parallel, as in A2C (Section <a href="#sec:advantage-actor-critic-a2c">4.2.1</a>).</p>
<p>The pseudo-algorithm of PPO is as follows:</p>
<hr />
<ul>
<li><p>Initialize an actor <span class="math inline">\(\pi_\theta\)</span> and a critic <span class="math inline">\(V_\varphi\)</span> with random weights.</p></li>
<li><p>while not converged :</p>
<ul>
<li><p>for <span class="math inline">\(N\)</span> actors in parallel:</p>
<ul>
<li><p>Collect <span class="math inline">\(T\)</span> transitions using <span class="math inline">\(\pi_\text{old}\)</span>.</p></li>
<li><p>Compute the generalized advantage of each transition using the critic.</p></li>
</ul></li>
<li><p>for <span class="math inline">\(K\)</span> epochs:</p>
<ul>
<li><p>Sample <span class="math inline">\(M\)</span> transitions from the ones previously collected.</p></li>
<li><p>Train the actor to maximize the clipped surrogate objective.</p></li>
<li><p>Train the critic to minimize the mse using TD learning.</p></li>
</ul></li>
<li><p><span class="math inline">\(\theta_\text{old} \leftarrow \theta\)</span></p></li>
</ul></li>
</ul>
<hr />
<p>The main advantage of PPO with respect to TRPO is its simplicity: the clipped objective can be directly maximized using first-order methods like stochastic gradient descent or Adam. It does not depend on assumptions about the parameter space: CNNs and RNNs can be used for the policy. It is sample-efficient, as several epochs of parameter updates are performed between two transition samplings: the policy network therefore needs less fresh samples that strictly on-policy algorithms to converge.</p>
<p>The only drawbacks of PPO is that there no convergence guarantee (although in practice it converges more often than other state-of-the-art methods) and that the right value for <span class="math inline">\(\epsilon\)</span> has to be determined. PPO has improved the state-of-the-art on Atari games and Mujoco robotic tasks. It has become the go-to method for continuous control problems.</p>
<p><strong>Additional resources</strong></p>
<ul>
<li>More explanations and demos from OpenAI: <a href="https://blog.openai.com/openai-baselines-ppo" class="uri">https://blog.openai.com/openai-baselines-ppo</a></li>
</ul>
<h3 id="sec:actor-critic-with-experience-replay-acer"><span class="header-section-number">4.5.5</span> Actor-Critic with Experience Replay (ACER)</h3>
<p>The natural gradient methods presented above are stochastic actor-critic methods, therefore strictly on-policy. Off-policy methods such as DQN or DDPG allow to reuse past transitions through the usage of an <strong>experience replay memory</strong>, potentially reducing the sample complexity at the cost of a higher variance and worse stability (Section <a href="#sec:off-policy-actor-critic">4.3</a>). <span class="citation" data-cites="Wang2017">Wang et al. (<a href="#ref-Wang2017" role="doc-biblioref">2017</a>)</span> proposed an off-policy actor-critic architecture using variance reduction techniques, the off-policy Retrace algorithm <span class="citation" data-cites="Munos2016">(Munos et al. <a href="#ref-Munos2016" role="doc-biblioref">2016</a>, Section <a href="#sec:retrace" role="doc-biblioref">4.3.3</a>)</span>, parallel training of multiple actor-learners <span class="citation" data-cites="Mnih2016">(Mnih et al. <a href="#ref-Mnih2016" role="doc-biblioref">2016</a>, Section <a href="#sec:asynchronous-advantage-actor-critic-a3c" role="doc-biblioref">4.2.2</a>)</span>, truncated importance sampling with bias correction (Section <a href="#sec:importance-sampling">4.3.1</a>), stochastic duelling network architectures <span class="citation" data-cites="Wang2016">(Wang et al. <a href="#ref-Wang2016" role="doc-biblioref">2016</a>, Section <a href="#sec:duelling-network" role="doc-biblioref">3.5</a>)</span>, and efficient trust region policy optimization. It can be seen as the off-policy counterpart to A3C.</p>
<p>The first aspect of ACER is that it interleaves on-policy learning with off-policy: the agent samples a trajectory <span class="math inline">\(\tau\)</span>, learns from it on-policy, stores it in the replay buffer, samples <span class="math inline">\(n\)</span> trajectories from the replay buffer and learns off-policy from each of them:</p>
<hr />
<ul>
<li><p>Sample a trajectory <span class="math inline">\(\tau\)</span> using the current policy.</p></li>
<li><p>Apply ACER on-policy on <span class="math inline">\(\tau\)</span>.</p></li>
<li><p>Store <span class="math inline">\(\tau\)</span> in the replay buffer.</p></li>
<li><p>Sample <span class="math inline">\(n\)</span> trajectories from the replay buffer.</p></li>
<li><p>for each sampled trajectory <span class="math inline">\(\tau_k\)</span>:</p>
<ul>
<li>Apply ACER off-policy on <span class="math inline">\(\tau_k\)</span>.</li>
</ul></li>
</ul>
<hr />
<p>Mixing on-policy learning with off-policy is quite similar to the Self-Imitation Learning approach of <span class="citation" data-cites="Oh2018">(Oh et al. <a href="#ref-Oh2018" role="doc-biblioref">2018</a>, Section <a href="#sec:self-imitation-learning-sil" role="doc-biblioref">4.3.4</a>)</span>.</p>
<h4 id="sec:retrace-evaluation" class="unnumbered">Retrace evaluation</h4>
<p>ACER comes in two flavors: one for discrete action spaces, one for continuous spaces. The discrete version is simpler, so let’s focus on this one. As any policy-gradient method, ACER tries to estimate the policy gradient for each transition of a trajectory, but using importance sampling <span class="citation" data-cites="Degris2012">(Degris, White, and Sutton <a href="#ref-Degris2012" role="doc-biblioref">2012</a>, Section <a href="#sec:linear-off-policy-actor-critic-off-pac" role="doc-biblioref">4.3.2</a>)</span>:</p>
<p><span class="math display">\[
    \nabla_\theta J(\theta)  = \mathbb{E}_{s_t, a_t \sim \rho_b} [\frac{\pi_\theta(s_t, a_t)}{b(s_t, a_t)} \, Q_\varphi(s_t, a_t) \, \nabla_\theta \log \pi_\theta(s_t, a_t)]
\]</span></p>
<p>The problem is now to train the critic <span class="math inline">\(Q_\varphi(s_t, a_t)\)</span> by computing the correct target. ACER learning builds on the Retrace algorithm <span class="citation" data-cites="Munos2016">(Munos et al. <a href="#ref-Munos2016" role="doc-biblioref">2016</a>, Section <a href="#sec:retrace" role="doc-biblioref">4.3.3</a>)</span>:</p>
<p><span class="math display">\[
    \Delta Q_\varphi(s_t, a_t) = \alpha \, \sum_{t&#39;=t}^T (\gamma)^{t&#39;-t} \left(\prod_{s=t+1}^{t&#39;} c_s \right) \, \delta_{t&#39;}
\]</span></p>
<p>with <span class="math inline">\(c_s = \lambda \min (1, \frac{\pi_\theta(s_s, a_s)}{b(s_s, a_s)})\)</span> being the clipped importance sampling weight and <span class="math inline">\(\delta_{t&#39;}\)</span> is the TD error at time <span class="math inline">\(t&#39;&gt;t\)</span>:</p>
<p><span class="math display">\[
    \delta_{t&#39;} = r_{t&#39;+1} + \gamma \, V(s_{t&#39;+1}) - V(s_{t&#39;})
\]</span></p>
<p>By noting <span class="math inline">\(Q^\text{ret}\)</span> the target value for the update of the critic (neglecting the learning rate <span class="math inline">\(\alpha\)</span>):</p>
<p><span class="math display">\[
    Q^\text{ret}(s_t, a_t) = Q_\varphi(s_t, a_t) +  \sum_{t&#39;=t}^T (\gamma)^{t&#39;-t} \left(\prod_{s=t+1}^{t&#39;} c_s \right) \, \delta_{t&#39;}
\]</span></p>
<p>we can transform the Retrace formula into a recurrent one:</p>
<p><span class="math display">\[
\begin{aligned}
    Q^\text{ret}(s_t, a_t) &amp; = Q_\varphi(s_t, a_t) + \delta_t + \sum_{t&#39;=t+1}^T (\gamma)^{t&#39;-t} \left(\prod_{s=t+1}^{t&#39;} c_s \right) \, \delta_{t&#39;} \\
    &amp; = Q_\varphi(s_t, a_t) + \delta_t + \gamma \, c_{t+1} \, (Q^\text{ret}(s_{t+1}, a_{t+1}) - Q_\varphi(s_{t+1}, a_{t+1})) \\
\end{aligned}
\]</span></p>
<p><span class="math inline">\(Q_\varphi(s_t, a_t) + \delta_t = Q_\varphi(s_t, a_t) + r_{t+1} + \gamma \, V(s_{t+1}) - V(s_t)\)</span> can furthermore be reduced to <span class="math inline">\(r_{t+1} + \gamma \, V(s_{t+1})\)</span> by considering that <span class="math inline">\(Q_\varphi(s_t, a_t) \approx V(s_t)\)</span> (the paper does not justify this assumption, but it should be true in expectation).</p>
<p>This gives us the following target value for the Q-values:</p>
<p><span class="math display">\[
    Q^\text{ret}(s_t, a_t) = r_{t+1} + \gamma \, c_{t+1} \, (Q^\text{ret}(s_{t+1}, a_{t+1}) - Q_\varphi(s_{t+1}, a_{t+1})) + \gamma \, V(s_{t+1})
\]</span></p>
<p>One remaining issue is that the critic would also need to output the value of each state <span class="math inline">\(V(s_{t+1})\)</span>, in addition to the Q-values <span class="math inline">\(Q_\varphi(s_t, a_t)\)</span>. In the discrete case, this is not necessary, as the value of a state is the expectation of the value of the available actions under the current policy:</p>
<p><span class="math display">\[
    V(s_{t+1}) = \mathbb{E}_{a_{t+1} \sim \pi_\theta} [Q_\varphi(s_{t+1}, a_{t+1})] = \sum_a \pi_\theta(s_{t+1}, a) \, Q_\varphi(s_{t+1}, a))
\]</span></p>
<p>The value of the next state can be easily computed when we have the policy <span class="math inline">\(\pi_\theta(s, a)\)</span> (actor) and the Q-value <span class="math inline">\(Q_\varphi(s, a)\)</span> (critic) of each action <span class="math inline">\(a\)</span> in a state <span class="math inline">\(s\)</span>.</p>
<p>The actor-critic architecture needed for ACER is therefore the following:</p>
<ul>
<li><p>The <strong>actor</strong> <span class="math inline">\(\pi_\theta\)</span> takes a state <span class="math inline">\(s\)</span> as input and outputs a vector of probabilities <span class="math inline">\(\pi_\theta\)</span> for each available action.</p></li>
<li><p>The <strong>critic</strong> <span class="math inline">\(Q_\varphi\)</span> takes a state <span class="math inline">\(s\)</span> as input and outputs a vector of Q-values.</p></li>
</ul>
<p>This is different from the architecture of A3C, where the critic “only” had to output the value of a state <span class="math inline">\(V_\varphi(s)\)</span>: it is now a vector of Q-values. Note that the actor and the critic can share most of their parameters: the network only needs to output two different vectors <span class="math inline">\(\pi_\theta(s)\)</span> and <span class="math inline">\(Q_\varphi(s)\)</span> for each input state <span class="math inline">\(s\)</span> (Fig. <a href="#fig:acer">32</a>). This makes a “two heads” NN, similar to the <strong>duelling architecture</strong> of <span class="citation" data-cites="Wang2016">(Wang et al. <a href="#ref-Wang2016" role="doc-biblioref">2016</a>, Section <a href="#sec:duelling-network" role="doc-biblioref">3.5</a>)</span>.</p>
<figure>
<img src="img/acer.png" alt="Figure 32: Architecture of the ACER actor-critic." id="fig:acer" style="width:90.0%" /><figcaption>Figure 32: Architecture of the ACER actor-critic.</figcaption>
</figure>
<p>The target Q-value <span class="math inline">\(Q^\text{ret}(s, a)\)</span> can be found recursively by iterating backwards over the episode:</p>
<hr />
<ul>
<li><p>Initialize <span class="math inline">\(Q^\text{ret}(s_T, a_T)\)</span>, <span class="math inline">\(Q_\varphi(s_T, a_T)\)</span> and <span class="math inline">\(V(s_T)\)</span> to 0, as the terminal state has no value.</p></li>
<li><p>for <span class="math inline">\(t \in [T-1, \ldots, 0]\)</span>:</p>
<ul>
<li>Update the target Q-value using the received reward, the critic and the previous target value:</li>
</ul>
<p><span class="math display">\[
      Q^\text{ret}(s_t, a_t) = r_{t+1} + \gamma \, c_{t+1} \, (Q^\text{ret}(s_{t+1}, a_{t+1}) - Q_\varphi(s_{t+1}, a_{t+1})) + \gamma \, V(s_{t+1})
  \]</span></p>
<ul>
<li>Apply the critic on the current action:</li>
</ul>
<p><span class="math display">\[
      Q_\varphi(s_t, a_t)
  \]</span></p>
<ul>
<li>Estimate the value of the state using the critic:</li>
</ul>
<p><span class="math display">\[
      V(s_t) = \sum_a \pi_\theta(s_t) \, Q_\varphi(s_t, a)
  \]</span></p></li>
</ul>
<hr />
<p>As the target value <span class="math inline">\(Q^\text{ret}(s, a)\)</span> use multiple “real” rewards <span class="math inline">\(r_{t+1}\)</span>, it is actually less biased than the critic <span class="math inline">\(Q_\varphi(s, a)\)</span>. It is then better to use it to update the actor:</p>
<p><span class="math display">\[
    \nabla_\theta J(\theta)  = \mathbb{E}_{s_t, a_t \sim \rho_b} [\frac{\pi_\theta(s_t, a_t)}{b(s_t, a_t)} \, Q^\text{ret}(s_t, a_t) \, \nabla_\theta \log \pi_\theta(s_t, a_t)]
\]</span></p>
<p>The critic just has to minimize the mse with the target value:</p>
<p><span class="math display">\[
    \mathcal{L}(\varphi) = \mathbb{E}_{s_t, a_t \sim \rho_b} [(Q^\text{ret}(s, a) - Q_\varphi(s, a))^2]
\]</span></p>
<h4 id="sec:importance-weight-truncation-with-bias-correction" class="unnumbered">Importance weight truncation with bias correction</h4>
<p>When updating the actor, we rely on the importance sampling weight <span class="math inline">\(\rho_t = \frac{\pi_\theta(s_t, a_t)}{b(s_t, a_t)}\)</span> which can vary a lot and destabilize learning.</p>
<p><span class="math display">\[
    \nabla_\theta J(\theta)  = \mathbb{E}_{s_t, a_t \sim \rho_b} [\rho_t \, Q^\text{ret}(s_t, a_t) \, \nabla_\theta \log \pi_\theta(s_t, a_t)]
\]</span></p>
<p>PPO (Section <a href="#sec:proximal-policy-optimization-ppo">4.5.4</a>) solved this problem by clipping the importance sampling weight between <span class="math inline">\(1- \epsilon\)</span> and <span class="math inline">\(1+\epsilon\)</span>. ACER uses a similar strategy, but only using an upper bound <span class="math inline">\(c = 10\)</span> on the weight:</p>
<p><span class="math display">\[
    \bar{\rho}_t = \min(c, \rho_t)
\]</span></p>
<p>Using <span class="math inline">\(\bar{\rho}_t\)</span> in the policy gradient directly would introduce a bias: actions whose importance sampling weight <span class="math inline">\(\rho_t\)</span> is higher than <span class="math inline">\(c\)</span> would contribute to the policy gradient with a smaller value than they should, introducing a bias.</p>
<p>The solution in ACER is to add a <strong>bias correcting term</strong>, that corrects the policy gradient when an action has a weight higher than <span class="math inline">\(c\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
    \nabla_\theta J(\theta)  = &amp; \mathbb{E}_{s_t \sim \rho_b} [\mathbb{E}_{a_t \sim b} [\bar{\rho}_t \, Q^\text{ret}(s_t, a_t) \, \nabla_\theta \log \pi_\theta(s_t, a_t)] \\
    &amp; + \mathbb{E}_{a \sim \pi_\theta}[(\frac{\rho_t(a) - c}{\rho_t(a)})^+ \, Q_\varphi(s_t, a) \, \nabla_\theta \log \pi_\theta(s_t, a)]] \\
\end{aligned}
\]</span></p>
<p>The left part of that equation is the same policy gradient as before, but using a clipped importance sampling weight.</p>
<p>The right part requires to integrate over all possible actions in the state <span class="math inline">\(s_t\)</span> according to the learned policy <span class="math inline">\(\pi_\theta\)</span>, although only the action <span class="math inline">\(a_t\)</span> was selected by the behavior policy <span class="math inline">\(b\)</span>. The term <span class="math inline">\((\frac{\rho_t(a) - c}{\rho_t(a)})^+\)</span> is zero for all actions having an importance sampling weight smaller than c, and has a maximum of 1. In practice, this correction term can be computed using the vectors <span class="math inline">\(\pi_\theta(s, a)\)</span> and <span class="math inline">\(Q_\varphi(s, a)\)</span>, which are the outputs of the actor and the critic, respectively.</p>
<p>Finally, the Q-values <span class="math inline">\(Q^\text{ret}(s_t, a_t)\)</span> and <span class="math inline">\(Q_\varphi(s_t, a)\)</span> are transformed into advantages <span class="math inline">\(Q^\text{ret}(s_t, a_t) - V_\varphi(s_t)\)</span> and <span class="math inline">\(Q_\varphi(s_t, a) - V_\varphi(s_t)\)</span> by substracting the value of the state in order to reduce the variance of the policy gradient.</p>
<p>In short, we now have an estimator of the policy gradient which is <strong>unbiased</strong> and of smaller variance.</p>
<h4 id="sec:efficient-trust-region-policy-optimization" class="unnumbered">Efficient trust region policy optimization</h4>
<p>However, the variance is still too high. The last important step of ACER is an efficient TRPO update for the parameters of the actor.</p>
<p>A first component of their TRPO update is they use a <strong>target actor network</strong> <span class="math inline">\(\theta&#39;\)</span> (called averaged policy network in the paper) slowly tracking the actor <span class="math inline">\(\theta\)</span> after each update:</p>
<p><span class="math display">\[
    \theta&#39; \leftarrow \alpha \, \theta&#39; + (1 - \alpha) \, \theta
\]</span></p>
<p>A second component is that the actor is decomposed into two components:</p>
<ol type="1">
<li>a distribution <span class="math inline">\(f\)</span>.</li>
<li>the statistics <span class="math inline">\(\Phi_\theta(x)\)</span> of this distribution.</li>
</ol>
<p>This is what you do when you apply the softmax action selection on Q-values: the distribution is the Gibbs (or Boltzmann) distribution and the Q-values are its statistics. In the discrete case, they take a categorical (or multinouilli) distribution: <span class="math inline">\(\Phi_\theta(s)\)</span> is the probability for each action to be taken and the distribution selects one of them. Think of a dice with one side per action and probabilities governed by the policy. In the continuous case, it could be anything, for example a normal distribution.</p>
<p>Let’s rewrite the policy gradient with that formulation (we omit here the bias correction, but ACER uses it), but only w.r.t the output of the actor <span class="math inline">\(\Phi_\theta(s_t)\)</span> for a state <span class="math inline">\(s_t\)</span>:</p>
<p><span class="math display">\[
    \hat{g_t}^\text{ACER} = \nabla_{\Phi_\theta(s_t)} J(\theta)  = \bar{\rho}_t \, (Q^\text{ret}(s_t, a_t) - V_\phi(s_t) ) \, \nabla_{\Phi_\theta(s_t)} \log f(a_t | \Phi_\theta(s_t))
\]</span></p>
<p>To compute the policy gradient, we would only need to apply the chain rule:</p>
<p><span class="math display">\[
    \nabla_\theta J(\theta) = \mathbb{E}_{s_t, a_t \sim \rho_b} [ \hat{g_t}^\text{ACER} \, \nabla_\theta \Phi_\theta(s_t) ]
\]</span></p>
<p>The variance of <span class="math inline">\(\hat{g_t}^\text{ACER}\)</span> is too high. ACER defines the following TRPO problem: we search for a gradient <span class="math inline">\(z\)</span> solution of:</p>
<p><span class="math display">\[
    \min_z ||\hat{g_t}^\text{ACER} - z ||^2 \\
    \text{s.t.} \quad \nabla_{\Phi_\theta(s_t)} D_{KL}( f(\cdot | \Phi_{\theta&#39;}(s_t) ) || f(\cdot | \Phi_{\theta&#39;}(s_t)) )^T \times z &lt; \delta
\]</span></p>
<p>The exact meaning of the constraint is hard to grasp, but here some intuition: the change of policy <span class="math inline">\(z\)</span> (remember that <span class="math inline">\(\hat{g_t}^\text{ACER}\)</span> is defined w.r.t the output of the actor) should be as orthogonal as possible (within a margin <span class="math inline">\(\delta\)</span>) to the change of the <strong>Kullback-Leibler</strong> divergence between the policy defined by the actor (<span class="math inline">\(\theta\)</span>) and the one defined by the <strong>target actor</strong> (<span class="math inline">\(\theta&#39;\)</span>). In other words, we want to update the actor, but without making the new policy too different from its past values (the target actor).</p>
<p>The advantage of this formulation is that the objective function is quadratic in <span class="math inline">\(z\)</span> and the constraint is linear. We can therefore very easily find its solution using KKT optimization:</p>
<p><span class="math display">\[
    z^* = \hat{g_t}^\text{ACER} - \max(0, \frac{k^T \, \hat{g_t}^\text{ACER} - \delta}{||k||^2}) \, k
\]</span></p>
<p>where <span class="math inline">\(k = \nabla_{\Phi_\theta(s_t)} D_{KL}( f(\cdot | \Phi_{\theta&#39;}(s_t) ) || f(\cdot | \Phi_{\theta&#39;}(s_t)) )\)</span>.</p>
<p>Having obtained <span class="math inline">\(z^*\)</span>, we can safely update the parameters of the actor in the direction of:</p>
<p><span class="math display">\[
    \nabla_\theta J(\theta) = \mathbb{E}_{s_t, a_t \sim \rho_b} [ z^* \, \nabla_\theta \Phi_\theta(s_t) ]
\]</span></p>
<p>As noted in the paper: <em>“The trust region step is carried out in the space of the statistics of the distribution <span class="math inline">\(f\)</span> , and not in the space of the policy parameters. This is done deliberately so as to avoid an additional back-propagation step through the policy network”</em>. We indeed need only one network update per transition. If the KL divergence was computed with respect to <span class="math inline">\(\pi_\theta\)</span> directly, one would need to apply backpropagation on the target network too.</p>
<p>The target network <span class="math inline">\(\theta&#39;\)</span> is furthermore used as the behavior policy <span class="math inline">\(b(s, a)\)</span>. here is also a <strong>target critic network</strong> <span class="math inline">\(\varphi&#39;\)</span>, which is primarily used to compute the value of the states <span class="math inline">\(V_{\varphi&#39;}(s)\)</span> for variance reduction.</p>
<p>For a complete description of the algorithm, refer to the paper… To summarize, ACER is an actor-critic architecture using Retrace estimated values, importance weight truncation with bias correction and efficient TRPO. Its variant for continuous action spaces furthermore uses a <strong>Stochastic Dueling Network</strong> (SDN) in order estimate both <span class="math inline">\(Q_\varphi(s, a)\)</span> and <span class="math inline">\(V_\varphi(s)\)</span>. It is straightforward in the discrete case (multiply the policy with the Q-values and take the average) but hard in the continuous case.</p>
<p>ACER improved the performance and/or the sample efficiency of the state-of-the-art (A3C, DDPG, etc) on a variety of tasks (Atari, Mujoco). Apart from truncation with bias correction, all aspects of the algorithm are essential to obtain this improvement, as shown by ablation studies.</p>
<!--PAGEBREAK-->
<h2 id="sec:maximum-entropy-rl"><span class="header-section-number">4.6</span> Maximum Entropy RL</h2>
<p><strong>Work in progress</strong></p>
<h3 id="sec:entropy-regularization-1"><span class="header-section-number">4.6.1</span> Entropy regularization</h3>
<p><span class="citation" data-cites="Todorov2008">Todorov (<a href="#ref-Todorov2008" role="doc-biblioref">2008</a>)</span> a stochastic policy emerges as the optimal answer when we consider the connection between optimal control and probabilistic inference.</p>
<p><span class="citation" data-cites="Toussaint2009">Toussaint (<a href="#ref-Toussaint2009" role="doc-biblioref">2009</a>)</span> Maximum entropy RL</p>
<p><span class="citation" data-cites="ODonoghue2016">O’Donoghue et al. (<a href="#ref-ODonoghue2016" role="doc-biblioref">2016</a>)</span> Combining policy gradient and Q-learning</p>
<p><span class="citation" data-cites="Haarnoja2017">Haarnoja et al. (<a href="#ref-Haarnoja2017" role="doc-biblioref">2017</a>)</span> Reinforcement Learning with Deep Energy-Based Policies</p>
<h3 id="sec:soft-actor-critic-sac"><span class="header-section-number">4.6.2</span> Soft Actor-Critic (SAC)</h3>
<ul>
<li><p>Main limitations of on-policy algorithms: high sample complexity and poor convergence properties, leading to difficult tuining of hyperparameters.</p></li>
<li><p>Off-policy reduces the sample complexity, but increases the variance = instability.</p></li>
</ul>
<p><span class="citation" data-cites="Haarnoja2018a">Haarnoja et al. (<a href="#ref-Haarnoja2018a" role="doc-biblioref">2018</a>)</span></p>
<h2 id="sec:distributional-learning"><span class="header-section-number">4.7</span> Distributional learning</h2>
<p>All RL methods based on the Bellman equations use the expectation operator to average returns and compute the values of states and actions:</p>
<p><span class="math display">\[
    Q^\pi(s, a) ) = \mathbb{E}_{\pi}[R(s, a)]
\]</span></p>
<p><span class="citation" data-cites="Bellemare2017">Bellemare, Dabney, and Munos (<a href="#ref-Bellemare2017" role="doc-biblioref">2017</a>)</span> propose to learn instead the <strong>value distribution</strong> through a modification of the Bellman equation. They show that learning the distribution of rewards rather than their mean leads to performance improvements.</p>
<p>See <a href="https://deepmind.com/blog/going-beyond-average-reinforcement-learning/" class="uri">https://deepmind.com/blog/going-beyond-average-reinforcement-learning/</a> for more explanations.</p>
<h3 id="sec:the-reactor"><span class="header-section-number">4.7.1</span> The Reactor</h3>
<p><span class="citation" data-cites="Gruslys2017">Gruslys et al. (<a href="#ref-Gruslys2017" role="doc-biblioref">2017</a>)</span></p>
<h2 id="sec:other-policy-search-methods"><span class="header-section-number">4.8</span> Other policy search methods</h2>
<h3 id="sec:stochastic-value-gradient-svg"><span class="header-section-number">4.8.1</span> Stochastic Value Gradient (SVG)</h3>
<p><span class="citation" data-cites="Heess2015">Heess et al. (<a href="#ref-Heess2015" role="doc-biblioref">2015</a>)</span></p>
<h3 id="sec:q-prop"><span class="header-section-number">4.8.2</span> Q-Prop</h3>
<p><span class="citation" data-cites="Gu2016">Gu, Lillicrap, Ghahramani, et al. (<a href="#ref-Gu2016" role="doc-biblioref">2016</a>)</span></p>
<h3 id="sec:normalized-advantage-function-naf"><span class="header-section-number">4.8.3</span> Normalized Advantage Function (NAF)</h3>
<p><span class="citation" data-cites="Gu2016a">Gu, Lillicrap, Sutskever, et al. (<a href="#ref-Gu2016a" role="doc-biblioref">2016</a>)</span></p>
<h3 id="sec:fictitious-self-play-fsp"><span class="header-section-number">4.8.4</span> Fictitious Self-Play (FSP)</h3>
<p><span class="citation" data-cites="Heinrich2015">Heinrich, Lanctot, and Silver (<a href="#ref-Heinrich2015" role="doc-biblioref">2015</a>)</span> <span class="citation" data-cites="Heinrich2016">Heinrich and Silver (<a href="#ref-Heinrich2016" role="doc-biblioref">2016</a>)</span></p>
<h2 id="sec:comparison-between-value-based-and-policy-gradient-methods"><span class="header-section-number">4.9</span> Comparison between value-based and policy gradient methods</h2>
<p>Having now reviewed both value-based methods (DQN and its variants) and policy gradient methods (A3C, DDPG, PPO), the question is which method to choose? While not much happens right now for value-based methods, policy gradient methods are attracting a lot of attention, as they are able to learn policies in continuous action spaces, what is very important in robotics. <a href="https://flyyufelix.github.io/2017/10/12/dqn-vs-pg.html" class="uri">https://flyyufelix.github.io/2017/10/12/dqn-vs-pg.html</a> summarizes the advantages and inconvenients of policy gradient methods.</p>
<p>Advantages of PG:</p>
<ul>
<li>Better convergence properties, more stable <span class="citation" data-cites="Duan2016">(Duan et al. <a href="#ref-Duan2016" role="doc-biblioref">2016</a>)</span>.</li>
<li>Effective in high-dimensional or continuous action spaces.</li>
<li>Can learn stochastic policies.</li>
</ul>
<p>Disadvantages of PG:</p>
<ul>
<li>Typically converge to a local rather than global optimum.</li>
<li>Evaluating a policy is often inefficient and having a high variance.</li>
<li>Worse sample efficiency (but it is getting better).</li>
</ul>
<h2 id="sec:gradient-free-policy-search"><span class="header-section-number">4.10</span> Gradient-free policy search</h2>
<p>The policy gradient methods presented above rely on backpropagation and gradient descent/ascent to update the parameters of the policy and maximize the objective function. Gradient descent is generally slow, sample inefficient and subject to local minima, but is nevertheless the go-to method in neural networks. However, it is not the only optimization that can be used in deep RL. This section presents some of the alternatives.</p>
<h3 id="sec:cross-entropy-method-cem"><span class="header-section-number">4.10.1</span> Cross-entropy Method (CEM)</h3>
<p><span class="citation" data-cites="Szita2006">Szita and Lörincz (<a href="#ref-Szita2006" role="doc-biblioref">2006</a>)</span></p>
<h3 id="sec:evolutionary-search-es"><span class="header-section-number">4.10.2</span> Evolutionary Search (ES)</h3>
<p><span class="citation" data-cites="Salimans2017">Salimans et al. (<a href="#ref-Salimans2017" role="doc-biblioref">2017</a>)</span></p>
<p>Explanations from OpenAI: <a href="https://blog.openai.com/evolution-strategies/" class="uri">https://blog.openai.com/evolution-strategies/</a></p>
<p>Deep neuroevolution at Uber: <a href="https://eng.uber.com/deep-neuroevolution/" class="uri">https://eng.uber.com/deep-neuroevolution/</a></p>
<!--PAGEBREAK-->
<h1 id="sec:deep-rl-in-practice"><span class="header-section-number">5</span> Deep RL in practice</h1>
<h2 id="sec:limitations"><span class="header-section-number">5.1</span> Limitations</h2>
<p>Excellent blog post from Alex Irpan on the limitations of deep RL: <a href="https://www.alexirpan.com/2018/02/14/rl-hard.html" class="uri">https://www.alexirpan.com/2018/02/14/rl-hard.html</a></p>
<p>Another documented critic on deep RL: <a href="https://thegradient.pub/why-rl-is-flawed/" class="uri">https://thegradient.pub/why-rl-is-flawed/</a></p>
<h2 id="sec:reward-shaping"><span class="header-section-number">5.2</span> Reward shaping</h2>
<p>Hindsight experience replay: <span class="citation" data-cites="Andrychowicz2017">Andrychowicz et al. (<a href="#ref-Andrychowicz2017" role="doc-biblioref">2017</a>)</span></p>
<h2 id="sec:simulation-environments"><span class="header-section-number">5.3</span> Simulation environments</h2>
<p>Standard RL environments are needed to better compare the performance of RL algorithms. Below is a list of the most popular ones.</p>
<ul>
<li>OpenAI Gym <a href="https://gym.openai.com" class="uri">https://gym.openai.com</a>: a standard toolkit for comparing RL algorithms provided by the OpenAI foundation. It provides many environments, from the classical toy problems in RL (GridWorld, pole-balancing) to more advanced problems (Mujoco simulated robots, Atari games, Minecraft…). The main advantage is the simplicity of the interface: the user only needs to select which task he wants to solve, and a simple for loop allows to perform actions and observe their consequences:</li>
</ul>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb1-1" title="1"><span class="im">import</span> gym</a>
<a class="sourceLine" id="cb1-2" title="2">env <span class="op">=</span> gym.make(<span class="st">&quot;Taxi-v1&quot;</span>)</a>
<a class="sourceLine" id="cb1-3" title="3">observation <span class="op">=</span> env.reset()</a>
<a class="sourceLine" id="cb1-4" title="4"><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1000</span>):</a>
<a class="sourceLine" id="cb1-5" title="5">    env.render()</a>
<a class="sourceLine" id="cb1-6" title="6">    action <span class="op">=</span> env.action_space.sample()</a>
<a class="sourceLine" id="cb1-7" title="7">    observation, reward, done, info <span class="op">=</span> env.step(action)</a></code></pre></div>
<ul>
<li><p>OpenAI Universe <a href="https://universe.openai.com" class="uri">https://universe.openai.com</a>: a similar framework from OpenAI, but to control realistic video games (GTA V, etc).</p></li>
<li><p>Darts environment <a href="https://github.com/DartEnv/dart-env" class="uri">https://github.com/DartEnv/dart-env</a>: a fork of gym to use the Darts simulator instead of Mujoco.</p></li>
<li><p>Roboschool <a href="https://github.com/openai/roboschool" class="uri">https://github.com/openai/roboschool</a>: another alternative to Mujoco for continuous robotic control, this time from openAI.</p></li>
<li><p>NIPS 2017 musculo-skeletal challenge <a href="https://github.com/stanfordnmbl/osim-rl" class="uri">https://github.com/stanfordnmbl/osim-rl</a></p></li>
<li><p>Deepmind Lab <a href="https://github.com/deepmind/lab" class="uri">https://github.com/deepmind/lab</a>: a 3D learning environment based on id Software’s Quake III Arena via ioquake3 and other open source software.</p></li>
<li><p>AnimalAI Olympics <a href="https://github.com/beyretb/AnimalAI-Olympics" class="uri">https://github.com/beyretb/AnimalAI-Olympics</a>, a gym-like environment aimed at confronting RL algorithms to typical tasks in the animal cognition literature.</p></li>
</ul>
<h2 id="sec:algorithm-implementations"><span class="header-section-number">5.4</span> Algorithm implementations</h2>
<p>State-of-the-art algorithms in deep RL are already implemented and freely available on the internet. Below is a preliminary list of the most popular ones. Most of them rely on tensorflow or keras for training the neural networks and interact directly with gym-like interfaces.</p>
<ul>
<li><p><a href="https://github.com/ShangtongZhang/reinforcement-learning-an-introduction" class="uri">https://github.com/ShangtongZhang/reinforcement-learning-an-introduction</a>: all the exercises in Python of the <span class="citation" data-cites="Sutton2017">(Sutton and Barto <a href="#ref-Sutton2017" role="doc-biblioref">2017</a>)</span> book.</p></li>
<li><p><code>rl-code</code> <a href="https://github.com/rlcode/reinforcement-learning" class="uri">https://github.com/rlcode/reinforcement-learning</a>: many code samples for simple RL problems (GridWorld, Cartpole, Atari Games). The code samples are mostly for educational purpose (Policy Iteration, Value Iteration, Monte-Carlo, SARSA, Q-learning, REINFORCE, DQN, A2C, A3C).</p></li>
<li><p><code>keras-rl</code> <a href="https://github.com/matthiasplappert/keras-rl" class="uri">https://github.com/matthiasplappert/keras-rl</a>: many deep RL algorithms implemented directly in keras: DQN, DDQN, DDPG, Continuous DQN (CDQN or NAF), Cross-Entropy Method (CEM), Dueling DQN, Deep SARSA…</p></li>
<li><p><code>Coach</code> <a href="https://github.com/NervanaSystems/coach" class="uri">https://github.com/NervanaSystems/coach</a> from Intel Nervana also provides many state-of-the-art algorithms: DQN, DDQN, Dueling DQN, Mixed Monte Carlo (MMC), Persistent Advantage Learning (PAL), Distributional Deep Q Network, Bootstrapped Deep Q Network, N-Step Q Learning, Neural Episodic Control (NEC), Normalized Advantage Functions (NAF), Policy Gradients (PG), A3C, DDPG, Proximal Policy Optimization (PPO), Clipped Proximal Policy Optimization, Direct Future Prediction (DFP)…</p></li>
<li><p><code>OpenAI Baselines</code> <a href="https://github.com/openai/baselines" class="uri">https://github.com/openai/baselines</a> from OpenAI too: A2C, ACER, ACKTR, DDPG, DQN, PPO, TRPO…</p></li>
<li><p><code>rlkit</code> <a href="https://github.com/vitchyr/rlkit" class="uri">https://github.com/vitchyr/rlkit</a> from Vitchyr Pong (PhD student at Berkeley) with in particular model-based algorithms (TDM, <span class="citation" data-cites="Pong2018">Pong et al. (<a href="#ref-Pong2018" role="doc-biblioref">2018</a>)</span>).</p></li>
<li><p><code>chainer-rl</code> <a href="https://github.com/chainer/chainerrl" class="uri">https://github.com/chainer/chainerrl</a> implemented in Chainer (an alternative to tensorflow): A3C, ACER, Categorical DQN; DQN (including Double DQN, Persistent Advantage Learning (PAL), Double PAL, Dynamic Policy Programming (DPP)), DDPG, , PGT (Policy Gradient Theorem), PCL (Path Consistency Learning), PPO, TRPO.</p></li>
</ul>
<!--PAGEBREAK-->
<h1 id="sec:references" class="unnumbered">References</h1>
<div id="refs" class="references" role="doc-bibliography">
<div id="ref-Amari1998">
<p>Amari, S.-I. 1998. “Natural Gradient Works Efficiently in Learning.” <em>Neural Computation</em> 10 (2): 251–76.</p>
</div>
<div id="ref-Andrychowicz2017">
<p>Andrychowicz, Marcin, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Josh Tobin, Pieter Abbeel, and Wojciech Zaremba. 2017. “Hindsight Experience Replay,” July. <a href="http://arxiv.org/abs/1707.01495">http://arxiv.org/abs/1707.01495</a>.</p>
</div>
<div id="ref-Anschel2016">
<p>Anschel, Oron, Nir Baram, and Nahum Shimkin. 2016. “Averaged-DQN: Variance Reduction and Stabilization for Deep Reinforcement Learning,” November. <a href="http://arxiv.org/abs/1611.01929">http://arxiv.org/abs/1611.01929</a>.</p>
</div>
<div id="ref-Arjovsky2017">
<p>Arjovsky, Martin, Soumith Chintala, and Léon Bottou. 2017. “Wasserstein GAN,” January. <a href="http://arxiv.org/abs/1701.07875">http://arxiv.org/abs/1701.07875</a>.</p>
</div>
<div id="ref-Arulkumaran2017">
<p>Arulkumaran, Kai, Marc Peter Deisenroth, Miles Brundage, and Anil Anthony Bharath. 2017. “A Brief Survey of Deep Reinforcement Learning.” <a href="https://arxiv.org/pdf/1708.05866.pdf">https://arxiv.org/pdf/1708.05866.pdf</a>.</p>
</div>
<div id="ref-Baird1993">
<p>Baird, L. C. 1993. “Advantage Updating.” Technical Report WL-TR-93-1146. Wright-Patterson Air Force Base. <a href="http://leemon.com/papers/1993b.pdf">http://leemon.com/papers/1993b.pdf</a>.</p>
</div>
<div id="ref-Bakker2001">
<p>Bakker, Bram. 2001. “Reinforcement Learning with Long Short-Term Memory.” In <em>Advances in Neural Information Processing Systems 14 (NIPS 2001)</em>, 1475–82. <a href="https://papers.nips.cc/paper/1953-reinforcement-learning-with-long-short-term-memory">https://papers.nips.cc/paper/1953-reinforcement-learning-with-long-short-term-memory</a>.</p>
</div>
<div id="ref-Barth-Maron2018">
<p>Barth-Maron, Gabriel, Matthew W. Hoffman, David Budden, Will Dabney, Dan Horgan, Dhruva TB, Alistair Muldal, Nicolas Heess, and Timothy Lillicrap. 2018. “Distributed Distributional Deterministic Policy Gradients,” April. <a href="http://arxiv.org/abs/1804.08617">http://arxiv.org/abs/1804.08617</a>.</p>
</div>
<div id="ref-Bellemare2017">
<p>Bellemare, Marc G., Will Dabney, and Rémi Munos. 2017. “A Distributional Perspective on Reinforcement Learning,” July. <a href="http://arxiv.org/abs/1707.06887">http://arxiv.org/abs/1707.06887</a>.</p>
</div>
<div id="ref-Cho2014">
<p>Cho, Kyunghyun, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. “Learning Phrase Representations Using RNN Encoder-Decoder for Statistical Machine Translation,” June. <a href="http://arxiv.org/abs/1406.1078">http://arxiv.org/abs/1406.1078</a>.</p>
</div>
<div id="ref-Chou2017">
<p>Chou, Po-Wei, Daniel Maturana, and Sebastian Scherer. 2017. “Improving Stochastic Policy Gradients in Continuous Control with Deep Reinforcement Learning Using the Beta Distribution.” In <em>International Conference on Machine Learning</em>. <a href="http://proceedings.mlr.press/v70/chou17a/chou17a.pdf">http://proceedings.mlr.press/v70/chou17a/chou17a.pdf</a>.</p>
</div>
<div id="ref-Degris2012">
<p>Degris, Thomas, Martha White, and Richard S. Sutton. 2012. “Linear Off-Policy Actor-Critic.” In <em>Proceedings of the 2012 International Conference on Machine Learning</em>. <a href="http://arxiv.org/abs/1205.4839">http://arxiv.org/abs/1205.4839</a>.</p>
</div>
<div id="ref-Duan2016">
<p>Duan, Yan, Xi Chen, Rein Houthooft, John Schulman, and Pieter Abbeel. 2016. “Benchmarking Deep Reinforcement Learning for Continuous Control,” April. <a href="http://arxiv.org/abs/1604.06778">http://arxiv.org/abs/1604.06778</a>.</p>
</div>
<div id="ref-Gers2001">
<p>Gers, Felix. 2001. “Long Short-Term Memory in Recurrent Neural Networks.” PhD thesis. <a href="http://www.felixgers.de/papers/phd.pdf">http://www.felixgers.de/papers/phd.pdf</a>.</p>
</div>
<div id="ref-Goodfellow2016">
<p>Goodfellow, Ian, Yoshua Bengio, and Aaron Courville. 2016. <em>Deep Learning</em>. MIT Press. <a href="http://www.deeplearningbook.org">http://www.deeplearningbook.org</a>.</p>
</div>
<div id="ref-Goodfellow2014">
<p>Goodfellow, Ian J., Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. “Generative Adversarial Networks,” June. <a href="http://arxiv.org/abs/1406.2661">http://arxiv.org/abs/1406.2661</a>.</p>
</div>
<div id="ref-Gruslys2017">
<p>Gruslys, Audrunas, Will Dabney, Mohammad Gheshlaghi Azar, Bilal Piot, Marc Bellemare, and Remi Munos. 2017. “The Reactor: A Fast and Sample-Efficient Actor-Critic Agent for Reinforcement Learning,” April. <a href="http://arxiv.org/abs/1704.04651">http://arxiv.org/abs/1704.04651</a>.</p>
</div>
<div id="ref-Gu2017">
<p>Gu, Shixiang, Ethan Holly, Timothy Lillicrap, and Sergey Levine. 2017. “Deep Reinforcement Learning for Robotic Manipulation with Asynchronous Off-Policy Updates.” In <em>Proc. ICRA</em>. <a href="http://arxiv.org/abs/1610.00633">http://arxiv.org/abs/1610.00633</a>.</p>
</div>
<div id="ref-Gu2016">
<p>Gu, Shixiang, Timothy Lillicrap, Zoubin Ghahramani, Richard E. Turner, and Sergey Levine. 2016. “Q-Prop: Sample-Efficient Policy Gradient with an Off-Policy Critic,” November. <a href="http://arxiv.org/abs/1611.02247">http://arxiv.org/abs/1611.02247</a>.</p>
</div>
<div id="ref-Gu2016a">
<p>Gu, Shixiang, Timothy Lillicrap, Ilya Sutskever, and Sergey Levine. 2016. “Continuous Deep Q-Learning with Model-Based Acceleration,” March. <a href="http://arxiv.org/abs/1603.00748">http://arxiv.org/abs/1603.00748</a>.</p>
</div>
<div id="ref-Haarnoja2017">
<p>Haarnoja, Tuomas, Haoran Tang, Pieter Abbeel, and Sergey Levine. 2017. “Reinforcement Learning with Deep Energy-Based Policies.” <em>arXiv:1702.08165 [Cs]</em>, February. <a href="http://arxiv.org/abs/1702.08165">http://arxiv.org/abs/1702.08165</a>.</p>
</div>
<div id="ref-Haarnoja2018a">
<p>Haarnoja, Tuomas, Aurick Zhou, Pieter Abbeel, and Sergey Levine. 2018. “Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor,” January. <a href="http://arxiv.org/abs/1801.01290">http://arxiv.org/abs/1801.01290</a>.</p>
</div>
<div id="ref-Hafner2011">
<p>Hafner, Roland, and Martin Riedmiller. 2011. “Reinforcement Learning in Feedback Control.” <em>Machine Learning</em> 84 (1-2): 137–69. <a href="https://doi.org/10.1007/s10994-011-5235-x">https://doi.org/10.1007/s10994-011-5235-x</a>.</p>
</div>
<div id="ref-Harutyunyan2016">
<p>Harutyunyan, A., M. G. Bellemare, T. Stepleton, and R. Munos. 2016. “Q(<span class="math inline">\(\lambda\)</span>) with Off-Policy Corrections.” <a href="http://arxiv.org/abs/1602.04951">http://arxiv.org/abs/1602.04951</a>.</p>
</div>
<div id="ref-Hausknecht2015">
<p>Hausknecht, Matthew, and Peter Stone. 2015. “Deep Recurrent Q-Learning for Partially Observable MDPs,” July. <a href="http://arxiv.org/abs/1507.06527">http://arxiv.org/abs/1507.06527</a>.</p>
</div>
<div id="ref-He2016">
<p>He, Frank S., Yang Liu, Alexander G. Schwing, and Jian Peng. 2016. “Learning to Play in a Day: Faster Deep Reinforcement Learning by Optimality Tightening,” November. <a href="http://arxiv.org/abs/1611.01606">http://arxiv.org/abs/1611.01606</a>.</p>
</div>
<div id="ref-He2015">
<p>He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015. “Deep Residual Learning for Image Recognition,” December. <a href="http://arxiv.org/abs/1512.03385">http://arxiv.org/abs/1512.03385</a>.</p>
</div>
<div id="ref-Heess2015">
<p>Heess, Nicolas, Greg Wayne, David Silver, Timothy Lillicrap, Yuval Tassa, and Tom Erez. 2015. “Learning Continuous Control Policies by Stochastic Value Gradients.” <em>Proc. International Conference on Neural Information Processing Systems</em>, 2944–52. <a href="http://dl.acm.org/citation.cfm?id=2969569">http://dl.acm.org/citation.cfm?id=2969569</a>.</p>
</div>
<div id="ref-Heinrich2015">
<p>Heinrich, Johannes, Marc Lanctot, and David Silver. 2015. “Fictitious Self-Play in Extensive-Form Games,” June, 805–13. <a href="http://proceedings.mlr.press/v37/heinrich15.html">http://proceedings.mlr.press/v37/heinrich15.html</a>.</p>
</div>
<div id="ref-Heinrich2016">
<p>Heinrich, Johannes, and David Silver. 2016. “Deep Reinforcement Learning from Self-Play in Imperfect-Information Games,” March. <a href="http://arxiv.org/abs/1603.01121">http://arxiv.org/abs/1603.01121</a>.</p>
</div>
<div id="ref-Hessel2017">
<p>Hessel, Matteo, Joseph Modayil, Hado van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan Horgan, Bilal Piot, Mohammad Azar, and David Silver. 2017. “Rainbow: Combining Improvements in Deep Reinforcement Learning,” October. <a href="http://arxiv.org/abs/1710.02298">http://arxiv.org/abs/1710.02298</a>.</p>
</div>
<div id="ref-Hochreiter1991">
<p>Hochreiter, Sepp. 1991. “Untersuchungen Zu Dynamischen Neuronalen Netzen.” PhD thesis. <a href="http://people.idsia.ch/~juergen/SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf">http://people.idsia.ch/~juergen/SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf</a>.</p>
</div>
<div id="ref-Hochreiter1997">
<p>Hochreiter, S, and J Schmidhuber. 1997. “Long Short-Term Memory.” <em>Neural Computation</em> 9 (8): 1735–80. <a href="http://www.ncbi.nlm.nih.gov/pubmed/9377276">http://www.ncbi.nlm.nih.gov/pubmed/9377276</a>.</p>
</div>
<div id="ref-Ioffe2015">
<p>Ioffe, Sergey, and Christian Szegedy. 2015. “Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift,” February. <a href="http://arxiv.org/abs/1502.03167">http://arxiv.org/abs/1502.03167</a>.</p>
</div>
<div id="ref-Kakade2001">
<p>Kakade, Sham. 2001. “A Natural Policy Gradient.” In <em>Advances in Neural Information Processing Systems 14</em>. <a href="https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf">https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf</a>.</p>
</div>
<div id="ref-Kakade2002">
<p>Kakade, Sham, and John Langford. 2002. “Approximately Optimal Approximate Reinforcement Learning.” <em>Proc. 19th International Conference on Machine Learning</em>, 267–74. <a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.7.7601">http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.7.7601</a>.</p>
</div>
<div id="ref-Kingma2013">
<p>Kingma, Diederik P, and Max Welling. 2013. “Auto-Encoding Variational Bayes,” December. <a href="http://arxiv.org/abs/1312.6114">http://arxiv.org/abs/1312.6114</a>.</p>
</div>
<div id="ref-Knight2018">
<p>Knight, Ethan, and Osher Lerner. 2018. “Natural Gradient Deep Q-Learning,” March. <a href="http://arxiv.org/abs/1803.07482">http://arxiv.org/abs/1803.07482</a>.</p>
</div>
<div id="ref-Krizhevsky2012">
<p>Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E Hinton. 2012. “ImageNet Classification with Deep Convolutional Neural Networks.” In <em>Advances in Neural Information Processing Systems (NIPS)</em>. <a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf</a>.</p>
</div>
<div id="ref-Levine2013">
<p>Levine, Sergey, and Vladlen Koltun. 2013. “Guided Policy Search.” In <em>Proceedings of Machine Learning Research</em>, 1–9. <a href="http://proceedings.mlr.press/v28/levine13.html">http://proceedings.mlr.press/v28/levine13.html</a>.</p>
</div>
<div id="ref-Li2017">
<p>Li, Yuxi. 2017. “Deep Reinforcement Learning: An Overview,” January. <a href="http://arxiv.org/abs/1701.07274">http://arxiv.org/abs/1701.07274</a>.</p>
</div>
<div id="ref-Lillicrap2015">
<p>Lillicrap, Timothy P., Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. 2015. “Continuous Control with Deep Reinforcement Learning.” <em>CoRR</em>. <a href="http://arxiv.org/abs/1509.02971">http://arxiv.org/abs/1509.02971</a>.</p>
</div>
<div id="ref-Lotzsch2017">
<p>Lötzsch, Winfried, Julien Vitay, and Fred H. Hamker. 2017. “Training a Deep Policy Gradient-Based Neural Network with Asynchronous Learners on a Simulated Robotic Problem.” In <em>INFORMATIK 2017. Gesellschaft Für Informatik</em>, edited by Maximilian Eibl and Martin Gaedke, 2143–54. Gesellschaft für Informatik, Bonn. <a href="https://dl.gi.de/handle/20.500.12116/3986">https://dl.gi.de/handle/20.500.12116/3986</a>.</p>
</div>
<div id="ref-Meuleau2000">
<p>Meuleau, Nicolas, Leonid Peshkin, Leslie P. Kaelbling, and Kee-eung Kim. 2000. “Off-Policy Policy Search.” <a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.28.894">http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.28.894</a>.</p>
</div>
<div id="ref-Mirowski2016">
<p>Mirowski, Piotr, Razvan Pascanu, Fabio Viola, Hubert Soyer, Andrew J. Ballard, Andrea Banino, Misha Denil, et al. 2016. “Learning to Navigate in Complex Environments,” November. <a href="http://arxiv.org/abs/1611.03673">http://arxiv.org/abs/1611.03673</a>.</p>
</div>
<div id="ref-Mnih2016">
<p>Mnih, Volodymyr, Adrià Puigdomènech Badia, Mehdi Mirza, Alex Graves, Timothy P. Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. 2016. “Asynchronous Methods for Deep Reinforcement Learning.” In <em>Proc. ICML</em>. <a href="http://arxiv.org/abs/1602.01783">http://arxiv.org/abs/1602.01783</a>.</p>
</div>
<div id="ref-Mnih2013">
<p>Mnih, Volodymyr, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. 2013. “Playing Atari with Deep Reinforcement Learning,” December. <a href="http://arxiv.org/abs/1312.5602">http://arxiv.org/abs/1312.5602</a>.</p>
</div>
<div id="ref-Mnih2015">
<p>Mnih, Volodymyr, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, et al. 2015. “Human-Level Control Through Deep Reinforcement Learning.” <em>Nature</em> 518 (7540): 529–33. <a href="https://doi.org/10.1038/nature14236">https://doi.org/10.1038/nature14236</a>.</p>
</div>
<div id="ref-Mousavi2018">
<p>Mousavi, Seyed Sajad, Michael Schukat, and Enda Howley. 2018. “Deep Reinforcement Learning: An Overview.” In, 426–40. Springer, Cham. <a href="https://doi.org/10.1007/978-3-319-56991-8_32">https://doi.org/10.1007/978-3-319-56991-8_32</a>.</p>
</div>
<div id="ref-Munos2016">
<p>Munos, Rémi, Tom Stepleton, Anna Harutyunyan, and Marc G. Bellemare. 2016. “Safe and Efficient Off-Policy Reinforcement Learning,” June. <a href="http://arxiv.org/abs/1606.02647">http://arxiv.org/abs/1606.02647</a>.</p>
</div>
<div id="ref-Nair2015">
<p>Nair, Arun, Praveen Srinivasan, Sam Blackwell, Cagdas Alcicek, Rory Fearon, Alessandro De Maria, Vedavyas Panneershelvam, et al. 2015. “Massively Parallel Methods for Deep Reinforcement Learning.” <a href="https://arxiv.org/pdf/1507.04296.pdf">https://arxiv.org/pdf/1507.04296.pdf</a>.</p>
</div>
<div id="ref-Nielsen2015">
<p>Nielsen, Michael A. 2015. <em>Neural Networks and Deep Learning</em>. Determination Press. <a href="http://neuralnetworksanddeeplearning.com/">http://neuralnetworksanddeeplearning.com/</a>.</p>
</div>
<div id="ref-Niu2011">
<p>Niu, Feng, Benjamin Recht, Christopher Re, and Stephen J. Wright. 2011. “HOGWILD!: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent.” In <em>Proc. Advances in Neural Information Processing Systems</em>, 21–21. <a href="http://arxiv.org/abs/1106.5730">http://arxiv.org/abs/1106.5730</a>.</p>
</div>
<div id="ref-ODonoghue2016">
<p>O’Donoghue, Brendan, Remi Munos, Koray Kavukcuoglu, and Volodymyr Mnih. 2016. “Combining Policy Gradient and Q-Learning.” <em>arXiv:1611.01626 [Cs, Math, Stat]</em>, November. <a href="http://arxiv.org/abs/1611.01626">http://arxiv.org/abs/1611.01626</a>.</p>
</div>
<div id="ref-Oh2018">
<p>Oh, Junhyuk, Yijie Guo, Satinder Singh, and Honglak Lee. 2018. “Self-Imitation Learning,” June. <a href="http://arxiv.org/abs/1806.05635">http://arxiv.org/abs/1806.05635</a>.</p>
</div>
<div id="ref-Pascanu2013">
<p>Pascanu, Razvan, and Yoshua Bengio. 2013. “Revisiting Natural Gradient for Deep Networks,” January. <a href="https://arxiv.org/abs/1301.3584">https://arxiv.org/abs/1301.3584</a>.</p>
</div>
<div id="ref-Peshkin2002">
<p>Peshkin, Leonid, and Christian R. Shelton. 2002. “Learning from Scarce Experience,” April. <a href="http://arxiv.org/abs/cs/0204043">http://arxiv.org/abs/cs/0204043</a>.</p>
</div>
<div id="ref-Peters2008">
<p>Peters, Jan, and Stefan Schaal. 2008. “Reinforcement Learning of Motor Skills with Policy Gradients.” <em>Neural Networks</em> 21 (4): 682–97. <a href="https://doi.org/10.1016/j.neunet.2008.02.003">https://doi.org/10.1016/j.neunet.2008.02.003</a>.</p>
</div>
<div id="ref-Pong2018">
<p>Pong, Vitchyr, Shixiang Gu, Murtaza Dalal, and Sergey Levine. 2018. “Temporal Difference Models: Model-Free Deep RL for Model-Based Control,” February. <a href="http://arxiv.org/abs/1802.09081">http://arxiv.org/abs/1802.09081</a>.</p>
</div>
<div id="ref-Popov2017">
<p>Popov, Ivaylo, Nicolas Heess, Timothy Lillicrap, Roland Hafner, Gabriel Barth-Maron, Matej Vecerik, Thomas Lampe, Yuval Tassa, Tom Erez, and Martin Riedmiller. 2017. “Data-Efficient Deep Reinforcement Learning for Dexterous Manipulation,” April. <a href="http://arxiv.org/abs/1704.03073">http://arxiv.org/abs/1704.03073</a>.</p>
</div>
<div id="ref-Precup2000">
<p>Precup, D., R. S Sutton, and S. Singh. 2000. “Eligibility Traces for Off-Policy Policy Evaluation.” In <em>Proceedings of the Seventeenth International Conference on Machine Learning.</em></p>
</div>
<div id="ref-Ruder2016">
<p>Ruder, Sebastian. 2016. “An Overview of Gradient Descent Optimization Algorithms,” September. <a href="http://arxiv.org/abs/1609.04747">http://arxiv.org/abs/1609.04747</a>.</p>
</div>
<div id="ref-Salimans2017">
<p>Salimans, Tim, Jonathan Ho, Xi Chen, Szymon Sidor, and Ilya Sutskever. 2017. “Evolution Strategies as a Scalable Alternative to Reinforcement Learning,” March. <a href="http://arxiv.org/abs/1703.03864">http://arxiv.org/abs/1703.03864</a>.</p>
</div>
<div id="ref-Schaul2015">
<p>Schaul, Tom, John Quan, Ioannis Antonoglou, and David Silver. 2015. “Prioritized Experience Replay,” November. <a href="http://arxiv.org/abs/1511.05952">http://arxiv.org/abs/1511.05952</a>.</p>
</div>
<div id="ref-Schulman2015">
<p>Schulman, John, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. 2015. “Trust Region Policy Optimization,” June, 1889–97. <a href="http://proceedings.mlr.press/v37/schulman15.html">http://proceedings.mlr.press/v37/schulman15.html</a>.</p>
</div>
<div id="ref-Schulman2015a">
<p>Schulman, John, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. 2015. “High-Dimensional Continuous Control Using Generalized Advantage Estimation,” June. <a href="http://arxiv.org/abs/1506.02438">http://arxiv.org/abs/1506.02438</a>.</p>
</div>
<div id="ref-Schulman2017">
<p>Schulman, John, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. “Proximal Policy Optimization Algorithms,” July. <a href="http://arxiv.org/abs/1707.06347">http://arxiv.org/abs/1707.06347</a>.</p>
</div>
<div id="ref-Silver2016">
<p>Silver, David, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, et al. 2016. “Mastering the Game of Go with Deep Neural Networks and Tree Search.” <em>Nature</em>. <a href="https://doi.org/10.1038/nature16961">https://doi.org/10.1038/nature16961</a>.</p>
</div>
<div id="ref-Silver2014">
<p>Silver, David, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller. 2014. “Deterministic Policy Gradient Algorithms.” In <em>Proc. ICML</em>, edited by Eric P Xing and Tony Jebara, 32:387–95. Proceedings of Machine Learning Research. PMLR. <a href="http://proceedings.mlr.press/v32/silver14.html">http://proceedings.mlr.press/v32/silver14.html</a>.</p>
</div>
<div id="ref-Simonyan2015">
<p>Simonyan, Karen, and Andrew Zisserman. 2015. “Very Deep Convolutional Networks for Large-Scale Image Recognition.” <em>International Conference on Learning Representations (ICRL)</em>, 1–14. <a href="https://doi.org/10.1016/j.infsof.2008.09.005">https://doi.org/10.1016/j.infsof.2008.09.005</a>.</p>
</div>
<div id="ref-Sutton1999">
<p>Sutton, Richard S., David McAllester, Satinder Singh, and Yishay Mansour. 1999. “Policy Gradient Methods for Reinforcement Learning with Function Approximation.” In <em>Proceedings of the 12th International Conference on Neural Information Processing Systems</em>, 1057–63. MIT Press. <a href="https://dl.acm.org/citation.cfm?id=3009806">https://dl.acm.org/citation.cfm?id=3009806</a>.</p>
</div>
<div id="ref-Sutton1998">
<p>Sutton, R. S., and A. G. Barto. 1998. <em>Reinforcement Learning: An Introduction</em>. Cambridge, MA: MIT press.</p>
</div>
<div id="ref-Sutton2017">
<p>———. 2017. <em>Reinforcement Learning: An Introduction</em>. 2nd ed. Cambridge, MA: MIT Press. <a href="http://incompleteideas.net/book/the-book-2nd.html">http://incompleteideas.net/book/the-book-2nd.html</a>.</p>
</div>
<div id="ref-Szita2006">
<p>Szita, István, and András Lörincz. 2006. “Learning Tetris Using the Noisy Cross-Entropy Method.” <em>Neural Computation</em> 18 (12): 2936–41. <a href="https://doi.org/10.1162/neco.2006.18.12.2936">https://doi.org/10.1162/neco.2006.18.12.2936</a>.</p>
</div>
<div id="ref-Tang2010">
<p>Tang, Jie, and Pieter Abbeel. 2010. “On a Connection Between Importance Sampling and the Likelihood Ratio Policy Gradient.” In <em>Adv. Neural Inf. Process. Syst.</em> <a href="http://rll.berkeley.edu/~jietang/pubs/nips10_Tang.pdf">http://rll.berkeley.edu/~jietang/pubs/nips10_Tang.pdf</a>.</p>
</div>
<div id="ref-Todorov2008">
<p>Todorov, E. 2008. “General Duality Between Optimal Control and Estimation.” In <em>2008 47th IEEE Conference on Decision and Control</em>, 4286–92. <a href="https://doi.org/10.1109/CDC.2008.4739438">https://doi.org/10.1109/CDC.2008.4739438</a>.</p>
</div>
<div id="ref-Toussaint2009">
<p>Toussaint, Marc. 2009. “Robot Trajectory Optimization Using Approximate Inference.” In <em>Proceedings of the 26th Annual International Conference on Machine Learning</em>, 1049–56. ICML ’09. New York, NY, USA: ACM. <a href="https://doi.org/10.1145/1553374.1553508">https://doi.org/10.1145/1553374.1553508</a>.</p>
</div>
<div id="ref-Uhlenbeck1930">
<p>Uhlenbeck, G. E., and L. S Ornstein. 1930. “On the Theory of the Brownian Motion.” <em>Physical Review</em> 36. <a href="https://doi.org/10.1103/PhysRev.36.823">https://doi.org/10.1103/PhysRev.36.823</a>.</p>
</div>
<div id="ref-vanHasselt2010">
<p>van Hasselt, Hado. 2010. “Double Q-Learning.” In <em>Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2</em>, 2613–21. Curran Associates Inc. <a href="https://dl.acm.org/citation.cfm?id=2997187">https://dl.acm.org/citation.cfm?id=2997187</a>.</p>
</div>
<div id="ref-vanHasselt2015">
<p>van Hasselt, Hado, Arthur Guez, and David Silver. 2015. “Deep Reinforcement Learning with Double Q-Learning,” September. <a href="http://arxiv.org/abs/1509.06461">http://arxiv.org/abs/1509.06461</a>.</p>
</div>
<div id="ref-Wang2017">
<p>Wang, Ziyu, Victor Bapst, Nicolas Heess, Volodymyr Mnih, Remi Munos, Koray Kavukcuoglu, and Nando de Freitas. 2017. “Sample Efficient Actor-Critic with Experience Replay,” November. <a href="http://arxiv.org/abs/1611.01224">http://arxiv.org/abs/1611.01224</a>.</p>
</div>
<div id="ref-Wang2016">
<p>Wang, Ziyu, Tom Schaul, Matteo Hessel, Hado van Hasselt, Marc Lanctot, and Nando de Freitas. 2016. “Dueling Network Architectures for Deep Reinforcement Learning.” In <em>Proc. ICML</em>. <a href="http://arxiv.org/abs/1511.06581">http://arxiv.org/abs/1511.06581</a>.</p>
</div>
<div id="ref-Watkins1989">
<p>Watkins, Christopher JCH. 1989. “Learning from Delayed Rewards.” PhD thesis.</p>
</div>
<div id="ref-Wierstra2007">
<p>Wierstra, Daan, Alexander Foerster, Jan Peters, and Jürgen Schmidhuber. 2007. “Solving Deep Memory POMDPs with Recurrent Policy Gradients.” In, 697–706. Springer, Berlin, Heidelberg. <a href="https://doi.org/10.1007/978-3-540-74690-4_71">https://doi.org/10.1007/978-3-540-74690-4_71</a>.</p>
</div>
<div id="ref-Williams1992">
<p>Williams, R. J. 1992. “Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning.” <em>Machine Learning</em> 8: 229–56.</p>
</div>
<div id="ref-Williams1991">
<p>Williams, Ronald J, and Jing Peng. 1991. “Function Optimization Using Connectionist Reinforcement Learning Algorithms.” <em>Connection Science</em> 3 (3): 241–68.</p>
</div>
</div>


</article>
</body>
</html>
