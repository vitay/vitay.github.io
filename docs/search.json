[
  {
    "objectID": "research/annarchy.html",
    "href": "research/annarchy.html",
    "title": "ANNarchy",
    "section": "",
    "text": "Neuro-computational models are different from classical neural networks (deep learning) in many aspects:\n\nThe complexity of the neurons, whose activity is governed by one or several differential equations instead of a simple weighted sum.\nThe complexity and diversity of the learning rules (synaptic plasticity), compared to gradient descent.\nThe size of the networks needed to simulate significant parts of the brain.\nThe huge diversity of models, architectures, frameworks used by researchers in computational neuroscience.\n\nThe increasing size of such networks asks for efficient parallel simulations, using distributed systems (OpenMP, MPI) or GPUs (CUDA). However, computational neuroscientists cannot be expected to be also experts in parallel computing. There is a need for a general-purpose neuro-simulator, with an easy but flexible interface allowing to define a huge variety of models, but which is internally efficient and allows for fast parallel simulations on various hardwares.\nOver many years, we have developed ANNarchy (Artificial Neural Networks architect), a parallel simulator for distributed rate-coded or spiking neural networks. The definition of the models is made in Python, but the library generates optimized C++ code to actually run the simulation on parallel hardware, using either openMP or CUDA. The current stable version is 4.7 and is released under the GNU GPL v2 or later.\nThe code is available at:\nhttps://github.com/ANNarchy/ANNarchy\nThe documentation is available at:\nhttps://annarchy.github.io\n\n\nANNarchy separates the description of a neural network from its simulation. The description is declared in a Python script, offering high flexibility and readability of the code, and allowing to use the huge ecosystem of scientific libraries available with Python (Numpy, Scipy, Matplotlib…). Using Python furthermore reduces the programming effort to a minimum, letting the modeller concentrate on network design and data analysis.\n\nA neural network is defined as a collection of interconnected populations of neurons. Each population comprises a set of similar artificial neurons (rate-coded or spiking point-neurons), whose activity is ruled by one or many ordinary differential equations. The activity of a neuron depends on the activity of other neurons through synapses, whose strength can evolve with time depending on pre- or post-synaptic activities (synaptic plasticity). Populations are interconnected with each other through projections, which contain synapses between two populations.\nANNarchy provides a set of classical neuron or synapse models, but also allows the definition of specific models. The ordinary differential equations (ODE) governing neural or synaptic dynamics have to be specified by the modeler. Contrary to other simulators (except Brian) which require to code these modules in a low-level language, ANNarchy provides a mathematical equation parser which can generate optimized C++ code depending on the chosen parallel framework. Bindings from C++ to Python are generated thanks to Cython (C-extensions to Python), which is a static compiler for Python. These bindings allow the Python script to access all data generated by the simulation (neuronal activity, connection weights) as if they were simple Python attributes. However, the simulation itself is independent from Python and its relatively low performance.\n\n\n\nTo demonstrate the simplicity of ANNarchy’s interface, let’s focus on the “Hello, World!” of spiking networks: the pulse-coupled network of Izhikevich neurons (Izhikevich, 2003). It can be defined in ANNarchy as:\nfrom ANNarchy import *\n\n# Create the excitatory and inhibitory population\npop = Population(geometry=1000, neuron=Izhikevich)\nExc = pop[:800]                 ; Inh = pop[800:]\n\n# Set the population parameters\nre = np.random.random(800)      ; ri = np.random.random(200)\nExc.noise = 5.0                 ; Inh.noise = 2.0\nExc.a = 0.02                    ; Inh.a = 0.02 + 0.08 * ri\nExc.b = 0.2                     ; Inh.b = 0.25 - 0.05 * ri\nExc.c = -65.0 + 15.0 * re**2    ; Inh.c = -65.0\nExc.d = 8.0 - 6.0 * re**2       ; Inh.d = 2.0\nExc.v = -65.0                   ; Inh.v = -65.0\nExc.u = Exc.v * Exc.b           ; Inh.u = Inh.v * Inh.b\n\n# Create the projections\nexc_proj = Projection(pre=Exc, post=pop, target='exc')\nexc_proj.connect_all_to_all(weights=Uniform(0.0, 0.5))\n\ninh_proj = Projection(pre=Inh, post=pop, target='inh')\ninh_proj.connect_all_to_all(weights=Uniform(0.0, 1.0))\n\n# Compile\ncompile()\n\n# Start recording the spikes in the network to produce the plots\nM = Monitor(pop, ['spike', 'v'])\n\n# Simulate 1 second\nsimulate(1000.0, measure_time=True)\n\n# Retrieve the spike recordings and the membrane potential\nspikes = M.get('spike')\nv = M.get('v')\n\n# Compute the raster plot\nt, n = M.raster_plot(spikes)\n\n# Compute the population firing rate\nfr = M.histogram(spikes)\n\n# Plot the results\nimport matplotlib.pyplot as plt\nax = plt.subplot(3,1,1)\nax.plot(t, n, 'b.', markersize=1.0)\nax = plt.subplot(3,1,2)\nax.plot(v[:, 15])\nax = plt.subplot(3,1,3)\nax.plot(fr)\nplt.show()"
  },
  {
    "objectID": "research/annarchy.html#related-publications",
    "href": "research/annarchy.html#related-publications",
    "title": "ANNarchy",
    "section": "Related publications",
    "text": "Related publications\nOliver Maith, Helge Ülo Dinkelbach, Javier Baladron, Julien Vitay, and Fred H. Hamker (2022).\nBOLD monitoring in the neural simulator ANNarchy.\nFrontiers in Neuroinformatics 16:790966\ndoi:10.3389/fninf.2022.790966\nHelge Ülo Dinkelbach, Badr-Eddine Bouhlal, Julien Vitay, and Fred H. Hamker (2022).\nAuto-selection of an optimal sparse matrix format in the neuro-simulator ANNarchy.\n*Frontiers in Neuroinformatics 16:877945\ndoi:10.3389/fninf.2022.877945\nHelge Ülo Dinkelbach, Julien Vitay, and Fred H. Hamker (2019).\nScalable simulation of rate-coded and spiking neural networks on shared memory systems.\n2019 Conference on Cognitive Computational Neuroscience, 13-16 September 2019, Berlin, Germany.\ndoi:10.32470/CCN.2019.1109-0\nJulien Vitay, Helge Ülo Dinkelbach, and Fred H. Hamker (2015).\nANNarchy: a code generation approach to neural simulations on parallel hardware.\nFrontiers in Neuroinformatics 9:19\ndoi:10.3389/fninf.2015.00019\nHelge Ü. Dinkelbach, Julien Vitay, and Fred H. Hamker (2012).\nComparison of GPU- and CPU-implementations of mean-firing rate neural networks on parallel hardware.\nNetwork: Computation in Neural Systems 23(4)\ndoi:10.3109/0954898X.2012.739292"
  },
  {
    "objectID": "research/basalganglia.html",
    "href": "research/basalganglia.html",
    "title": "Julien Vitay",
    "section": "",
    "text": "The Basal Ganglia (BG) are a set of nuclei located in the basal forebrain, receiving inputs mostly from the cerebral cortex (especially the frontal lobe) and projecting on various motor centers, as well as back to the cortex through the thalamus, forming a closed-loop.\nThe main input station is the striatum, which can be anatomically divided into three parts: the nucleus accumbens (NAcc), the caudate nucleus (CN) and the putamen (PUT). Striatal neurons are excited by cortical activity and inhibit in turn the tonically active neurons of the output nuclei of the BG: the substantia nigra pars reticulata (SNr) and the internal segment of the globus pallidus (GPi). These output structures further inhibit some motor centers and thalamic nuclei.\n\nThis double inhibition allows to selectively open some recurrent loops between the thalamus and the cortex, increasing the signal-to-noise ratio in the cortex and triggering movements or cognitive functions.\nOther nuclei in the BG, such as the subthalamic nucleus (STN) and the external part of the globus pallidus (GPe), create functionally different pathways to allow for a more complex role of BG in adapting behavior.\nThe main characteristic of the BG is its dense innervation by dopaminergic (DA) cells in the substantia nigra pars compacta (SNc) and ventral tegmental area (VTA), whose firing is related to reward delivery and prediction. DA can modulate the activation and learning of most cells in the BG, placing it as a core structure in reinforcement learning processes.\n\n\n\nCarolin Scholl, Javier Baladron, Julien Vitay, and Fred H. Hamker (2022).\nEnhanced Habit Formation in Tourette Patients Explained by Shortcut Modulation in a Hierarchical Cortico-Basal Ganglia Model.\nBrain Structure and Function 227, 1031-1050\ndoi:10.1007/s00429-021-02446-x\nFrancesc Villagrasa, Javier Baladron, Julien Vitay, Henning Schroll, Evan G. Antzoulatos, Earl K. Miller, and Fred H. Hamker (2018).\nOn the role of cortex-basal ganglia interactions for category learning: A neuro-computational approach.\nJournal of Neuroscience 38(44) 9551-9562\ndoi:10.1523/JNEUROSCI.0874-18.2018\nHenning Schroll, Julien Vitay, and Fred H. Hamker (2014).\nDysfunctional and compensatory synaptic plasticity in Parkinson’s disease.\nEuropean Journal of Neuroscience, 39: 688-702\ndoi:10.1111/ejn.12434\nHenning Schroll, Julien Vitay, and Fred H. Hamker (2012).\nWorking memory and response selection: A computational account of interactions among cortico-basalganglio-thalamic loops.\nNeural Networks, 26\ndoi:10.1016/j.neunet.2011.10.008\nJulien Vitay and Fred H. Hamker (2012).\nBasal Ganglia learning.\nIn Encyclopedia of the Sciences of Learning. Seel, Norbert M. (Ed.).\ndoi:10.1007/978-1-4419-1428-6\nJulien Vitay and Fred H. Hamker (2010).\nA computational model of basal ganglia and its role in memory retrieval in rewarded visual memory tasks.\nFrontiers in Computational Neuroscience 4(13)\ndoi:10.3389/fncom.2010.00013"
  },
  {
    "objectID": "research/reservoircomputing.html",
    "href": "research/reservoircomputing.html",
    "title": "Julien Vitay",
    "section": "",
    "text": "The field of reservoir computing, covering Echo-State Networks (ESN; Jaeger, 2000) and Liquid State Machines (Maas, 2001), studies the dynamical properties of recurrent neural networks. Depending on the strength of the recurrent connections, the reservoirs can exhibit either deterministic or chaotic trajectories following a stimulation.\n\nThese trajectories can serve as a complex temporal basis to represent events. In their early formulation, reservoirs were fixed and read-out neurons used this basis to mimic specific target signals using supervised learning.\nIn the recent years, methods have been developed to train the connections inside the reservoir too, either using supervised learning (e.g. Laje and Buonomano 2013) or reinforcement learning (Miconi, 2017). This unleashes the potential of reservoirs for both machine learning applications and computational neuroscience.\nWe study the properties of reservoir computing at the neuroscientific level, with an emphasis on reinforcement learning, forward models in the cerebellum (Schmid et al, 2019) or interval timing.\n\n\n\nKatharina Schmid, Julien Vitay, and Fred H. Hamker (2019).\nForward Models in the Cerebellum using Reservoirs and Perturbation Learning.\n2019 Conference on Cognitive Computational Neuroscience, 13-16 September 2019, Berlin, Germany\ndoi:10.32470/CCN.2019.1139-0\nJulien Vitay (2016).\n[Re] Robust timing and motor patterns by taming chaos in recurrent neural networks.\nRescience, 2(1)\ndoi:10.5281/zenodo.159545"
  },
  {
    "objectID": "research/reinforcementlearning.html",
    "href": "research/reinforcementlearning.html",
    "title": "Julien Vitay",
    "section": "",
    "text": "Deep reinforcement learning (deep RL) is the integration of deep learning methods, classically used in supervised or unsupervised learning contexts, with reinforcement learning (RL), a well-studied adaptive control method used in problems with delayed and partial feedback.\n\nCourse on deep RL taught at the TU Chemnitz:\nhttps://julien-vitay.net/course-deeprl\n\n\n\nWinfried Lötzsch, Julien Vitay, and Fred H. Hamker (2017).\nTraining a deep policy gradient-based neural network with asynchronous learners on a simulated robotic problem.\nIn: Eibl, M. & Gaedke, M. (Eds.), INFORMATIK 2017. Gesellschaft für Informatik, Bonn. (S. 2143-2154)\ndoi:10.18420/in2017_214"
  },
  {
    "objectID": "research/dopamine.html",
    "href": "research/dopamine.html",
    "title": "Julien Vitay",
    "section": "",
    "text": "The dopaminergic system is composed of the ventral tegmental area (VTA) and the substantia nigra pars compacta (SNc). The neurotransmitter dopamine (DA) released by neurons in these two small areas exerts a strong influence on neural excitability and plasticity in many brain areas: mostly the basal ganglia (BG), but also the prefrontal cortex, the hippocampus or the amygdala.\nA striking feature of VTA cells is their response during classical (or Pavlovian) conditioning, as observed by Schultz et al (1998). Early on, VTA cells respond phasically (a burst) to unconditioned stimuli (US, or rewards in operant conditioning). Gradually during learning, the amplitude of this response decreases, replaced by a response to the conditioned stimuli (CS) which are predictive of reward delivery. Moreover, if a reward is predicted by the CS but omitted, VTA cells show a brief depression of activity (a dip) at the time where the US was expected. This pattern resembles the temporal difference (TD) error signal used in reinforcement learning, what generated multitudes of models based on that analogy.\n\nWhat remains unclear is how VTA cells access information about the US, the CS and more importantly the time elapsed since CS onset. The goal of this research project is to investigate the mechanisms by which VTA is able to exhibit these properties, by looking at the afferent system to VTA. VTa indeed receives information from many brain areas, either directly as the rostromedial tegmental area (RMTg), the pedunculopontine nucleus (PPTN) or the nucleus accumbens (NAcc), or indirectly as the amygdala, the lateral habenula (LHb), the ventral pallidum (VP) or the ventromedial prefrontal cortex (vmPFC).\n\n\n\nJulien Vitay (2017).\nOn the role of dopamine in motivated behavior: a neuro-computational approach.\nHabilitation (Technische Universität Chemnitz).\n\nJulien Vitay and Fred H. Hamker (2014).\nTiming and expectation of reward: a neuro-computational model of the afferents to the ventral tegmental area.\nFrontiers in Neurorobotics. 8:4\ndoi:10.3389/fnbot.2014.00004\nJulien Vitay and Fred H. Hamker (2007).\nOn the role of dopamine in cognitive vision.\nIn: Paletta, L., Rome, E. (eds) Attention in Cognitive Systems. Theories and Systems from an Interdisciplinary Viewpoint. WAPCV 2007. Lecture Notes in Computer Science(), vol 4840. Springer, Berlin, Heidelberg\ndoi:10.1007/978-3-540-77343-6_23"
  },
  {
    "objectID": "research/hippocampus.html",
    "href": "research/hippocampus.html",
    "title": "Julien Vitay",
    "section": "",
    "text": "The hippocampus is a key structure for episodic memory and spatial navigation. A fundamental step in hippocampus research was the discovery of place cells, which fire whenever an animal traverses a certain location known as the place field (O’Keefe and Nadel, 1978). At rest, place cells exhibit brief periods of fast oscillations termed sharp wave-ripples. During these events, place cell activity shows sequential patterns called forward replay and reverse replay: time-compressed, and sometimes time-reversed, reproductions of previously experienced sequences. Spatial experiences stored in the hippocampus can therefore be recalled at will during behavior.\nOur modeling work focuses on understanding how these replay patterns are learned and used for high-level cognitive processes such as decision-making and model-based reinforcement learning.\n\n\n\n\nLorenz Gönner, Julien Vitay, and Fred H. Hamker (2017).\nPredictive Place-Cell Sequences for Goal-Finding Emerge from Goal Memory and the Cognitive Map: A Computational Model.\nFrontiers in Computational Neuroscience 11:84\ndoi:10.3389/fncom.2017.00084"
  },
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "Level: Master.\nResponsability: lectures and exercises.\nCourse website: https://www.tu-chemnitz.de/informatik/KI/edu/neurocomputing\nMaterials: https://julien-vitay.net/course-neurocomputing/\n\n\n\nLinear algorithms\n\nOptimization\nLinear regression\nLinear classification\nLearning theory\n\nNeural networks\n\nMulti-layer perceptron\nModern neural networks\n\nComputer Vision\n\nConvolutional neural networks\nObject detection\nSemantic segmentation\n\nGenerative modeling\n\nAutoencoders\nRestricted Boltzmann machines\nGenerative adversarial networks\n\nRecurrent neural networks\n\nRecurrent neural networks, LSTM\nNatural Language Processing\nAttentional neural networks\n\nSelf-supervised learning\n\nTransformers\nContrastive learning"
  },
  {
    "objectID": "teaching.html#deep-reinforcement-learning",
    "href": "teaching.html#deep-reinforcement-learning",
    "title": "Teaching",
    "section": "Deep Reinforcement Learning",
    "text": "Deep Reinforcement Learning\nLevel: Master.\nResponsability: lectures and exercises.\nCourse website: https://www.tu-chemnitz.de/informatik/KI/edu/deeplrl\nMaterials: https://julien-vitay.net/course-deeprl/\n\nSyllabus\n\nTabular RL\n\nBandits\nMarkov Decision Processes\nDynamic Programming\nMonte Carlo control\nTemporal Difference\nFunction approximation\nDeep learning\n\nModel-free RL\n\nDeep Q-network\nBeyond DQN\nPolicy Gradient\nA2C / A3C\nDDPG\nTRPO / PPO\nSAC\n\nModel-based RL\n\nModel-based RL\nLearned models\nAlphaGo\nSuccessor representations"
  },
  {
    "objectID": "teaching.html#introduction-to-ai",
    "href": "teaching.html#introduction-to-ai",
    "title": "Teaching",
    "section": "Introduction to AI",
    "text": "Introduction to AI\nLevel: Bachelor.\nResponsability: exercises.\nCourse website: https://www.tu-chemnitz.de/informatik/KI/edu/ki\n\nSyllabus\n\nBlind search\nHeuristic search\nGame theory\nConstraint propagation\nOptimization\nNeural networks\nSupport vector machines\nProbability theory\nInformation theory\nDecision trees\nEstimators\nReinforcement learning"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "Description\nThe Basal Ganglia (BG) are a set of nuclei located in the basal forebrain, receiving inputs mostly from the cerebral cortex and projecting to various motor centers, as well as back to the cortex through the thalamus, forming a closed-loop. It is involved in major functions such as reinforcement learning, habit formation, planning and motor control, but also in diseases such as Parkinson’s disease or Tourette syndrome.\n\n\n\n\n\n\n\n\n\nDescription\nThe dopaminergic system is composed of the ventral tegmental area (VTA) and the substantia nigra pars compacta (SNc). The neurotransmitter dopamine (DA) released by neurons in these two small areas exerts a strong influence on neural excitability and plasticity in many brain areas: mostly the basal ganglia (BG), but also the prefrontal cortex, the hippocampus or the amygdala.\n\n\n\n\n\n\n\n\n\nDescription\nThe hippocampus is a key structure for episodic memory and spatial navigation. A fundamental step in hippocampus research was the discovery of place cells, which fire whenever an animal traverses a certain location known as the place field (O’Keefe and Nadel, 1978). At rest, place cells exhibit brief periods of fast oscillations termed sharp wave-ripples. During these events, place cell activity shows sequential patterns called forward replay and reverse replay: time-compressed, and sometimes time-reversed, reproductions of previously experienced sequences. Spatial experiences stored in the hippocampus can therefore be recalled at will during behavior."
  },
  {
    "objectID": "research.html#neurocomputing",
    "href": "research.html#neurocomputing",
    "title": "Research",
    "section": "Neurocomputing",
    "text": "Neurocomputing\n\nNeurosimulator ANNarchy\n\n\nDescription\nI develop with Helge Dinkelbach ANNarchy (Artificial Neural Networks architect), a parallel simulator for distributed rate-coded or spiking neural networks. The definition of the models is in Python, but the library generates optimized C++ code to actually run the simulation on parallel hardware, using either openMP or CUDA."
  },
  {
    "objectID": "research.html#machine-learning",
    "href": "research.html#machine-learning",
    "title": "Research",
    "section": "Machine Learning",
    "text": "Machine Learning\n\nReservoir computing\n\n\nDescription\nReservoir computing studies the dynamical properties of recurrently connected populations of neurons. Their rich dynamics allow to represent and learn complex tasks currently out of reach of the classical machine learning methods, but also allow to better understand brain activities.\n\n\n\n\n\n\nReinforcement learning\n\n\nDescription\nReinforcement Learning (RL) is a machine learning framework studying how to derive optimal policies from reward signals. Coupled with deep neural networks, it became the most promising approach to artificial intelligence."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Dr. habil. Julien Vitay",
    "section": "",
    "text": "Researcher and lecturer at the Chemnitz University of Technology, in the lab of Artificial Intelligence, Department of Computer Science."
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "Dr. habil. Julien Vitay",
    "section": "Contact",
    "text": "Contact\njulien.vitay@informatik.tu-chemnitz.de\n(+49) 371 531 39468\nStraße der Nationen 62, D-09107 Chemnitz\nRoom 1/348"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "Carolin Scholl, Javier Baladron, Julien Vitay, and Fred H. Hamker (2022).Enhanced Habit Formation in Tourette Patients Explained by Shortcut Modulation in a Hierarchical Cortico-Basal Ganglia Model. Brain Structure and Function 227, 1031-1050 doi:10.1007/s00429-021-02446-x\n        \n        URL\n     \n        \n        PDF\n     \n        \n        Preprint\n    \nOliver Maith, Helge Ülo Dinkelbach, Javier Baladron, Julien Vitay, and Fred H. Hamker (2022).BOLD monitoring in the neural simulator ANNarchy. Frontiers in Neuroinformatics 16:790966 doi:10.3389/fninf.2022.790966\n        \n        URL\n     \n        \n        PDF\n     \n        \n        Code\n    \nHelge Ülo Dinkelbach, Badr-Eddine Bouhlal, Julien Vitay, and Fred H. Hamker (2022).Auto-selection of an optimal sparse matrix format in the neuro-simulator ANNarchy. Frontiers in Neuroinformatics 16:877945 doi:10.3389/fninf.2022.877945\n        \n        URL\n     \n        \n        PDF\n     \n        \n        Code\n    \nAida Farahani, Julien Vitay, and Fred H. Hamker (2022).Deep Neural Networks for Geometric Shape Deformation. In: Bergmann, R., Malburg, L., Rodermund, S.C., Timm, I.J. (eds) KI 2022: Advances in Artificial Intelligence. KI 2022. Lecture Notes in Computer Science(), vol 13404. Springer, Cham doi:10.1007/978-3-031-15791-2_9\n        \n        URL\n     \n        \n        PDF\n    \nNicolas Kuske, Marco Ragni, Florian Röhrbein, Julien Vitay, and Fred H. Hamker (2022).Demands and potentials of different levels of neuro-cognitive models für human spatial cognition. In: E. Ferstl, L. Konieczny, & R. Stülpnagel  (Eds.), Proceedings of KogWiss2022, the 15th Biannual Conference of the German Society for Cognitive Science (pp. 115-116) doi:10.6094/UNIFR/229611\n        \n        URL\n     \n        \n        PDF\n    \n2021\nChaithanya Kumar Mummadi, Ranjitha Subramaniam, Robin Hutmacher, Julien Vitay, Volker Fischer, and Jan Hendrik Metzen (2021).Does enhanced shape bias improve neural network robustness to common corruptions?. International Conference on Learning Representations (ICLR)\n        \n        URL\n     \n        \n        PDF\n    \nAida Farahani, Julien Vitay, and Fred H. Hamker (2021).Geometric Deep Learning and solutions for the industry. Workshop 3D-NordOst 2021. Tagungsband 23. Anwendungsbezogener Workshop zur Erfassung, Modellierung, Verarbeitung und Auswertung von 3D-Daten, Berlin, 02./03.12.2021: 105-113. ISBN: 978-3-942709-27-9\nPayam Atoofi, Julien Vitay, and Fred H. Hamker (2021).Geometric Deep Learning: Graph Neural Networks, Challenges, and Breakthroughs. Workshop 3D-NordOst 2021. Tagungsband 23. Anwendungsbezogener Workshop zur Erfassung, Modellierung, Verarbeitung und Auswertung von 3D-Daten, Berlin, 02./03.12.2021:115-124. ISBN: 978-3-942709-27-9\n2020\nEnrico Schröder, Mirko Mählisch, Julien Vitay, and Fred H. Hamker (2020).Monocular 3D Object Detection Using Feature Map Transformation: Towards Learning Perspective-Invariant Scene Representations. 2020 Fourth IEEE International Conference on Robotic Computing (IRC) doi:10.1109/IRC.2020.00066\n        \n        URL\n    \nEnrico Schröder, Sascha Braun, Mirko Mählisch, Julien Vitay, and Fred H. Hamker (2020).Feature Map Transformation for Multi-sensor Fusion in Object Detection Networks for Autonomous Driving. In: Arai, K., Kapoor, S. (eds) Advances in Computer Vision. CVC 2019. Advances in Intelligent Systems and Computing, vol 944. Springer, Cham doi:10.1007/978-3-030-17798-0_12\n        \n        URL\n     \n        \n        PDF\n    \n2019\nKatharina Schmid, Julien Vitay, and Fred H. Hamker (2019).Forward Models in the Cerebellum using Reservoirs and Perturbation Learning. 2019 Conference on Cognitive Computational Neuroscience, 13-16 September 2019, Berlin, Germany doi:10.32470/CCN.2019.1139-0\n        \n        URL\n     \n        \n        PDF\n     \n        \n        Poster\n    \nHelge Ülo Dinkelbach, Julien Vitay, and Fred H. Hamker (2019).Scalable simulation of rate-coded and spiking neural networks on shared memory systems. 2019 Conference on Cognitive Computational Neuroscience, 13-16 September 2019, Berlin, Germany doi:10.32470/CCN.2019.1109-0\n        \n        URL\n     \n        \n        PDF\n     \n        \n        Poster\n    \nValentin Forch, Julien Vitay, and Fred H. Hamker (2019).Recurrent Spatial Attention for Facial Emotion Recognition. In Proceedings of Workshop Localize IT, Chemnitz Linux-Tage, Chemnitz (Germany).\n        \n        PDF\n    \n2018\nFrancesc Villagrasa, Javier Baladron, Julien Vitay, Henning Schroll, Evan G. Antzoulatos, Earl K. Miller, and Fred H. Hamker (2018).On the role of cortex-basal ganglia interactions for category learning: A neuro-computational approach. Journal of Neuroscience 38(44) 9551-9562 doi:10.1523/JNEUROSCI.0874-18.2018\n        \n        URL\n     \n        \n        PDF\n    \nEnrico Schröder, Mirko Mählisch, Julien Vitay, and Fred H. Hamker (2018).Fusion of Camera and Lidar Data for Object Detection using Neural Networks. In Proceedings of 12. Workshop Fahrerassistenzsysteme und automatisiertes Fahren FAS 2018, 26-28.09.2018, Walting im Altmühltal (Germany). pp138-146. Darmstadt:Uni-DAS e.V.\n        \n        PDF\n    \n2017\nJulien Vitay (2017).On the role of dopamine in motivated behavior: a neuro-computational approach.. Habilitation (Technische Universität Chemnitz).\n        \n        PDF\n    \nLorenz Gönner, Julien Vitay, and Fred H. Hamker (2017).Predictive Place-Cell Sequences for Goal-Finding Emerge from Goal Memory and the Cognitive Map: A Computational Model. Frontiers in Computational Neuroscience 11:84 doi:10.3389/fncom.2017.00084\n        \n        URL\n     \n        \n        PDF\n    \nNicolas P. Rougier, Konrad Hinsen, Frédéric Alexandre, Thomas Arildsen, Lorena A. Barba, Fabien C.Y. Benureau, C. Titus Brown, Pierre de Buyl, Ozan Caglayan, Andrew P. Davison, Marc-André Delsuc, Georgios Detorakis, Alexandra K. Diem, Damien Drix, Pierre Enel, Benoît Girard, Olivia Guest, Matt G. Hall, Rafael N. Henriques, Xavier Hinaut, Kamil S. Jaron, Mehdi Khamassi, Almar Klein, Tiina Manninen, Pietro Marchesi, Daniel McGlinn, Christoph Metzner, Owen Petchey, Hans Ekkehard Plesser, Timothée Poisot, Karthik Ram, Yoav Ram, Etienne Roesch, Cyrille Rossant, Vahid Rostami, Aaron Shifman, Joseph Stachelek, Marcel Stimberg, Frank Stollmeier, Federico Vaggi, Guillaume Viejo, Julien Vitay, Anya E. Vostinar, Roman Yurchak, and Tiziano Zito (2017).Sustainable computational science: the ReScience initiative. PeerJ Computer Science 3:e142 doi:10.7717/peerj-cs.142\n        \n        URL\n     \n        \n        PDF\n    \nWinfried Lötzsch, Julien Vitay, and Fred H. Hamker (2017).Training a deep policy gradient-based neural network with asynchronous learners on a simulated robotic problem. In: Eibl, M. & Gaedke, M. (Eds.), INFORMATIK 2017. Gesellschaft für Informatik, Bonn. (S. 2143-2154) doi:10.18420/in2017_214\n        \n        URL\n     \n        \n        PDF\n    \n2016\nJulien Vitay (2016).[Re] Robust timing and motor patterns by taming chaos in recurrent neural networks. Rescience, 2(1) doi:10.5281/zenodo.159545\n        \n        URL\n     \n        \n        PDF\n     \n        \n        Code\n    \n2015\nJulien Vitay, Helge Ülo Dinkelbach, and Fred H. Hamker (2015).ANNarchy: a code generation approach to neural simulations on parallel hardware. Frontiers in Neuroinformatics 9:19 doi:10.3389/fninf.2015.00019\n        \n        URL\n     \n        \n        PDF\n     \n        \n        Code\n    \n2014\nJulien Vitay and Fred H. Hamker (2014).Timing and expectation of reward: a neuro-computational model of the afferents to the ventral tegmental area. Frontiers in Neurorobotics. 8:4 doi:10.3389/fnbot.2014.00004\n        \n        URL\n     \n        \n        PDF\n     \n        \n        Code\n    \nHenning Schroll, Julien Vitay, and Fred H. Hamker (2014).Dysfunctional and compensatory synaptic plasticity in Parkinson's disease. European Journal of Neuroscience, 39: 688-702 doi:10.1111/ejn.12434\n        \n        URL\n     \n        \n        PDF\n    \n2012\nHelge Ü. Dinkelbach, Julien Vitay, and Fred H. Hamker (2012).Comparison of GPU- and CPU-implementations of mean-firing rate neural networks on parallel hardware. Network: Computation in Neural Systems 23(4) doi:10.3109/0954898X.2012.739292\n        \n        URL\n     \n        \n        PDF\n    \nHenning Schroll, Julien Vitay, and Fred H. Hamker (2012).Working memory and response selection: A computational account of interactions among cortico-basalganglio-thalamic loops. Neural Networks, 26 doi:10.1016/j.neunet.2011.10.008\n        \n        URL\n     \n        \n        PDF\n    \nJulien Vitay and Fred H. Hamker (2012).Basal Ganglia learning. In Encyclopedia of the Sciences of Learning. Seel, Norbert M. (Ed.). doi:10.1007/978-1-4419-1428-6\n        \n        URL\n    \n2011\nJulien Vitay and Fred H. Hamker (2011).A Neuroscientific View on the Role of Emotions in Behaving Cognitive Agents. Künstliche Intelligenz 25(3) doi:10.1007/s13218-011-0106-y\n        \n        PDF\n    \n2010\nJulien Vitay and Fred H. Hamker (2010).A computational model of basal ganglia and its role in memory retrieval in rewarded visual memory tasks. Frontiers in Computational Neuroscience 4(13) doi:10.3389/fncom.2010.00013\n        \n        URL\n     \n        \n        PDF\n    \n2009\nJulien Vitay, Jérémy Fix, Frederik Beuth, Henning Schroll, and Fred H. Hamker (2009).Biological Models of Reinforcement Learning. Künstliche Intelligenz 23(3)\n        \n        PDF\n    \n2008\nJulien Vitay and Fred H. Hamker (2008).Sustained Activities and Retrieval in a Computational Model of the Perirhinal Cortex. Journal of Cognitive Neuroscience 20(11) doi:10.1162/jocn.2008.20147\n        \n        URL\n     \n        \n        PDF\n    \n2007\nJulien Vitay and Fred H. Hamker (2007).On the role of dopamine in cognitive vision. In: Paletta, L., Rome, E. (eds) Attention in Cognitive Systems. Theories and Systems from an Interdisciplinary Viewpoint. WAPCV 2007. Lecture Notes in Computer Science(), vol 4840. Springer, Berlin, Heidelberg doi:10.1007/978-3-540-77343-6_23\n        \n        URL\n     \n        \n        PDF\n    \n2006\nJulien Vitay (2006).Emergence of sensorimotor functions on a numerical distributed neural substrate. PhD thesis (Université Henri-Poincaré Nancy-I)\n        \n        PDF\n    \nJérémy Fix, Julien Vitay, and Nicolas P. Rougier (2006).A Distributed Computational Model of Spatial Memory Anticipation During a Visual Search Task. In: Butz, M.V., Sigaud, O., Pezzulo, G., Baldassarre, G. (eds) Anticipatory Behavior in Adaptive Learning Systems. ABiALS 2006. Lecture Notes in Computer Science(), vol 4520. Springer, Berlin, Heidelberg doi:10.1007/978-3-540-74262-3_10\n        \n        URL\n     \n        \n        PDF\n    \n2005\nNicolas P. Rougier and Julien Vitay (2005).Emergence of attention within a neural population. Neural Networks 19(5) doi:10.1016/j.neunet.2005.04.004\n        \n        URL\n     \n        \n        PDF\n    \nJulien Vitay, Nicolas P. Rougier, and Frédéric Alexandre (2005).A distributed model of spatial visual attention. In: Wermter, S., Palm, G., Elshaw, M. (eds) Biomimetic Neural Learning for Intelligent Robots. Lecture Notes in Computer Science(), vol 3575. Springer, Berlin, Heidelberg doi:10.1007/11521082_4\n        \n        URL\n     \n        \n        PDF"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Dr. habil. Julien Vitay",
    "section": "",
    "text": "Researcher / lecturer at the Chemnitz University of Technology, Department of Computer Science, Professorship for Artificial Intelligence."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Dr. habil. Julien Vitay",
    "section": "Education",
    "text": "Education\n\n\nChemnitz University of Technology\nHabilitation in Computer Science\n\n\nChemnitz, Germany - 2017\n\n\n\n\nUniversité Henri Poincaré Nancy-I\nPhD in Computer Science\n\n\nNancy, France - 2006\n\n\n\n\nÉcole Supérieure d’Électricité (Supélec)\nEngineering degree in Signal Processing and Microelectronics\n\n\nRennes, France - 2002\n\n\n\n\nUniversité Rennes-I\nMaster of Science in Microelectronics\n\n\nRennes, France - 2002"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "Dr. habil. Julien Vitay",
    "section": "Experience",
    "text": "Experience\n\n\nChemnitz University of Technology\n\n\nMarch 2011 - present\n\n\nResearcher / lecturer\n\n\nUniversity of Münster\n\n\nAugust 2006 - February 2011\n\n\nPostdoc\n\nSupervisor: Fred Hamker\n\n\n\nInria Lorraine\n\n\nOctober 2002 - June 2006\n\n\nPhD student\n\nSupervisors: Frédéric Alexandre and Nicolas Rougier"
  }
]