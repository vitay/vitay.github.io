[{"authors":null,"categories":null,"content":"I am researcher and lecturer at the TU Chemnitz, in the lab of Artificial Intelligence of the Department of Computer Science.\nI was previously postdoc in the Psychology Department of the University of Münster (Germany), under the supervision of Prof. Dr. Fred Hamker and a PhD student at Inria Nancy (France), in the Cortex lab headed by Dr. Frédéric Alexandre.\nMy research interests focus on computational neuroscience (basal ganglia, hippocampus, dopaminergic system) and neuro-informatics (neuro-simulator ANNarchy). I am also interested in machine learning, especially the recent advances in deep reinforcement learning.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://julien-vitay.net/author/julien-vitay/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/julien-vitay/","section":"authors","summary":"I am researcher and lecturer at the TU Chemnitz, in the lab of Artificial Intelligence of the Department of Computer Science.\nI was previously postdoc in the Psychology Department of the University of Münster (Germany), under the supervision of Prof.","tags":null,"title":"Julien Vitay","type":"authors"},{"authors":["Helge Ü Dinkelbach","Julien Vitay","Fred H. Hamker"],"categories":null,"content":"","date":1568412000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568412000,"objectID":"be1353a9b8478750d6010345ce92fcd0","permalink":"https://julien-vitay.net/publication/dinkelbach2019/","publishdate":"2019-09-14T00:00:00+02:00","relpermalink":"/publication/dinkelbach2019/","section":"publication","summary":"The size and complexity of the neural networks investigated in computational neuroscience are increasing, leading to a need for efficient neural simulation tools to support their development. Several neuro-simulators have been developed over the years by the community, all with different scopes (rate-coded, spiking, mean-field), target platforms (CPU, GPU, clusters) or modeling principles (fixed model library, code generation). We compare here the current version of the neuro-simulator ANNarchy against other state-of-the-art simulators on ratecoded and spiking benchmarks with a focus on their parallel performance.","tags":[],"title":"Scalable simulation of rate-coded and spiking neural networks on shared memory systems","type":"publication"},{"authors":["Katharina Schmid","Julien Vitay","Fred H. Hamker"],"categories":null,"content":"","date":1568325600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568325600,"objectID":"a7691efa0d727d6971cb76396a361606","permalink":"https://julien-vitay.net/publication/schmid2019/","publishdate":"2019-09-13T00:00:00+02:00","relpermalink":"/publication/schmid2019/","section":"publication","summary":"The cerebellum is thought to be able to learn forward models, which allow to predict the sensory consequences of planned movements and adapt behavior accordingly. Although classically considered as a feedforward structure learning in a supervised manner, recent proposals highlighted the importance of the internal recurrent connectivity of the cerebellum to produce rich dynamics (Rössert et al., 2015), as well as the importance of reinforcement-like mechanisms for its plasticity (Bouvier et al. 2019). Based on these models, we propose a neuro-computational model of the cerebellum using an inhibitory reservoir architecture and biologically plausible learning mechanisms based on perturbation learning. The model is trained to predict the position of a simple robotic arm after ballistic movements. Understanding how the cerebellum is able to learn forward models might allow elucidating the biological basis of model-based reinforcement learning.","tags":[],"title":"Forward Models in the Cerebellum using Reservoirs and Perturbation Learning","type":"publication"},{"authors":["Julien Vitay"],"categories":[],"content":"Successor representations (SR) attract a lot of attention these days, both in the neuroscientific and machine learning / deep RL communities. This post is intended to explain the main difference between SR and model-free / model-based RL algorithms and to point out its usefulness to understand goal-directed behavior.\nTable of Contents  1 - Motivation  1.1 - Model-free vs. model-based RL 1.2 - Goal-directed behavior vs. habits   2 - Successor representations in reinforcement learning  2.1 - Main idea 2.2 - Linear function approximation 2.3 - Successor representations of actions   3 - Successor representations in neuroscience  3.1 - Human goal-directed behavior 3.2 - Neural substrates of successor representations   Discussion References   1 - Motivation 1.1 - Model-free vs. model-based RL There are two main families of reinforcement learning (RL; Sutton and Barto, 2017) algorithms:\n Model-free (MF) methods estimate the value of a state $V^\\pi(s)$ or of a state-action pair $Q^\\pi(s, a)$ by sampling trajectories and averaging the obtained returns (Monte-Carlo control), or by estimating the Bellman equations (Temporal difference - TD):   $$V^\\pi(s) = \\mathbb{E}_{\\pi} [\\sum_{k=0} \\gamma^k \\, r_{t+k+1} | s_t = s] = \\mathbb{E}_{\\pi} [r_{t+1} + \\gamma \\, V^\\pi(s_{t+1}) | s_t = s]$$   $$Q^\\pi(s, a) = \\mathbb{E}_{\\pi} [\\sum_{k=0} \\gamma^k \\, r_{t+k+1} | s_t = s, a_t = a] = \\mathbb{E}_{\\pi} [r_{t+1} + \\gamma \\, Q^\\pi(s_{t+1}, a_{t+1}) | s_t = s, a_t = a]$$   Model-based (MB) methods use (or learn) a model of the environment - transition probabilities $p(s_{t+1} | s_t, a_t)$ and reward probabilities $r(s_t, a_t, s_{t+1})$ - and use it to plan trajectories maximizing the theoretical return, either through some form of forward planning (search tree) or using dynamic programming (solving the Bellman equations directly).   $$\\pi^* = \\text{argmax}_{\\pi} \\; p(s_0) \\, \\sum_{t=0}^\\infty \\gamma^t \\, p(s_{t+1} | s_t, a_t) \\, \\pi(s_t, a_t) \\, r(s_t, a_t, s_{t+1})$$   $$V^*(s) = \\max_a \\sum_{s'} p(s' | s, a) \\, (r(s_t, a, s')+ \\gamma \\, V^*(s'))$$  The main advantage of model-free methods is their speed: they cache the future of the system into value functions. When having to take a decision at time $t$, we only need to look at the action with the highest Q-value in the state $s_t$ and take it. If the Q-values are optimal, this is the optimal policy. Oppositely, model-based algorithms have to plan sequentially in the state-action space, which can be very long if the problem has a long temporal horizon.\nThe main drawback of MF methods is their inflexibility when the reward distribution changes. When the reward associated with a transition changes (the source of reward has vanished, its nature has changed, the rules of the game have changed, etc), each action leading to that transition has to be experienced multiple times before the corresponding values reflect that change. This is due to the use of the temporal difference (TD) algorithm, where the reward prediction error (RPE) is used to update values:\n$$\\delta_t = r_{t+1} + \\gamma \\, V^\\pi(s_{t+1}) - V^\\pi(s_t)$$ $$\\Delta V^\\pi(s_t) = \\alpha \\, \\delta_t$$ When the reward associated to a transition changes drastically, only the last state (or action) is updated after that experience (unless we use eligibility traces). Only multiple repetitions of the same trajectory would allow changing the initial decisions. This is opposite to MB methods, where a change in the reward distribution would very quickly influence the planning of the optimal trajectory. In MB, the reward probabilities can be estimated with:\n$$ \\Delta r(s_t, a_t, s_{t+1}) = \\alpha \\, (r_{t+1} - r(s_t, a_t, s_{t+1})) $$  with $r_{t+1}$ being the reward obtained during one sampled transition. The transition probabilities can also be learned from experience using:\n$$ \\Delta p(s' | s_t, a_t) = \\alpha \\, (\\mathbb{I}(s_{t+1} = s') - p(s' | s_t, a_t)) $$  where $\\mathbb{I}(b)$ is 1 when $b$ is true, 0 otherwise. Depending on the learning rate, changes in the environment dynamics can be very quickly learned by MB methods, as updates do not depend on other estimates (there is no bootstrapping contrary to TD).\n1.2 - Goal-directed behavior vs. habits The model-free RPE has become a very influential model of dopaminergic (DA) activation in the ventral tegmental area (VTA) and substantia nigra pars compacta (SNc). At the beginning of classical Pavlovian conditioning, DA cells react phasically to unconditioned stimuli (US, rewards). After enough conditioning trials, DA cells only react to conditioned stimuli (CS), i.e. stimuli which predict the delivery of a reward. Moreover, if the reward is omitted, DA cells exhibit a pause in firing. This pattern of activation corresponds to the RPE: DA cells respond to unexpected reward events, either positively when more reward than expected is received, or negatively when less reward is delivered. The simplicity of this model has made RPE a successful model of DA activity (but see Vitay and Hamker, 2014 for a more detailed model).\nA similar but not identical functional dichotomy as MF/MB opposes deliberative goal-directed behavior and inflexible stimulus-response associations called habits (Dickinson and Balleine, 2002). Goal-directed behavior is sensitive to reward devaluation: if an outcome was previously rewarding but ceases to be (for example, a poisonous product is injected into some food reward, even outside the conditioning phase), goal-directed behavior would quickly learn to avoid that outcome, while habitual behavior will continue to seek for it. Over-training can transform goal-directed behavior into habits (Corbit and Balleine, 2011). Habits are usually considered as a model-free learning behavior, while goal-directed behavior implies the use of a world model. The dual system theory discusses the arbitration mechanisms necessary to coordinate these two learning frameworks (Lee, Shimojo and O\u0026rsquo;Doherty, 2014).\nBoth forms of behavior are thought to happen concurrently in the brain, with model-based / goal-directed behavior classically assigned to the prefrontal cortex and the hippocampus and model-free / habitual behavior mapped to the ventral basal ganglia and the dopaminergic system. However, recent results and theories suggest that these two functional systems are largely overlapping and that even dopamine firing might reflect model-based processes (Doll, Simon and Daw, 2012; Miller et al., 2018). It is yet to be understood how these two extreme mechanisms of the RL spectrum might coexist in the brain and be coordinated: successor representations might provide us with additional useful insights into the functioning of the brain.\n2 - Successor representations in reinforcement learning 2.1 - Main idea The original formulation of successor representations (SR) is actually not recent (Dayan, 1993), but it is subject to a revival since a couple of years with the work of Samuel J. Gershman (e.g. Gershman et al., 2012, 2018, Momennejad et al., 2017, Stachenfeld et al., 2017, Gardner et al, 2018).\nThe SR algorithm learns two quantities:\n The expected immediate reward received after each state:  $$ r(s) = \\mathbb{E}_{\\pi} [r_{t+1} | s_t = s] $$  The expected discounted future state occupancy (the SR itself):  $$ M(s, s') = \\mathbb{E}_{\\pi} [\\sum_{k=0}^\\infty \\gamma^k \\, \\mathbb{I}(s_{t+k+1} = s') | s_t = s] $$  I omit here the dependency of $r$ and $M$ on the policy itself in the notation, but it is of course implicitly there.\nThe SR represents the fact that a state $s'$ can be reached after $s$, with a value decreasing with the temporal gap between the two states: states occurring in rapid succession will have a high SR, very distant states will have a low SR. If $s'$ happens consistently before $s$, the SR should be 0 (causality principle). This is in principle similar to model-based RL, but without an explicit representation of the transition structure: it only represents how states are temporally correlated, not which action leads to which state.\nThe value of a state $s$ is then defined by:\n$$ V^\\pi(s) = \\sum_{s'} M(s, s') \\, r(s') $$  The value of a state $s$ depends on which states $s'$ can be visited after it (following the current policy, implicitly), how far in the future they will happen (discount factor in $M(s, s')$) and how much reward can be obtained immediately in those states ($r(s')$). Note that it is merely a rewriting of the definition of the value of a state, with rewards explicitly separated from state visitation and time replaced by succession probabilities:\n$$V^\\pi(s_t) = \\mathbb{E}_{\\pi} [\\sum_{k=0} \\gamma^k \\, r_{t+k+1}] = \\mathbb{E}_{\\pi} [\\sum_{k=0} r(s_{t+k+1}) \\times (\\gamma^k \\, \\mathbb{I}(s_{t+k+1}))]$$ The SR also obeys a recursive relationship similar to the Bellman equation, as it is based on a discounted sum:\n$$ M(s_t, s') = \\mathbb{I}(s_t = s') + \\gamma \\, M(s_{t+1}, s') $$  The discounted probability of arriving in $s'$ after being in $s_t$ is one if we are already in $s'$, and gamma times the discounted probability of arriving in $s'$ after being in the next state $s_{t+1}$ otherwise.\nThis recursive relationship implies that we are going to be able to estimate the SR $M(s, s')$ using a sensory prediction error (SPE) similar to the TD RPE (Gershman et al., 2012):\n$$ \\delta^\\text{SR}_t = \\mathbb{I}(s_t = s') + \\gamma \\, M(s_{t+1}, s') - M(s_t, s') $$   $$ \\Delta M(s_t, s') = \\alpha \\, \\delta^\\text{SR}_t $$  The SPE states that the expected occupancy for states that are visited more frequently than expected (positive sensory prediction error) should be increased, while the expected occupancy for states that are visited less frequently than expected (negative sensory prediction error) should be decreased. In short: is arriving in this new state surprising? It should be noted that the SPE is defined over all possible successor states $s'$, so the SPE is actually a vector.\nWe can already observe that SR is a trade-off between MF and MB methods. A change in the reward distribution can be quickly tracked by SR algorithms, as the immediate reward $r(s)$ can be updated with:\n$$ \\Delta r(s) = \\alpha \\, (r_{t+1} - r(s)) $$ However, the SR $M(s, s')$ uses other estimates for its update (bootstrapping), so changes in the transition structure may take more time to propagate to all state-state discounted occupancies (Gershman, 2018).\n2.2 - Linear function approximation Before looking at the biological plausibility of this algorithm, we need to deal with the curse of dimensionality. The SR $M(s, s')$ is a matrix associating each state of the system to all other states (size $|\\mathcal{S}| \\times |\\mathcal{S}|$). This is of course impracticable for most problems and we need to rely on function approximation. The simplest solution is to represent each state $s$ by a set of $d$ features $[f_i(s)]_{i=1}^d$. Each feature can for example be the presence of an object in the scene, some encoding of the position of the agent in the world, etc. The SR for a state $s$ only needs to predict the expected discounted probability that a feature $f_j$ will be observed in the future, not the complete state representation. This should ensure generalization across states, as only the presence of relevant features is needed. The SR can be linearly approximated by:\n$$ M_j(s) = \\sum_{i=1}^d w_{i, j} \\, f_i(s) $$ The expected discounted probability of observing the feature $f_j$ in the future is defined as a weighted sum of the features of the state $s$. The value of a state is now defined as:\n$$ V^\\pi(s) = \\sum_{j=1}^d M_j(s) \\, r(f_j) = \\sum_{j=1}^d r(f_j) \\, \\sum_{i=1}^d w_{i, j} \\, f_i(s) $$ where $r(f_j)$ is the expected immediate reward when observing the feature $f_j$, what can be easily tracked as before. Computing the value of a state based on the SR now involves a double sum over a $d \\times d$ matrix, $d$ being the number of features, what should generally be much more tractable than over the total number of states squared.\nAs we use linear approximation, the learning rule for the weights $w_{i, j}$ becomes linearly dependent on the SPE:\n$$ \\delta^\\text{SR}_t(f_j) = f_j(s_t) + \\gamma \\, M_j(s_{t+1}) - M_j(s) $$ $$ \\Delta w_{i, j} = \\alpha \\, \\delta^\\text{SR}_t(f_j) \\, f_i(s_t) $$ The SPE tells us how surprising is each feature $f_j$ when being in the state $s_t$. This explains the term sensory prediction error: we are now not learning based on how surprising rewards are anymore, but on how surprising the sensory features of the outcome are. Did I expect that door to open at some point? Should this event happen soon? What kind of outcome is likely to happen? As the SPE is now a vector for all sensory features, we see why successor representation have a great potential: instead of a single scalar RPE dealing only with reward magnitudes, we now can learn from very diverse representations describing the various relevant dimensions of the task. It can then deal with different rewards: food and monetary rewards are treated the same by RPEs, while we can distinguish them with SPEs.\nThe main potential problem is of course to extract the relevant features for the task, either by hand-engineering them or through learning (one could work in the latent space of a variational autoencoder, for example). Feature-based state representations still have to be Markovian for SR to work. It is also possible to use non-linear function approximators such as deep networks (Kulkarni et al., 2016, Baretto et al., 2016, Zhang et al., 2016, Machado et al., 2018, Ma et al., 2018), but this is out of the scope of this post.\n2.3 - Successor representations of actions The previous sections focused on the successor representation of states to obtain the value function $V^\\pi(s)$. The same idea can be applied to state-action pairs and their $Q^\\pi(s, a)$ values. The Q-value of a state action pair can be defined as:\n$$ Q^\\pi(s, a) = \\sum_{s', a'} M(s, a, s', a') \\, r(s', a') $$ where $r(s', a')$ is the expected immediate reward obtained after $(s', a')$ and $M(s, a, s', a')$ is the SR between the pairs $(s, a)$ and $(s', a')$ as in (Momennejad et al., 2017). Ducarouge and Sigaud (2017) use a SR representation between a state-action pair $(s, a)$ and a successor state $s'$:\n$$ Q^\\pi(s, a) = \\sum_{s'} M(s, a, s') \\, r(s') $$ In both cases, the SR can be learned using a sensory prediction error, such as:\n$$ \\delta^\\text{SR}_{s_t, a_t} = \\mathbb{I}(s_t = s') + \\gamma \\, M(s_{t+1}, a_{t+1}, s') - M(s_t, a_t, s') $$  Note that eligibility traces can be used in SR learning as easily as in TD methods.\n3 - Successor representations in neuroscience 3.1 - Human goal-directed behavior So great, we now have a third form of reinforcement learning. Could it be the missing theory to explain human reinforcement learning and the dichotomy goal-directed behavior / habits?\nMomennejad et al. (2017) designed a two-steps sequential learning task with reward and transition revaluations. In the first learning phase, the subjects are presented with sequences of images (the states) and obtain different rewards (Fig. 1). The sequence $1 \\rightarrow 3 \\rightarrow 5$ is rewarded with 10 dollars while the sequence $2 \\rightarrow 4 \\rightarrow 6$ is rewarded with 1 dollar only. Successful learning is tested by asking the participant whether he/she prefers the states 1 or 2 (the answer is obviously 1).\n Two-steps sequential learning task of Momennejad et al. (2017).   In the reward revaluation task, the transitions $3 \\rightarrow 5$ and $4 \\rightarrow 6$ are experienced again in the re-learning phase, but this time with reversed rewards (1 and 10 dollars respectively). In the transition revaluation task, the transitions $3 \\rightarrow 6$ and $4 \\rightarrow 5$ are now experienced, but the states $5$ and $6$ still receive the same amount of reward. The preference for $1$ or $2$ is again tested at the end of the re-learning phase ($2$ should now be preferred in both tasks) and a revaluation score is computed (how much the subject changes his preference between the two phases).\nWhat would the different ML methods predict?\n  Model-free methods would not change their preference in both conditions. The value of the $3 \\rightarrow 5$ and $4 \\rightarrow 6$ transitions (reward revaluation) or $3 \\rightarrow 6$ and $4 \\rightarrow 5$ (transition revaluation) would change during the re-learning phase, but the transitions $1 \\rightarrow 3$ and $2 \\rightarrow 4$ are never experienced again, so the value of the states $1$ and $2$ can only stay the same, even with eligibility traces.\n  Model-based methods would change their preference in both conditions. The reward and transition probabilities would both be re-learned completely to reflect the change, so the new value of $1$ and $2$ can be computed correctly using dynamic programming.\n  Successor representation methods would adapt to the reward revaluation ($r(s)$ will quickly fit the new reward distribution for the states $5$ and $6$), but not to the transition revaluation: $6$ is never a successor state of $1$ in the re-learning phase, so the SR matrix will not be updated for the states $1$ and $2$.\n  We have three different mechanisms with testable predictions on these two tasks: the human experiments should tell us which method is the best model of human RL. Well\u0026hellip; Not really.\n Revaluation score in the reward (red) and transition (blue) revaluation conditions for the model-free (MF), model-based (MB), successor representation (SR) and human data as reported in Momennejad et al. (2017).   Human participants show a revaluation behavior in the two conditions (reward and transition) somehow in between the model-based and successor representation algorithms. The difference between the reward and transition conditions is statistically significant, so unlike MB, but not as dramatic as for SR. The authors propose a hybrid SR-MB model, linearly combining the outputs of the MB and SR algorithms, and fit it to the human data to obtain a satisfying match. A second task requiring the model to actually take actions confirms this observation.\nIt is hard to conclude anything definitive from this model and the somehow artificial fit to the data. Reward revaluation was the typical test to distinguish between MB and MF processes, or between goal-directed behavior and habits. This paper suggests that transition revaluation (and policy revaluation, investigated in the second experiment) might allow distinguishing between MB and SR mechanisms, supporting the existence of SR mechanisms in the brain. How MB and SR might interact in the brain and whether there is an arbitration mechanism between the two is still an open issue. (Russek et al., 2017) has a very interesting discussion on the link between MF and MB processes in the brain, based on different versions of the SR.\n3.2 - Neural substrates of successor representations In addition to describing human behavior at the functional level, the SR might also allow to better understand the computations made by the areas involved in goal-directed behavior, in particular the prefrontal cortex, the basal ganglia, the dopaminergic system, and the hippocampus. The key idea of Gershman and colleagues is that the SR $M(s, s')$ might be encoded in the place cells of the hippocampus (Stachenfeld et al., 2017), which are known to be critical for reward-based navigation. The sensory prediction error (SPE $\\delta^\\text{SR}_t$) might be encoded in the activation of the dopaminergic cells in VTA (or in a fronto-striatal network), driving learning of the SR in the hippocampus (Gardner et al., 2018), while the value of a state $V^\\pi(s) = \\sum_{s'} M(s, s') , r(s')$ could be computed either in the prefrontal cortex (ventromedial or orbitofrontal) or in the ventral striatum (nucleus accumbens in rats), ultimately allowing action selection in the dorsal BG.\nDopamine as a SPE The most striking prediction of the SR hypothesis is that the SPE is a vector of prediction errors, with one element per state (in the original formulation) or per reward feature (using linear function approximation, section 2.2). This contrasts with the classical RPE formulation, where dopaminergic activation is a single scalar signal driving reinforcement learning in the BG and prefrontal cortex. Although this would certainly be an advantage in terms of functionality and flexible learning, it remains to be shown whether VTA actually encodes such a feature-specific signal.\nNeurons in VTA have a rather uniform response to rewards or reward-predicting cues, encoding mostly the value of the outcome regardless its sensory features, except for those projecting to the tail of the striatum which mostly respond to threats and punishments (Watabe-Uchida and Uchida, 2019). The current state of knowledge seems to rule out VTA as a direct source of SPE signals.\nInterestingly, Oemisch et al. (2019) showed that feature-specific prediction errors signals (analogous to the SPE with linear approximation) are detected in the fronto-striatal network including the anterior cingulate area (ACC), dorsolateral prefrontal cortex (dlPFC), dorsal striatum and ventral striatum (VS) / nucleus accumbens (NAcc). These SPE-like signals appear shortly after non-specific RPE signals, first in ACC and then in the rest of the network. This suggests that SPE would actually be the result of a more complex calculation than proposed in the SR hypothesis, involving a network of interconnected areas. A detailed neuro-computational model of this network still has to be proposed.\nHippocampus as a predictive map Another interesting prediction of the SR hypothesis is that the hippocampus might be the site where the SR matrix is represented (Stachenfeld et al., 2017). In navigation tasks, the so-called place cells in the hippocampus exhibit roughly circular receptive fields centered on different locations in the environment (O\u0026rsquo;Keefe and Nadel, 1978). Altogether, place cells are thought to provide a sparse code of the animal\u0026rsquo;s location. Strikingly, place fields change with the environment: moving the animal from a circular to a rectangular environment, or introducing barriers, modifies the distribution of place fields. Additionally, grid cells in the entorhinal cortex (reciprocally connected to the hippocampus) show a hexagonal grid pattern of receptive fields, i.e. a single grid cell responds for several positions of the animal inside the environment (Hafting et al., 2005). Grid cells' receptive fields also depend on the environment and have been shown to depend on place cells, not the other way around. The mechanism behind the flexibility of place and grid fields is still to be understood.\nStachenfeld et al. (2017) propose that place cells actually encode the SR $M(s, s')$ between the current location $s$ and their preferred location $s'$, rather than simply an Euclidian distance between $s$ and $s'$ as classically used in hippocampal models. Because of the discount rate in the SR and its dependency on the animal\u0026rsquo;s policy, place fields are then roughly circular (exponentially decreasing) in an open environment, where the animal can theoretically reach any neighboring location from its current position. When constraints are added to the environment, such as walls and barriers, certain transitions are not possible anymore, which will modify the shape of the place fields. This fits with experimental observations, contrary to most models of place field formation using Gaussian receptive fields around fixed locations. Additionally, the SR hypothesis is in agreement with the observation that rewarded locations are represented by a higher number of place cells, as the animal spends more time around them.\n Place field on a linear track with an obstacle at x=1, using a Euclidian model (left) and the SR hypothesis (right). Adapted from Fig. 2 of (Stachenfeld et al., 2017).   Fig. 3 illustrates this prediction: if a rat is placed on a linear track with an obstacle, the place cell whose RF is centered on the obstacle would react identically on both sides of the obstacle using a Euclidian model, while it would only respond on the side it has explored using the SR. When the rat is put on the other side, the SRs would initially be 0 for that cell (but would grow with more exploration).\nStachenfeld et al. (2017) also propose a mechanism for grid cell formation in the entorhinal cortex. Grid cells are understood as a low-dimensional eigendecomposition of the SR place cells (dimensionality reduction, as in principal component analysis). This allows to explain why grid cells change in different environments (circular, rectangular or triangular), as experimentally observed. They also propose a mechanism for sub-goal formation using grid cells, but using the normalized min-cut algorithm, so quite far from being biologically realistic.\nDiscussion Successor representations are an interesting trade-off between model-free and model-based RL algorithms, explicitly separating state transitions from reward estimation. It allows reacting quickly to distal reward changes without the computational burden of completely model-based planning. Deep RL variants of SR (Kulkarni et al., 2016) obtain satisfying results on classical RL tasks such as Atari games and simulated robots, but are still outperformed by modern model-free algorithms. Similar to human behavior, hybrid architectures using both SR and MF methods might be able to combine the optimality of MF methods with the flexibility of the SR.\nAt the neuroscientific level, the SR hypothesis raises a lot of interesting questions, especially regarding the interplay between the prefrontal cortex, the hippocampus, and the basal ganglia during goal-directed behavior. Here are just a few aspects that need to be investigated both experimentally and theoretically:\n What is the relationship between the RPE and the SPE? Does VTA compute the SPE (still to be proven) and send it directly to the hippocampus through dopaminergic projections? Or does the RPE VTA somehow \u0026ldquo;train\u0026rdquo; ACC and PFC to compute the SPE, what is then sent to the hippocampus to update the SR representation? How? How does the hippocampus learn from SPE signals? The SR hypothesis still has to be linked with evidence on plasticity in the hippocampus. If dopamine does not carry the SPE, what is the role of the dopaminergic innervation of the hippocampus? The SR representation is in principle independent from rewards (except that animals may spend more time around reward location). How is the value of a state / action computed based on the SR representation in the hippocampus? Do sharp wave ripples (SWR, also called forward/inverse replays) actually sample the SR matrix (a list of achievable states from the current one), what is then integrated elsewhere (ventral striatum?) to guide behavior?  References Barreto, A., Dabney, W., Munos, R., Hunt, J. J., Schaul, T., van Hasselt, H., et al. (2016). Successor Features for Transfer in Reinforcement Learning. arXiv:1606.05312. Available at: http://arxiv.org/abs/1606.05312.\nCorbit, L. H., and Balleine, B. W. (2011). The general and outcome-specific forms of Pavlovian-instrumental transfer are differentially mediated by the nucleus accumbens core and shell. The Journal of neuroscience 31, 11786–94. doi:10.1523/JNEUROSCI.2711-11.2011.\nDayan, P. (1993). Improving Generalization for Temporal Difference Learning: The Successor Representation. Neural Computation 5, 613–624. doi:10.1162/neco.1993.5.4.613.\nDickinson, A., and Balleine, B. (2002). The role of learning in the operation of motivational systems. In: Gallistel CR, editor. Steven’s handbook of experimental psychology: learning, motivation and emotion . 3rd ed.New York: John Wiley \u0026amp; Sons, 497–534.\nDoll, B. B., Simon, D. A., and Daw, N. D. (2012). The ubiquity of model-based reinforcement learning. Current Opinion in Neurobiology 22, 1075–1081. doi:10.1016/j.conb.2012.08.003.\nDucarouge, A., and Sigaud, O. (2017). The Successor Representation as a model of behavioural flexibility. In Journées Francophones sur la Planification, la Décision et l’Apprentissage pour la conduite de systèmes (JFPDA 2017). Caen, France. Available at: https://hal.archives-ouvertes.fr/hal-01576352.\nGardner, M. P. H., Schoenbaum, G., and Gershman, S. J. (2018). Rethinking dopamine as generalized prediction error. Proceedings of the Royal Society B: Biological Sciences 285, 20181645. doi:10.1098/rspb.2018.1645.\nGershman, S.J., Moore, C.D:, Todd, M.T., Norman, K.A., and Sederberg, P.B. (2012). The successor representation and temporal context. Neural Computation, 24(6):1553–1568, 2012.\nGershman, S. J. (2018). The Successor Representation: Its Computational Logic and Neural Substrates. The Journal of neuroscience : the official journal of the Society for Neuroscience 38, 7193–7200. doi:10.1523/JNEUROSCI.0151-18.2018.\nHafting, T., Fyhn, M., Molden, S., Moser, M.-B., and Moser, E. I. (2005). Microstructure of a spatial map in the entorhinal cortex. Nature 436, 801. doi:10.1038/nature03721.\nKulkarni, T. D., Saeedi, A., Gautam, S., and Gershman, S. J. (2016). Deep Successor Reinforcement Learning. arXiv:1606.02396. Available at: http://arxiv.org/abs/1606.02396.\nLee, S. W., Shimojo, S., and O\u0026rsquo;Doherty J. P. (2014). Neural computations underlying arbitration between model-based and model-free learning. Neuron, 81(3), 687–699. doi:10.1016/j.neuron.2013.11.028.\nMa, C., Wen, J., and Bengio, Y. (2018). Universal Successor Representations for Transfer Reinforcement Learning. arXiv:1804.03758. Available at: http://arxiv.org/abs/1804.03758.\nMachado, M. C., Bellemare, M. G., and Bowling, M. (2018). Count-Based Exploration with the Successor Representation. arXiv:1807.11622. Available at: http://arxiv.org/abs/1807.11622.\nMiller, K., Ludvig, E. A., Pezzulo, G., and Shenhav, A. (2018). Re-aligning models of habitual and goal-directed decision-making. In Goal-Directed Decision Making : Computations and Neural Circuits, eds. A. Bornstein, R. W. Morris, and A. Shenhav (Academic Press). Available at: https://www.elsevier.com/books/goal-directed-decision-making/morris/978-0-12-812098-9.\nMomennejad, I., Russek, E. M., Cheong, J. H., Botvinick, M. M., Daw, N. D., and Gershman, S. J. (2017). The successor representation in human reinforcement learning. Nature Human Behaviour 1, 680–692. doi:10.1038/s41562-017-0180-8.\nOemisch, M., Westendorff, S., Azimi, M., Hassani, S. A., Ardid, S., Tiesinga, P., et al. (2019). Feature-specific prediction errors and surprise across macaque fronto-striatal circuits. Nature Communications 10, 176. doi:10.1038/s41467-018-08184-9.\nO\u0026rsquo;Keefe, J., and Nadel, L. (1978). The hippocampus as a cognitive map. Oxford : New York: Clarendon Press ; Oxford University Press.\nRussek, E. M., Momennejad, I., Botvinick, M. M., Gershman, S. J., and Daw, N. D. (2017). Predictive representations can link model-based reinforcement learning to model-free mechanisms. PLoS Computational Biology, 13, e1005768. doi:10.1371/journal.pcbi.1005768\nSchultz, W. (1998). Predictive reward signal of dopamine neurons. J Neurophysiol 80, 1–27.\nStachenfeld, K. L., Botvinick, M. M., and Gershman, S. J. (2017). The hippocampus as a predictive map. Nature Neuroscience 20, 1643–1653. doi:10.1038/nn.4650.\nSutton, R. S., and Barto, A. G. (2017). Reinforcement Learning: An Introduction. 2nd ed. Cambridge, MA: MIT Press. Available at: http://incompleteideas.net/book/the-book-2nd.html.\nVitay, J., and Hamker, F. H. (2014). Timing and expectation of reward: A neuro-computational model of the afferents to the ventral tegmental area. Frontiers in Neurorobotics 8. doi:10.3389/fnbot.2014.00004.\nWatabe-Uchida, M., and Uchida, N. (2019). Multiple Dopamine Systems: Weal and Woe of Dopamine. Cold Spring Harb Symp Quant Biol, 037648. doi:10.1101/sqb.2018.83.037648.\nZhang, J., Springenberg, J. T., Boedecker, J., and Burgard, W. (2016). Deep Reinforcement Learning with Successor Features for Navigation across Similar Environments. arXiv:1612.05533. Available at: http://arxiv.org/abs/1612.05533.\n","date":1557297166,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557297166,"objectID":"6dc6f07e4b5bc1944af67550a865c68b","permalink":"https://julien-vitay.net/post/successor_representations/","publishdate":"2019-05-08T07:32:46+01:00","relpermalink":"/post/successor_representations/","section":"post","summary":"Successor representations (SR) attract a lot of attention these days, both in the neuroscientific and machine learning / deep RL communities. This post is intended to explain the main difference between SR and model-free / model-based RL algorithms and to point out its usefulness to understand goal-directed behavior.","tags":["Reinforcement Learning","Machine Learning","Dopamine"],"title":"Successor Representations","type":"post"},{"authors":["Enrico Schröder","Sascha Braun","Mirko Mählisch","Julien Vitay","Fred H. Hamker"],"categories":null,"content":"","date":1556056800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1556056800,"objectID":"08a32bdff0a17772cd3da063df2d36cb","permalink":"https://julien-vitay.net/publication/schroeder2019/","publishdate":"2019-04-24T00:00:00+02:00","relpermalink":"/publication/schroeder2019/","section":"publication","summary":"We present a general framework for fusing pre-trained multisensor object detection networks for perception in autonomous cars at an intermediate stage using perspective invariant features. Key innovation is an autoencoder-inspired Transformer module which transforms perspective as well as feature activation layout from one sensor modality to another. Transformed feature maps can be combined with those of a modality-native object detector to enhance performance and reliability through a simple fusion scheme. Our approach is not limited to a specific object detection network architecture or even to specific sensor modalities. We show effectiveness of the proposed scheme through experiments on our own as well as on the KITTI dataset.","tags":[],"title":"Feature Map Transformation for Fusion of Multi-Sensor Object Detection Networks for Autonomous Driving","type":"publication"},{"authors":["Julien Vitay"],"categories":null,"content":"","date":1552734000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1552734000,"objectID":"27267fa0a92187b285124d53514439c0","permalink":"https://julien-vitay.net/talk/clt2019/","publishdate":"2019-03-16T12:00:00+01:00","relpermalink":"/talk/clt2019/","section":"talk","summary":"Artificial Intelligence (AI) has become extremely popular in the last years through the advancement of Deep Learning, a modernized version of the good old neural networks. Deep Learning has enabled huge progress in pattern recognition, for example in object recognition or localization, speech recognition and synthesis, natural language understanding or in robotic control. These advances and the many novel applications that they allow seem to promise a bright future to AI, some self-proclaimed prophets even affirming that AI could soon be comparable to or even exceed human intelligence (the singularity). This presentation will look at the current state-of-the-art in deep learning and its applications, observe some of its limitations and discuss whether this is a suitable approach to Artificial General Intelligence.","tags":[],"title":"Deep Learning and Intelligence: the right approach?","type":"talk"},{"authors":["Valentin Forch","Julien Vitay","Fred H. Hamker"],"categories":null,"content":"","date":1548975600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548975600,"objectID":"de883880f2461fa1848db6b5c60dee4d","permalink":"https://julien-vitay.net/publication/forch2019/","publishdate":"2019-02-01T00:00:00+01:00","relpermalink":"/publication/forch2019/","section":"publication","summary":"Automatic processing of emotion information through deep neural networks (DNN) can have great benefits for human-machine interaction. Vice versa, machine learning can profit from concepts known from human information processing (e.g., visual attention). We employed a recurrent DNN incorporating a spatial attention mechanism for facial emotion recognition (FER) and compared the output of the network with results from human experiments. The attention mechanism enabled the network to select relevant face regions to achieve state-of-the-art performance on a FER database containing images from realistic settings. A visual search strategy showing some similarities with human saccading behavior emerged when the model’s perceptive capabilities were restricted. However, the model then failed to form a useful scene representation.","tags":[],"title":"Recurrent Spatial Attention for Facial Emotion Recognition","type":"publication"},{"authors":["Enrico Schröder","Mirko Mählisch","Julien Vitay","Fred H. Hamker"],"categories":null,"content":"","date":1537912800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1537912800,"objectID":"b4d1c4baafb0ba3a1770d7374ef3dc73","permalink":"https://julien-vitay.net/publication/schroeder2018/","publishdate":"2018-09-26T00:00:00+02:00","relpermalink":"/publication/schroeder2018/","section":"publication","summary":"We present a novel architecture for intermediate fusion of Lidar and camera data for neural network-based object detection. Key component is a transformer module which learns a transformation of feature maps from one sensor space to another. This allows large parts of the multi-modal object detection network to be trained unimodally, reducing the required amount of costly multi-modal labeled data. We show effectiveness of the transformer as well as the proposed fusion scheme.","tags":[],"title":"Fusion of Camera and Lidar Data for Object Detection using Neural Networks","type":"publication"},{"authors":["Francesc Villagrasa","Javier Baladron","Julien Vitay","Henning Schroll","Evan G. Antzoulatos","Earl K. Miller","Fred H. Hamker"],"categories":null,"content":"   ","date":1537221600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1537221600,"objectID":"9accb64a61ade6f3b4f5a4a09806ea6b","permalink":"https://julien-vitay.net/publication/villagrasa2018/","publishdate":"2018-09-18T00:00:00+02:00","relpermalink":"/publication/villagrasa2018/","section":"publication","summary":"In addition to the prefrontal cortex (PFC), the basal ganglia (BG) have been increasingly often reported to play a fundamental role in category learning, but the systems-level circuits of how both interact remain to be explored. We developed a novel neuro-computational model of category learning that particularly addresses the BG-PFC interplay. We propose that the BG bias PFC activity by removing the inhibition of cortico-thalamo-cortical loop ahugo new --kind publication publication/nd thereby provide a teaching signal to guide the acquisition of category representations in the cortico-cortical associations to the PFC. Our model replicates key behavioral and physiological data of macaque monkey learning a prototype distortion task from Antzoulatos and Miller (2011). Our simulations allowed us to gain a deeper insight into the observed drop of category selectivity in striatal neurons seen in the experimental data and in the model. The simulation results and a new analysis of the experimental data, based on the model's predictions, show that the drop in category selectivity of the striatum emerges as the variability of responses in the striatum rises when confronting the BG with an increasingly larger number of stimuli to be classified. The neuro-computational model therefore provides new testable insights of systems-level brain circuits involved in category learning which may also be generalized to better understand other cortico-basal ganglia-cortical loops.","tags":["Basal Ganglia"],"title":"On the role of cortex-basal ganglia interactions for category learning: A neuro-computational approach","type":"publication"},{"authors":["Nicolas P. Rougier​","Konrad Hinsen","Frédéric Alexandre","Thomas Arildsen","Lorena A. Barba","Fabien C.Y. Benureau","C. Titus Brown","Pierre de Buyl","Ozan Caglayan","Andrew P. Davison","Marc-André Delsuc","Georgios Detorakis","Alexandra K. Diem","Damien Drix","Pierre Enel","Benoît Girard","Olivia Guest","Matt G. Hall","Rafael N. Henriques","Xavier Hinau","Kamil S. Jaron","Mehdi Khamassi","Almar Klein","Tiina Manninen","Pietro Marchesi","Daniel McGlinn","Christoph Metzner","Owen Petchey","Hans Ekkehard Plesser","Timothée Poisot","Karthik Ram","Yoav Ram","Etienne Roesch","Cyrille Rossant","Vahid Rostami","Aaron Shifman","Joseph Stachelek","Marcel Stimberg","Frank Stollmeier","Federico Vaggi","Guillaume Viejo","Julien Vitay","Anya E. Vostinar","Roman Yurchak","Tiziano Zito"],"categories":null,"content":"","date":1513551600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1513551600,"objectID":"d763b82a242ee202d5419cac2b8953d3","permalink":"https://julien-vitay.net/publication/rougier2017/","publishdate":"2017-12-18T00:00:00+01:00","relpermalink":"/publication/rougier2017/","section":"publication","summary":"Computer science offers a large set of tools for prototyping, writing, running, testing, validating, sharing and reproducing results; however, computational science lags behind. In the best case, authors may provide their source code as a compressed archive and they may feel confident their research is reproducible. But this is not exactly true. James Buckheit and David Donoho proposed more than two decades ago that an article about computational results is advertising, not scholarship. The actual scholarship is the full software environment, code, and data that produced the result. This implies new workflows, in particular in peer-reviews. Existing journals have been slow to adapt: source codes are rarely requested and are hardly ever actually executed to check that they produce the results advertised in the article. ReScience is a peer-reviewed journal that targets computational research and encourages the explicit replication of already published research, promoting new and open-source implementations in order to ensure that the original research can be replicated from its description. To achieve this goal, the whole publishing chain is radically different from other traditional scientific journals. ReScience resides on GitHub where each new implementation of a computational study is made available together with comments, explanations, and software tests.","tags":[],"title":"Sustainable computational science: the ReScience initiative","type":"publication"},{"authors":["Lorenz Gönner","Julien Vitay","Fred H. Hamker"],"categories":null,"content":"   ","date":1507759200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1507759200,"objectID":"5ee18f223459f73afd2e695e0d836572","permalink":"https://julien-vitay.net/publication/goenner2017/","publishdate":"2017-10-12T00:00:00+02:00","relpermalink":"/publication/goenner2017/","section":"publication","summary":"Hippocampal place-cell sequences observed during awake immobility often represent previous experience, suggesting a role in memory processes. However, recent reports of goals being overrepresented in sequential activity suggest a role in short-term planning, although a detailed understanding of the origins of hippocampal sequential activity and of its functional role is still lacking. In particular, it is unknown which mechanism could support efficient planning by generating place-cell sequences biased toward known goal locations, in an adaptive and constructive fashion. To address these questions, we propose a model of spatial learning and sequence generation as interdependent processes, integrating cortical contextual coding, synaptic plasticity and neuromodulatory mechanisms into a map-based approach. Following goal learning, sequential activity emerges from continuous attractor network dynamics biased by goal memory inputs. We apply Bayesian decoding on the resulting spike trains, allowing a direct comparison with experimental data. Simulations show that this model (1) explains the generation of never-experienced sequence trajectories in familiar environments, without requiring virtual self-motion signals, (2) accounts for the bias in place-cell sequences toward goal locations, (3) highlights their utility in flexible route planning, and (4) provides specific testable predictions.","tags":["computational neuroscience"],"title":"Predictive Place-Cell Sequences for Goal-Finding Emerge from Goal Memory and the Cognitive Map: A Computational Model","type":"publication"},{"authors":["Winfried Lötzsch","Julien Vitay","Fred H. Hamker"],"categories":null,"content":"","date":1505426400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1505426400,"objectID":"40edd0911ed2f061f59949f035eda35c","permalink":"https://julien-vitay.net/publication/loetzsch2017/","publishdate":"2017-09-15T00:00:00+02:00","relpermalink":"/publication/loetzsch2017/","section":"publication","summary":"Recent advances in deep reinforcement learning methods have attracted a lot of attention, because of their ability to use raw signals such as video streams as inputs, instead of pre-processed state variables. However, the most popular methods (value-based methods, e.g. deep Q-networks) focus on discrete action spaces (e.g. the left/right buttons), while realistic robotic applications usually require a continuous action space (for example the joint space). Policy gradient methods, such as stochastic policy gradient or deep deterministic policy gradient, propose to overcome this problem by allowing continuous action spaces. Despite their promises, they suffer from long training times as they need huge numbers of interactions to converge. In this paper, we investigate in how far a recent asynchronously parallel actor-critic approach, initially proposed to speed up discrete RL algorithms, could be used for the continuous control of robotic arms. We demonstrate the capabilities of this end-to-end learning algorithm on a simulated 2 degrees-of-freedom robotic arm and discuss its applications to more realistic scenarios.","tags":[],"title":"Training a deep policy gradient-based neural network with asynchronous learners on a simulated robotic problem","type":"publication"},{"authors":["Julien Vitay"],"categories":null,"content":"","date":1483225200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483225200,"objectID":"cfa8f37e8805d2a7949cf01bb5f201dc","permalink":"https://julien-vitay.net/publication/vitay2017/","publishdate":"2017-01-01T00:00:00+01:00","relpermalink":"/publication/vitay2017/","section":"publication","summary":"Neuro-computational models allow to study the brain mechanisms involved in intelligent behavior and extract essential computational principles which can be implemented in cognitive systems. They are a promising solution to achieve a brain-like artificial intelligence that can compete with natural intelligence on realistic behaviors. A crucial property of intelligent behavior is motivation, defined as the incentive to interact with the world in order to achieve specific goals, either extrinsic (obtaining rewards such as food or money, or avoiding pain) or intrinsic (satisfying one's curiosity, fun). In the human brain, motivated or goal-directed behavior depends on a network of different structures, including the prefrontal cortex, the basal ganglia and the limbic system. Dopamine, a neuro-transmitter associated with reward processing, plays a central role in coordinating the activity of this network. It structures processing in high-level cognitive areas along a limbic-associative-motor gradient and impacts the learning capabilities of the whole system. In this habilitation thesis, I present biologically-constrained neuro-computational models which investigate the role of dopamine in visual object categorization and memory retrieval (Vitay and Hamker, 2008), reinforcement learning and action selection (Vitay and Hamker, 2010), the updating, learning and maintenance of working memory (Schroll, Vitay and Hamker, 2012) and timing processes (Vitay and Hamker, 2014). These models outline the many mechanisms by which the dopaminergic system regulates cognitive and emotional behavior: bistable processing modes in the cerebral cortex, modulation of synaptic transmission and plasticity, allocation of cognitive resources and signaling of relevant events. Finally, I present a neural simulator able to simulate a variety of neuro-computational models efficiently on parallel architectures (Vitay, Dinkelbach and Hamker, 2015).","tags":[],"title":"On the role of dopamine in motivated behavior: a neuro-computational approach","type":"publication"},{"authors":["Julien Vitay"],"categories":null,"content":"","date":1475791200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1475791200,"objectID":"1fc6ac98276866e5efc601fffb81aa00","permalink":"https://julien-vitay.net/publication/vitay2016/","publishdate":"2016-10-07T00:00:00+02:00","relpermalink":"/publication/vitay2016/","section":"publication","summary":"A reference implementation of: Laje, R. and Buonomano, D.V. (2013). Robust timing and motor patterns by taming chaos in recurrent neural networks. Nat Neurosci. 16(7) pp 925-33 doi://10.1038/nn.3405","tags":[],"title":"[Re] Robust timing and motor patterns by taming chaos in recurrent neural networks","type":"publication"},{"authors":["Julien Vitay","Helge Ü. Dinkelbach","Fred H. Hamker"],"categories":null,"content":"","date":1438293600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1438293600,"objectID":"271e3121740d74e7f219bd6651aec288","permalink":"https://julien-vitay.net/publication/vitay2015/","publishdate":"2015-07-31T00:00:00+02:00","relpermalink":"/publication/vitay2015/","section":"publication","summary":"Many modern neural simulators focus on the simulation of networks of spiking neurons on parallel hardware. Another important framework in computational neuroscience, rate-coded neural networks, is mostly difficult or impossible to implement using these simulators. We present here the ANNarchy (Artificial Neural Networks architect) neural simulator, which allows to easily define and simulate rate-coded and spiking networks, as well as combinations of both. The interface in Python has been designed to be close to the PyNN interface, while the definition of neuron and synapse models can be specified using an equation-oriented mathematical description similar to the Brian neural simulator. This information is used to generate C++ code that will efficiently perform the simulation on the chosen parallel hardware (multi-core system or graphical processing unit). Several numerical methods are available to transform ordinary differential equations into an efficient C++code. We compare the parallel performance of the simulator to existing solutions.","tags":[],"title":"ANNarchy: a code generation approach to neural simulations on parallel hardware","type":"publication"},{"authors":["Julien Vitay","Fred H. Hamker"],"categories":null,"content":"","date":1391122800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1391122800,"objectID":"3c29360a061b50e70ddcd6b074002006","permalink":"https://julien-vitay.net/publication/vitay2014/","publishdate":"2014-01-31T00:00:00+01:00","relpermalink":"/publication/vitay2014/","section":"publication","summary":"Neural activity in dopaminergic areas such as the ventral tegmental area is influenced by timing processes, in particular by the temporal expectation of rewards during Pavlovian conditioning. Receipt of a reward at the expected time allows to compute reward-prediction errors which can drive learning in motor or cognitive structures. Reciprocally, dopamine plays an important role in the timing of external events. Several models of the dopaminergic system exist, but the substrate of temporal learning is rather unclear. In this article, we propose a neuro-computational model of the afferent network to the ventral tegmental area, including the lateral hypothalamus, the pedunculopontine nucleus, the amygdala, the ventromedial prefrontal cortex, the ventral basal ganglia (including the nucleus accumbens and the ventral pallidum), as well as the lateral habenula and the rostromedial tegmental nucleus. Based on a plausible connectivity and realistic learning rules, this neuro-computational model reproduces several experimental observations, such as the progressive cancelation of dopaminergic bursts at reward delivery, the appearance of bursts at the onset of reward-predicting cues or the influence of reward magnitude on activity in the amygdala and ventral tegmental area. While associative learning occurs primarily in the amygdala, learning of the temporal relationship between the cue and the associated reward is implemented as a dopamine-modulated coincidence detection mechanism in the nucleus accumbens.","tags":[],"title":"Timing and expectation of reward: a neuro-computational model of the afferents to the ventral tegmental area","type":"publication"},{"authors":["Henning Schroll","Julien Vitay","Fred H. Hamker"],"categories":null,"content":"","date":1388530800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388530800,"objectID":"850adb49939ede430a70a206304e8028","permalink":"https://julien-vitay.net/publication/schroll2014/","publishdate":"2014-01-01T00:00:00+01:00","relpermalink":"/publication/schroll2014/","section":"publication","summary":"In Parkinson's disease, a loss of dopamine neurons causes severe motor impairments. These motor impairments have long been thought to result exclusively from immediate effects of dopamine loss on neuronal firing in basal ganglia, causing imbalances of basal ganglia pathways. However, motor impairments and pathway imbalances may also result from dysfunctional synaptic plasticity – a novel concept of how Parkinsonian symptoms evolve. Here we built a neuro-computational model that allows us to simulate the effects of dopamine loss on synaptic plasticity in basal ganglia. Our simulations confirm that dysfunctional synaptic plasticity can indeed explain the emergence of both motor impairments and pathway imbalances in Parkinson's disease, thus corroborating the novel concept. By predicting that dysfunctional plasticity results not only in reduced activation of desired responses, but also in their active inhibition, our simulations provide novel testable predictions. When simulating dopamine replacement therapy (which is a standard treatment in clinical practice), we observe a new balance of pathway outputs, rather than a simple restoration of non-Parkinsonian states. In addition, high doses of replacement are shown to result in overshooting motor activity, in line with empirical evidence. Finally, our simulations provide an explanation for the intensely debated paradox that focused basal ganglia lesions alleviate Parkinsonian symptoms, but do not impair performance in healthy animals. Overall, our simulations suggest that the effects of dopamine loss on synaptic plasticity play an essential role in the development of Parkinsonian symptoms, thus arguing for a re-conceptualisation of Parkinsonian pathophysiology.","tags":[],"title":"Dysfunctional and compensatory synaptic plasticity in Parkinson's disease","type":"publication"},{"authors":["Helge Ü. Dinkelbach","Julien Vitay","Fred H. Hamker"],"categories":null,"content":"","date":1352415600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1352415600,"objectID":"3a81a62bf787d25800b3ab358277137f","permalink":"https://julien-vitay.net/publication/dinkelbach2012/","publishdate":"2012-11-09T00:00:00+01:00","relpermalink":"/publication/dinkelbach2012/","section":"publication","summary":"Modern parallel hardware such as multi-core processors (CPUs) and graphics processing units (GPUs) have a high computational power which can be greatly beneficial to the simulation of large-scale neural networks. Over the past years, a number of efforts have focused on developing parallel algorithms and simulators best suited for the simulation of spiking neural models. In this article, we aim at investigating the advantages and drawbacks of the CPU and GPU parallelization of mean-firing rate neurons, widely used in systems-level computational neuroscience. By comparing OpenMP, CUDA and OpenCL implementations towards a serial CPU implementation, we show that GPUs are better suited than CPUs for the simulation of very large networks, but that smaller networks would benefit more from an OpenMP implementation. As this performance strongly depends on data organization, we analyze the impact of various factors such as data structure, memory alignment and floating precision. We then discuss the suitability of the different hardware depending on the networks' size and connectivity, as random or sparse connectivities in mean-firing rate networks tend to break parallel performance on GPUs due to the violation of coalescence.","tags":[],"title":"Comparison of GPU- and CPU-implementations of mean-firing rate neural networks on parallel hardware","type":"publication"},{"authors":["Julien Vitay","Fred H. Hamker"],"categories":null,"content":"","date":1325372400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1325372400,"objectID":"3d3c75c126f95913dc31d83426b62a7c","permalink":"https://julien-vitay.net/publication/vitay2012/","publishdate":"2012-01-01T00:00:00+01:00","relpermalink":"/publication/vitay2012/","section":"publication","summary":"","tags":[],"title":"Basal Ganglia learning","type":"publication"},{"authors":["Henning Schroll","Julien Vitay","Fred H. Hamker"],"categories":null,"content":"","date":1325372400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1325372400,"objectID":"28da28bb750c6e4b3460332afd2b4bb4","permalink":"https://julien-vitay.net/publication/schroll2012/","publishdate":"2012-01-01T00:00:00+01:00","relpermalink":"/publication/schroll2012/","section":"publication","summary":"Cortico-basalganglio-thalamic loops are involved in both cognitive processes and motor control. We present a biologically meaningful computational model of how these loops contribute to the organization of working memory and the development of response behavior. Via reinforcement learning in basal ganglia, the model develops flexible control of working memory within prefrontal loops and achieves selection of appropriate responses based on working memory content and visual stimulation within a motor loop. We show that both working memory control and response selection can evolve within parallel and interacting cortico-basalganglio-thalamic loops by Hebbian and three-factor learning rules. Furthermore, the model gives a coherent explanation for how complex strategies of working memory control and response selection can derive from basic cognitive operations that can be learned via trial and error.","tags":[],"title":"Working memory and response selection: A computational account of interactions among cortico-basalganglio-thalamic loops","type":"publication"},{"authors":["Julien Vitay","Fred H. Hamker"],"categories":null,"content":"","date":1312149600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1312149600,"objectID":"bca3652d6e654b72c0afeb452d74d60b","permalink":"https://julien-vitay.net/publication/vitay2011/","publishdate":"2011-08-01T00:00:00+02:00","relpermalink":"/publication/vitay2011/","section":"publication","summary":"While classical theories systematically opposed emotion and cognition, suggesting that emotions perturbed the normal functioning of the rational thought, recent progress in neuroscience highlights on the contrary that emotional processes are at the core of cognitive processes, directing attention to emotionally-relevant stimuli, favoring the memorization of external events, valuating the association between an action and its consequences, biasing decision making by allowing to compare the motivational value of different goals and, more generally, guiding behavior towards fulfilling the needs of the organism. This article first proposes an overview of the brain areas involved in the emotional modulation of behavior and suggests a functional architecture allowing to perform efficient decision making. It then reviews a series of biologically-inspired computational models of emotion dealing with behavioral tasks like classical conditioning and decision making, which highlight the computational mechanisms involved in emotional behavior. It underlines the importance of embodied cognition in artificial intelligence, as emotional processing is at the core of the cognitive computations deciding which behavior is more appropriate for the agent. ","tags":[],"title":"A Neuroscientific View on the Role of Emotions in Behaving Cognitive Agents","type":"publication"},{"authors":["Julien Vitay","Fred H. Hamker"],"categories":null,"content":"","date":1274997600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1274997600,"objectID":"925ba0b0e3350ca06ee0e7157db89105","permalink":"https://julien-vitay.net/publication/vitay2010/","publishdate":"2010-05-28T00:00:00+02:00","relpermalink":"/publication/vitay2010/","section":"publication","summary":"Visual working memory (WM) tasks involve a network of cortical areas such as inferotemporal, medial temporal and prefrontal cortices. We suggest here to investigate the role of the basal ganglia (BG) in the learning of delayed rewarded tasks through the selective gating of thalamocortical loops. We designed a computational model of the visual loop linking the perirhinal cortex, the BG and the thalamus, biased by sustained representations in prefrontal cortex. This model learns concurrently different delayed rewarded tasks that require to maintain a visual cue and to associate it to itself or to another visual object to obtain reward. The retrieval of visual information is achieved through thalamic stimulation of the perirhinal cortex. The input structure of the BG, the striatum, learns to represent visual information based on its association to reward, while the output structure, the substantia nigra pars reticulata, learns to link striatal representations to the disinhibition of the correct thalamocortical loop. In parallel, a dopaminergic cell learns to associate striatal representations to reward and modulates learning of connections within the BG. The model provides testable predictions about the behavior of several areas during such tasks, while providing a new functional organization of learning within the BG, putting emphasis on the learning of the striatonigral connections as well as the lateral connections within the substantia nigra pars reticulata. It suggests that the learning of visual WM tasks is achieved rapidly in the BG and used as a teacher for feedback connections from prefrontal cortex to posterior cortices.","tags":[],"title":"A computational model of basal ganglia and its role in memory retrieval in rewarded visual memory tasks","type":"publication"},{"authors":["Julien Vitay","Jérémy Fix","Frederik Beuth","Henning Schroll","Fred H. Hamker"],"categories":null,"content":"","date":1251756000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1251756000,"objectID":"4cc9f5e0799ad57e70b9e0248b482623","permalink":"https://julien-vitay.net/publication/vitay2009/","publishdate":"2009-09-01T00:00:00+02:00","relpermalink":"/publication/vitay2009/","section":"publication","summary":"This review focuses on biological issues of reinforcement learning. Since the influential discovery of W. Schultz of an analogy between the reward prediction error signal of the temporal difference algorithm and the firing pattern of some dopaminergic neurons in the midbrain during classical conditioning, biological models have emerged that use computational reinforcement learning concepts to explain adaptative behavior. In particular, the basal ganglia has been proposed to implement among other things reinforcement learning for action selection, motor control or working memory. We discuss to which extent the analogy between the temporal difference algorithm and the firing of dopamine cells can be considered as valid. Our review then focuses on the basal ganglia, their anatomy and key computational properties as demonstrated by three recent, influential models.","tags":[],"title":"Biological Models of Reinforcement Learning","type":"publication"},{"authors":["Julien Vitay","Fred H. Hamker"],"categories":null,"content":"","date":1223935200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1223935200,"objectID":"e30ed5087fed8186a1f9f7297f5e0c0b","permalink":"https://julien-vitay.net/publication/vitay2008/","publishdate":"2008-10-14T00:00:00+02:00","relpermalink":"/publication/vitay2008/","section":"publication","summary":"The perirhinal cortex is involved not only in object recognition and novelty detection but also in multimodal integration, reward association, and visual working memory. We propose a computational model that focuses on the role of the perirhinal cortex in working memory, particularly with respect to sustained activities and memory retrieval. This model describes how different partial informations are integrated into assemblies of neurons that represent the identity of an object. Through dopaminergic modulation, the resulting clusters can retrieve the global information with recurrent interactions between neurons. Dopamine leads to sustained activities after stimulus disappearance that form the basis of the involvement of the perirhinal cortex in visual working memory processes. The information carried by a cluster can also be retrieved by a partial thalamic or prefrontal stimulation. Thus, we suggest that areas involved in planning and memory coordination encode a pointer to access the detailed information encoded in the associative cortex such as the perirhinal cortex.","tags":[],"title":"Sustained Activities and Retrieval in a Computational Model of the Perirhinal Cortex","type":"publication"},{"authors":["Julien Vitay","Fred H. Hamker"],"categories":null,"content":"","date":1167606000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1167606000,"objectID":"bac476c8ffcd2039eddd09b825f1ad1f","permalink":"https://julien-vitay.net/publication/vitay2007/","publishdate":"2007-01-01T00:00:00+01:00","relpermalink":"/publication/vitay2007/","section":"publication","summary":"Although dopamine is one of the most studied neurotransmitter in the brain, its exact function is still unclear. This short review focuses on its role in different levels of cognitive vision: visual processing, visual attention and working memory. Dopamine can influence cognitive vision either through direct modulation of visual cells or through gating of basal ganglia functioning. Even if its classically assigned role is to signal reward prediction error, we review evidence that dopamine is also involved in novelty detection and attention shifting and discuss the possible implications for computational modeling.","tags":[],"title":"On the role of dopamine in cognitive vision","type":"publication"},{"authors":["Julien Vitay"],"categories":null,"content":"","date":1151013600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1151013600,"objectID":"2945480723c2b451a5e837ea3d41dfbc","permalink":"https://julien-vitay.net/publication/phd/","publishdate":"2006-06-23T00:00:00+02:00","relpermalink":"/publication/phd/","section":"publication","summary":"This thesis ascribes in the field of computational neuroscience whose goal is to model complex cognitive functions by means of numerical computer simulations while getting inspiration from cerebral functioning. Contrary to a top-down approach necessitating to know an analytic expression of the function to be simulated, the chosen bottom-up approach allows to observe the emergence of a function thanks to the interaction of artificial neural populations without any prior knowledge. We first present a particular neural network type, neural fields, whose properties of robustness to noise and spatio-temporal continuity allow that emergence. In order to guide the emergence of sensorimotor transformations onto this substrate, we then present the architecture of the visual and motor systems to highlight the central role of visual attention in the realization of these functions by the brain. We then propose a functional diagram of sensorimotor transformations where the preparation of an ocular saccade guides attention towards a region of visual space and allow movement preparation. We last describe a computational model of attentional spotlight displacement that, by using a dynamical spatial working memory, allows sequential search of a target in a visual scene thanks to the phenomenon of inhibition of return. The performances of this model (robustness to noise, to object movement and to saccade execution) are analyzed in simulation and on a robotic platform.","tags":[],"title":"Emergence of sensorimotor functions on a numerical distributed neural substrate","type":"publication"},{"authors":["Jérémy Fix","Julien Vitay","Nicolas P. Rougier"],"categories":null,"content":"","date":1136070000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1136070000,"objectID":"752a936880192382486303164b39aefa","permalink":"https://julien-vitay.net/publication/fix2006/","publishdate":"2006-01-01T00:00:00+01:00","relpermalink":"/publication/fix2006/","section":"publication","summary":"Some visual search tasks require to memorize the location of stimuli that have been previously scanned. Considerations about the eye movements raise the question of how we are able to maintain a coherent memory, despite the frequent drastically changes in the perception. In this article, we present a computational model that is able to anticipate the consequences of the eye movements on the visual perception in order to update a spatial memory.","tags":[],"title":"A computational model of spatial memory anticipation during visual search","type":"publication"},{"authors":["Nicolas P. Rougier","Julien Vitay"],"categories":null,"content":"","date":1117576800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1117576800,"objectID":"dc7fa3ac56ba9ce9ccfdf73981599746","permalink":"https://julien-vitay.net/publication/rougier2005/","publishdate":"2005-06-01T00:00:00+02:00","relpermalink":"/publication/rougier2005/","section":"publication","summary":"We present a dynamic model of attention based on the Continuum Neural Field Theory that explains attention as being an emergent property of a neural population. This model is experimentally proved to be very robust and able to track one static or moving target in the presence of very strong noise or in the presence of a lot of distractors, even more salient than the target. This attentional property is not restricted to the visual case and can be considered as a generic attentional process of any spatio-temporal continuous input.","tags":[],"title":"Emergence of attention within a neural population","type":"publication"},{"authors":["Julien Vitay","Nicolas P. Rougier","Frédéric Alexandre"],"categories":null,"content":"","date":1104534000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1104534000,"objectID":"4be0f60d95b2407ef701d218b7fe333a","permalink":"https://julien-vitay.net/publication/vitay2005/","publishdate":"2005-01-01T00:00:00+01:00","relpermalink":"/publication/vitay2005/","section":"publication","summary":"Although biomimetic autonomous robotics relies on the massively parallel architecture of the brain, the key issue is to temporally organize behaviour. The distributed representation of the sensory information has to be coherently processed to generate relevant actions. In the visual domain, we propose here a model of visual exploration of a scene by the means of localized computations in neural populations whose architecture allows the emergence of a coherent behaviour of sequential scanning of salient stimuli. It has been implemented on a real robotic platform exploring a moving and noisy scene including several identical targets.","tags":[],"title":"A distributed model of spatial visual attention","type":"publication"},{"authors":null,"categories":null,"content":"Neuro-computational models are different from classical neural networks (deep learning) in many aspects:\n The complexity of the neurons, whose activity is governed by one or several differential equations instead of a simple weighted sum. The complexity and diversity of the learning rules (synaptic plasticity), compared to gradient descent. The size of the networks needed to simulate significant parts of the brain. The huge diversity of models, architectures, frameworks used by researchers in computational neuroscience.  The increasing size of such networks asks for efficient parallel simulations, using distributed systems (OpenMP, MPI) or GPUs (CUDA). However, computational neuroscientists cannot be expected to be also experts in parallel computing. There is a need for a general-purpose neuro-simulator, with an easy but flexible interface allowing to define a huge variety of models, but which is internally efficient and allows for fast parallel simulations on various hardwares.\nOver many years, we have developed ANNarchy (Artificial Neural Networks architect), a parallel simulator for distributed rate-coded or spiking neural networks. The definition of the models is made in Python, but the library generates optimized C++ code to actually run the simulation on parallel hardware, using either openMP or CUDA. The current stable version is 4.6 and is released under the GNU GPL v2 or later.\nThe code is available at:\nhttps://bitbucket.org/annarchy/annarchy\nThe documentation is available at:\nhttps://annarchy.readthedocs.org\nCore principles ANNarchy separates the description of a neural network from its simulation. The description is declared in a Python script, offering high flexibility and readability of the code, and allowing to use the huge ecosystem of scientific libraries available with Python (Numpy, Scipy, Matplotlib\u0026hellip;). Using Python furthermore reduces the programming effort to a minimum, letting the modeller concentrate on network design and data analysis.\nA neural network is defined as a collection of interconnected populations of neurons. Each population comprises a set of similar artificial neurons (rate-coded or spiking point-neurons), whose activity is ruled by one or many ordinary differential equations. The activity of a neuron depends on the activity of other neurons through synapses, whose strength can evolve with time depending on pre- or post-synaptic activities (synaptic plasticity). Populations are interconnected with each other through projections, which contain synapses between two populations.\nANNarchy provides a set of classical neuron or synapse models, but also allows the definition of specific models. The ordinary differential equations (ODE) governing neural or synaptic dynamics have to be specified by the modeler. Contrary to other simulators (except Brian) which require to code these modules in a low-level language, ANNarchy provides a mathematical equation parser which can generate optimized C++ code depending on the chosen parallel framework. Bindings from C++ to Python are generated thanks to Cython (C-extensions to Python), which is a static compiler for Python. These bindings allow the Python script to access all data generated by the simulation (neuronal activity, connection weights) as if they were simple Python attributes. However, the simulation itself is independent from Python and its relatively low performance.\nExample of a pulse-coupled network of Izhikevich neurons To demonstrate the simplicity of ANNarchy\u0026rsquo;s interface, let\u0026rsquo;s focus on the \u0026ldquo;Hello, World!\u0026rdquo; of spiking networks: the pulse-coupled network of Izhikevich neurons (Izhikevich, 2003). It can be defined in ANNarchy as:\nfrom ANNarchy import * # Create the excitatory and inhibitory population pop = Population(geometry=1000, neuron=Izhikevich) Exc = pop[:800] ; Inh = pop[800:] # Set the population parameters re = np.random.random(800) ; ri = np.random.random(200) Exc.noise = 5.0 ; Inh.noise = 2.0 Exc.a = 0.02 ; Inh.a = 0.02 + 0.08 * ri Exc.b = 0.2 ; Inh.b = 0.25 - 0.05 * ri Exc.c = -65.0 + 15.0 * re**2 ; Inh.c = -65.0 Exc.d = 8.0 - 6.0 * re**2 ; Inh.d = 2.0 Exc.v = -65.0 ; Inh.v = -65.0 Exc.u = Exc.v * Exc.b ; Inh.u = Inh.v * Inh.b # Create the projections exc_proj = Projection(pre=Exc, post=pop, target='exc') exc_proj.connect_all_to_all(weights=Uniform(0.0, 0.5)) inh_proj = Projection(pre=Inh, post=pop, target='inh') inh_proj.connect_all_to_all(weights=Uniform(0.0, 1.0)) # Compile compile() # Start recording the spikes in the network to produce the plots M = Monitor(pop, ['spike', 'v']) # Simulate 1 second simulate(1000.0, measure_time=True) # Retrieve the spike recordings and the membrane potential spikes = M.get('spike') v = M.get('v') # Compute the raster plot t, n = M.raster_plot(spikes) # Compute the population firing rate fr = M.histogram(spikes) # Plot the results import matplotlib.pyplot as plt ax = plt.subplot(3,1,1) ax.plot(t, n, 'b.', markersize=1.0) ax = plt.subplot(3,1,2) ax.plot(v[:, 15]) ax = plt.subplot(3,1,3) ax.plot(fr) plt.show()  ","date":2019,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":2019,"objectID":"bc653bba084e5cb400a33372aa0df56a","permalink":"https://julien-vitay.net/project/annarchy/","publishdate":"1970-01-01T01:33:39+01:00","relpermalink":"/project/annarchy/","section":"project","summary":"ANNarchy (Artificial Neural Networks architect) is a general-purpose parallel neuro-simulator for rate-coded or spiking neural networks.","tags":["parallel computing","computational neuroscience"],"title":"ANNarchy","type":"project"},{"authors":null,"categories":null,"content":"The Basal Ganglia (BG) are a set of nuclei located in the basal forebrain, receiving inputs mostly from the cerebral cortex (especially the frontal lobe) and projecting on various motor centers, as well as back to the cortex through the thalamus, forming a closed-loop.\nThe main input station is the striatum, which can be anatomically divided into three parts: the nucleus accumbens (NAcc), the caudate nucleus (CN) and the putamen (PUT). Striatal neurons are excited by cortical activity and inhibit in turn the tonically active neurons of the output nuclei of the BG: the substantia nigra pars reticulata (SNr) and the internal segment of the globus pallidus (GPi). These output structures further inhibit some motor centers and thalamic nuclei.\nThis double inhibition allows to selectively open some recurrent loops between the thalamus and the cortex, increasing the signal-to-noise ratio in the cortex and triggering movements or cognitive functions.\nOther nuclei in the BG, such as the subthalamic nucleus (STN) and the external part of the globus pallidus (GPe), create functionally different pathways to allow for a more complex role of BG in adapting behavior.\nThe main characteristic of the BG is its dense innervation by dopaminergic (DA) cells in the substantia nigra pars compacta (SNc) and ventral tegmental area (VTA), whose firing is related to reward delivery and prediction. DA can modulate the activation and learning of most cells in the BG, placing it as a core structure in reinforcement learning processes.\n","date":2018,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":2018,"objectID":"50ba683e61037e6f049d19b270c7fbdb","permalink":"https://julien-vitay.net/project/basalganglia/","publishdate":"1970-01-01T01:33:38+01:00","relpermalink":"/project/basalganglia/","section":"project","summary":"The Basal Ganglia (BG) are the main nuclei involved in reinforcement learning processes in the brain and allow a variety of cognitive functions such as working memory, decision making and action selection.","tags":["Computational Neuroscience"],"title":"Basal Ganglia","type":"project"},{"authors":null,"categories":null,"content":"The recent hype on deep learning has revived the interest for artificial neural networks and their applications. Here are some projects done lately.\nProject 1 : Facial emotion recognition Facial expression recognition is an important research field in computer vision. Although detecting facial features is an easy task for a human, computers still have a hard time doing it. Factors such as interpersonal variation (gender, skin color), intrapersonal variation (pose, expression) and different recording conditions (image resolution, lighting) add to the complexity of the problem. This is particularly relevant in the context of emotion recognition, where systems should be able to automatically recognize in which emotional state humans are.\nOn human faces, emotional expression heavily relies on the activation of individual facial muscles. A classical approach to describe facial expressions at the muscular level is the Facial Action Coding System (FACS) proposed by Ekman (1978). In this framework, movement of specific facial regions are described as Actions Units (AU), which basically describe deviations from a neutral expression. AUs are specific to facial regions (corner of the mouth or the eye, etc.). Although there are 69 AUs in the FACS theory, 28 of them are mostly useful for emotion recognition. We have focused on 12 of them: 1 (Inner Brow Raiser), 2 (Outer Brow Raiser), 4 (Brow Lowerer), 6 (Cheek Raiser), 7 (Lid Tightener), 10 (Upper Lip Raiser), 12 (Lip Corner Puller), 14 (Dimpler), 15 (Lip Corner Depressor), 17 (Chin Raiser), 23 (Lip Tightener), 24 (Lip Pressor).\nThere are different training sets generally available to the community containing various number of FACS-annotated images, with different numbers of annotated AUs: CCK+, MMI, UNBC-McMaster PAIN, DUSFA, BP4D, SEMAINE, etc. The 12 selected AUs correspond to the annotated AUs in BP4D, which is the most massive dataset. The main interest of these AUs is that they are mostly sufficient to predict the occurence of the 6 basic emotions using the EMFACS correspondance table:\n   Emotion Action Units     Happiness 6, 12   Sadness 1, 4, 15   Surprise 1, 2, 5B, 26   Fear 1, 2, 4, 5, 7, 20, 26   Anger 4, 5, 7, 23   Disgust 9, 15, 16    After investigating various architectures to automatically predict AU occurence on faces, we converged towards a neural network architecture inspired from VGG-16:\n Convolutional Neural Network for FACS recognition.   It consists of 4 convolutional blocks, each composed of 2 convolutional layers (kernel size 3x3, ReLU activation function) and a max-pooling layers (2x2). A dropout layer with p=0.2 is added after the max-pooling. After 4 such convolutional blocks with increasing numbers of features (32, 64, 126 and 256), the last tensor (6x6x256) is flattened into a vector of 9216 elements and projected on a fully connected layer of 500 neurons. The output layer has 12 neurons using the sigmoid activation function, each representing one of the 12 AUs present in the combined dataset. The network has a total of 5.786.192 trainable parameters (weights and biases), what makes it a middle-sized deep network that can fit into the available GPUs at the lab. The model was trained over 120 epochs using Stochastic Gradient Descent (SGD) on minibatches of 128 samples, with a learning rate of 0.01 and a Nesterov momentum of 0.9. The network has successfully learned the training data (final loss of 0.02) and has only very slightly overfitted. F1 scores for each AU on the test set are well over 0.9.\nThe video below shows the performance of the network in real conditions. The detected AUs are in the top-left corner, the recognized emotion in the bottom-left one.\n Project 2 : Scene understanding Recurrent neural networks coupled with attentional mechanisms have the ability to sequentially focus of the relevant parts of a visual scene. Coupled with a language production network, scene understanding abilities can be improved by finding the spatial location of the important objects in a scene while describing it.\nThe idea of the work done by Saransh Vora during his Master thesis in 2018 at the professorship was to study and reimplement the Show, attend and tell model of (Xu et al 2015, arXiv:1502.03044). The attentional signal is used to locate the most important objects of the sentence in the image, and have a Nao point at them while pronouncing the sentence.\n  ","date":2017,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":2017,"objectID":"e852b07eddb0686d7d5ddcd7824f9387","permalink":"https://julien-vitay.net/project/deeplearning/","publishdate":"1970-01-01T01:33:37+01:00","relpermalink":"/project/deeplearning/","section":"project","summary":"Deep neural networks and their applications to emotion recognition, attention, sensor fusion...","tags":["machine-learning","deep-learning"],"title":"Deep Learning","type":"project"},{"authors":null,"categories":null,"content":"The dopaminergic system is composed of the ventral tegmental area (VTA) and the substantia nigra pars compacta (SNc). The neurotransmitter dopamine (DA) released by neurons in these two small areas exerts a strong influence on neural excitability and plasticity in many brain areas: mostly the basal ganglia (BG), but also the prefrontal cortex, the hippocampus or the amygdala.\nA striking feature of VTA cells is their response during classical (or Pavlovian) conditioning, as observed by Schultz et al (1998). Early on, VTA cells respond phasically (a burst) to unconditioned stimuli (US, or rewards in operant conditioning). Gradually during learning, the amplitude of this response decreases, replaced by a response to the conditioned stimuli (CS) which are predictive of reward delivery. Moreover, if a reward is predicted by the CS but omitted, VTA cells show a brief depression of activity (a dip) at the time where the US was expected. This pattern resembles the temporal difference (TD) error signal used in reinforcement learning, what generated multitudes of models based on that analogy.\nWhat remains unclear is how VTA cells access information about the US, the CS and more importantly the time elapsed since CS onset. The goal of this research project is to investigate the mechanisms by which VTA is able to exhibit these properties, by looking at the afferent system to VTA. VTa indeed receives information from many brain areas, either directly as the rostromedial tegmental area (RMTg), the pedunculopontine nucleus (PPTN) or the nucleus accumbens (NAcc), or indirectly as the amygdala, the lateral habenula (LHb), the ventral pallidum (VP) or the ventromedial prefrontal cortex (vmPFC).\n","date":2016,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":2016,"objectID":"e26cb680b5b5bd2046cc8002093cf86c","permalink":"https://julien-vitay.net/project/dopamine/","publishdate":"1970-01-01T01:33:36+01:00","relpermalink":"/project/dopamine/","section":"project","summary":"Modeling the dopaminergic system (VTA, SNc), its afferent system and its influence on the basal ganglia, prefrontal cortex and hippocampus.","tags":["computational-neuroscience"],"title":"Dopaminergic system","type":"project"},{"authors":null,"categories":null,"content":"The hippocampus is a key structure for episodic memory and spatial navigation. A fundamental step in hippocampus research was the discovery of place cells, which fire whenever an animal traverses a certain location known as the place field (O\u0026rsquo;Keefe and Nadel, 1978). At rest, place cells exhibit brief periods of fast oscillations termed sharp wave-ripples. During these events, place cell activity shows sequential patterns called forward replay and reverse replay: time-compressed, and sometimes time-reversed, reproductions of previously experienced sequences. Spatial experiences stored in the hippocampus can therefore be recalled at will during behavior.\nOur modeling work focuses on understanding how these replay patterns are learned and used for high-level cognitive processes such as decision-making and model-based reinforcement learning.\n","date":2015,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":2015,"objectID":"7e0de273b7298af5874493d7959090a2","permalink":"https://julien-vitay.net/project/hippocampus/","publishdate":"1970-01-01T01:33:35+01:00","relpermalink":"/project/hippocampus/","section":"project","summary":"The hippocampus is a key structure for mnemonic processes (episodic memory) and spatial navigation. Its importance in model-based behavior is increasingly recognized.","tags":[],"title":"Hippocampus","type":"project"},{"authors":null,"categories":null,"content":"Deep reinforcement learning (deep RL) is the integration of deep learning methods, classically used in supervised or unsupervised learning contexts, with reinforcement learning (RL), a well-studied adaptive control method used in problems with delayed and partial feedback.\nHome-made textbook on deep RL:\nhttps://julien-vitay.net/deeprl\n","date":2014,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":2014,"objectID":"1e3da4aaefde9904f0d2d4f899ebda67","permalink":"https://julien-vitay.net/project/reinforcementlearning/","publishdate":"1970-01-01T01:33:34+01:00","relpermalink":"/project/reinforcementlearning/","section":"project","summary":"Reinforcement Learning (RL) is a machine learning framework studying how to derive optimal policies from reward signals. Coupled with deep neural networks, it became the most promising approach to artificial intelligence.","tags":[],"title":"Deep Reinforcement Learning","type":"project"},{"authors":null,"categories":null,"content":"The field of reservoir computing, covering Echo-State Networks (ESN; Jaeger, 2000) and Liquid State Machines (Maas, 2001), studies the dynamical properties of recurrent neural networks. Depending on the strength of the recurrent connections, the reservoirs can exhibit either deterministic or chaotic trajectories following a stimulation.\nThese trajectories can serve as a complex temporal basis to represent events. In their early formulation, reservoirs were fixed and read-out neurons used this basis to mimic specific target signals using supervised learning.\nIn the recent years, methods have been developed to train the connections inside the reservoir too, either using supervised learning (e.g. Laje and Buonomano 2013) or reinforcement learning (Miconi, 2017). This unleashes the potential of reservoirs for both machine learning applications and computational neuroscience.\nWe study the properties of reservoir computing at the neuroscientific level, with an emphasis on reinforcement learning, forward models in the cerebellum (Schmid et al, 2019) or interval timing.\n","date":2013,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":2013,"objectID":"1a53fa0ad9d414fa3294e3c4c1d06b3a","permalink":"https://julien-vitay.net/project/reservoircomputing/","publishdate":"1970-01-01T01:33:33+01:00","relpermalink":"/project/reservoircomputing/","section":"project","summary":"Reservoir computing studies the dynamical properties of recurrently connected populations of neurons. Their rich dynamics allow to represent and learn complex tasks currently out of reach of the classical machine learning methods, but also allow to better understand brain activities.","tags":[],"title":"Reservoir Computing","type":"project"},{"authors":["Julien Vitay"],"categories":null,"content":"ANNarchy (Artificial Neural Networks architect) Julien Vitay  ANNarchy (Artificial Neural Networks architect) Source code:\nhttps://bitbucket.org/annarchy/annarchy\nDocumentation:\nhttps://annarchy.readthedocs.io/en/stable/\nForum:\nhttps://groups.google.com/forum/#!forum/annarchy\nNotebooks used in this tutorial:\nhttps://github.com/vitay/ANNarchy-notebooks\n Installation Installation guide: https://annarchy.readthedocs.io/en/stable/intro/Installation.html\nFrom pip:\npip install ANNarchy  From source:\ngit clone https://bitbucket.org/annarchy/annarchy.git cd annarchy python setup.py install  Requirements (Linux and MacOS):\n g++/clang++, python 2.7 or 3.5+, numpy, scipy, matplotlib, sympy, cython   Features   Simulation of both rate-coded and spiking neural networks.\n  Only local biologically realistic mechanisms are possible (no backpropagation).\n  Equation-oriented description of neural/synaptic dynamics (à la Brian).\n  Code generation in C++, parallelized using OpenMP on CPU and CUDA on GPU (MPI is coming).\n  Synaptic, intrinsic and structural plasticity mechanisms.\n    Structure of a script from ANNarchy import * setup(dt=1.0) neuron = Neuron(...) # Create neuron types stdp = Synapse(...) # Create synapse types for transmission and/or plasticity pop = Population(1000, neuron) # Create populations of neurons proj = Projection(pop, pop, 'exc', stdp) # Connect the populations proj.connect_fixed_probability(weights=0.0, probability=0.1) compile() # Generate and compile the code m = Monitor(pop, ['spike']) # Record spikes simulate(1000.) # Simulate for 1 second data = m.get('spike') # Retrieve the data and plot it   1 - Rate-coded networks  Example 1 : Echo-State Network  Echo-State Network ESN rate-coded neurons typically follow first-order ODEs:\n$$ \\tau \\frac{dx(t)}{dt} + x(t) = \\sum w^\\text{in} , r^\\text{in}(t) + g , \\sum w^\\text{rec} , r(t) + \\xi(t) $$\n$$ r(t) = \\tanh(x(t)) $$\nfrom ANNarchy import * ESN_Neuron = Neuron( parameters = \u0026quot;\u0026quot;\u0026quot; tau = 30.0 # Time constant g = 1.0 : population # Scaling noise = 0.01 : population # Noise amplitude \u0026quot;\u0026quot;\u0026quot;, equations=\u0026quot;\u0026quot;\u0026quot; tau * dx/dt + x = sum(in) + g * sum(exc) + noise * Uniform(-1, 1) : init=0.0 r = tanh(x) \u0026quot;\u0026quot;\u0026quot; )   Parameters parameters = \u0026quot;\u0026quot;\u0026quot; tau = 30.0 # Time constant g = 1.0 : population # Scaling noise = 0.01 : population # Noise amplitude \u0026quot;\u0026quot;\u0026quot;  All parameters used in the equations must be declared in the Neuron definition.\nParameters can have one value per neuron in the population (default) or be common to all neurons (flag population or projection).\nParameters and variables are double floats by default, but the type can be specified (int, bool).\n Variables equations=\u0026quot;\u0026quot;\u0026quot; tau * dx/dt + x = sum(in) + g * sum(exc) + noise * Uniform(-1, 1) : init=0.0 r = tanh(x) \u0026quot;\u0026quot;\u0026quot;  Variables are evaluated at each time step in the order of their declaration, except for coupled ODEs.\nVariables can be updated with assignments (=, +=, etc) or by defining first order ODEs.\nThe math C library symbols can be used (tanh, cos, exp, etc).\nInitial values at $t=0$ can be specified with init (default: 0.0).\nLower/higher bounds on the values of the variables can be set with the min/max flags:\nr = x : min=0.0 # ReLU  Additive noise can be drawn from several distributions, including Uniform, Normal, LogNormal, Exponential, Gamma\u0026hellip;\nThe output variable of a rate-coded neuron must be r.\n ODEs First-order ODEs are parsed and manipulated using sympy:\n# All equivalent: tau * dx/dt + x = 0.0 tau * dx/dt = - x dx/dt = (-x)/tau  Several numerical methods are available (https://annarchy.readthedocs.io/en/stable/manual/NumericalMethods.html):\n  Explicit (forward) Euler (default): tau * dx/dt + x = 0.0 : init=0.0, explicit\n  Implicit (backward) Euler: tau * dx/dt + x = 0.0 : init=0.0, implicit\n  Exponential Euler (exact for linear ODE): tau * dx/dt + x = 0.0 : init=0.0, exponential\n  Midpoint (RK2): tau * dx/dt + x = 0.0 : init=0.0, midpoint\n  Event-driven (spiking synapses): tau * dx/dt + x = 0.0 : init=0.0, event-driven\n   Coupled ODEs ODEs are solved concurrently, instead of sequentially for assignments:\n# I is updated I = sum(exc) - sum(inh) + b # u and v are solved concurrently using the current of I tau * dv/dt + v = I - u tau * du/dt + u = v # r uses the updated value of v r = tanh(v)  The order of the equations therefore matters a lot.\nA single variable can only be updated once in the equations field.\n Populations Populations are creating by specifying a number of neurons and a neuron type:\npop = Population(1000, ESN_Neuron)  For visualization purposes or when using convolutional layers, a tuple geometry can be passed instead of the size:\npop = Population((100, 100), ESN_Neuron)  All parameters and variables become attributes of the population (read and write) as numpy arrays:\npop.tau = np.linspace(20.0, 40.0, 1000) pop.r = np.tanh(pop.v)  Single neurons can be individually modified, if the population flag was not set:\npop[10].r = 1.0  Slices of populations are called PopulationView and can be addressed separately:\npop = Population(1000, ESN_Neuron) E = pop[:800] I = pop[800:]   Projections Projections link two populations (or views) in a uni-directional way.\nproj_exc = Projection(E, pop, 'exc') proj_inh = Projection(I, pop, 'inh')  Each target ('exc', 'inh', 'AMPA', 'NMDA', 'GABA') can be defined as needed and will be treated differently by the post-synaptic neurons.\nThe weighted sum of inputs for a specific target is accessed in the equations by sum(target):\nequations=\u0026quot;\u0026quot;\u0026quot; tau * dx/dt + x = sum(exc) - sum(inh) r = tanh(x) \u0026quot;\u0026quot;\u0026quot;  It is therefore possible to model modulatory effects, divisive inhibition, etc.\n Connection methods Projections must be populated with a connectivity matrix (who is connected to who), a weight w and optionally a delay d (uniform or variable).\nSeveral patterns are predefined:\nproj.connect_all_to_all(weights=Normal(0.0, 1.0), delays=2.0, allow_self_connections=False) proj.connect_one_to_one(weights=1.0, delays=Uniform(1.0, 10.0)) proj.connect_fixed_number_pre(number=20, weights=1.0) proj.connect_fixed_number_post(number=20, weights=1.0) proj.connect_fixed_probability(probability=0.2, weights=1.0) proj.connect_gaussian(amp=1.0, sigma=0.2, limit=0.001) proj.connect_dog(amp_pos=1.0, sigma_pos=0.2, amp_neg=0.3, sigma_neg=0.7, limit=0.001)  But you can also load Numpy arrays or Scipy sparse matrices. Example for synfire chains:\nw = np.array([[None]*pre.size]*post.size) for i in range(post.size): w[i, (i-1)%pre.size] = 1.0 proj.connect_from_matrix(w) w = lil_matrix((pre.size, post.size)) for i in range(pre.size): w[pre.size, (i+1)%post.size] = 1.0 proj.connect_from_sparse(w)   Compiling and running the simulation Once all populations and projections are created, you have to generate to the C++ code and compile it:\ncompile()  You can now manipulate all parameters/variables from Python thanks to the Cython bindings.\nA simulation is simply run for a fixed duration with:\nsimulate(1000.) # 1 second  You can also run a simulation until a criteria is filled, check:\nhttps://annarchy.readthedocs.io/en/stable/manual/Simulation.html#early-stopping\n Monitoring By default, a simulation is run in C++ without interaction with Python.\nYou may want to record some variables (neural or synaptic) during the simulation with a Monitor:\nm = Monitor(pop, ['v', 'r']) n = Monitor(proj, ['w'])  After the simulation, you can retrieve the recordings with:\nrecorded_v = m.get('v') recorded_r = m.get('r') recorded_w = n.get('w')  Warning: calling get() flushes the array.\nWarning: recording projections can quickly fill up the RAM (see Dendrites).\n Example 1: Echo-State Network Link to the Jupyter notebook on github: RC.ipynb\n 2 - Spiking networks  Spiking neurons Spiking neurons must also define two additional fields:\n  spike: condition for emitting a spike.\n  reset: what happens after a spike is emitted (at the start of the refractory period).\n  A refractory period in ms can also be specified.\nExample of the Leaky Integrate-and-Fire:\nLIF = Neuron( parameters=\u0026quot;\u0026quot;\u0026quot; tau = 20. E_L = -70. v_T = 0. v_r = -58. I = 50.0 \u0026quot;\u0026quot;\u0026quot;, equations=\u0026quot;\u0026quot;\u0026quot; tau * dv/dt = (E_L - v) + I : init=E_L \u0026quot;\u0026quot;\u0026quot;, spike=\u0026quot; v \u0026gt;= v_T \u0026quot;, reset=\u0026quot; v = v_r \u0026quot;, refractory = 2.0 )   Conductances / currents A pre-synaptic spike arriving to a spiking neuron increase the conductance g_target (e.g. g_exc or g_inh, depending on the projection).\nLIF = Neuron( parameters=\u0026quot;...\u0026quot;, equations=\u0026quot;\u0026quot;\u0026quot; tau * dv/dt = (E_L - v) + g_exc - g_inh \u0026quot;\u0026quot;\u0026quot;, spike=\u0026quot; v \u0026gt;= v_T \u0026quot;, reset=\u0026quot; v = v_r \u0026quot;, refractory = 2.0 )  Each spike increments g_target from the synaptic efficiency w of the corresponding synapse.\ng_target += w  This defines an instantaneous model of synaptic transmission.\n Conductances / currents For exponentially-decreasing or alpha-shaped synapses, ODEs have to be introduced for the conductance/current.\nThe exponential numerical method should be preferred, as integration is exact.\nLIF = Neuron( parameters=\u0026quot;...\u0026quot;, equations=\u0026quot;\u0026quot;\u0026quot; tau * dv/dt = (E_L - v) + g_exc + alpha_exc # exponential or alpha tau_exc * dg_exc/dt = - g_exc : exponential tau_exc * dalpha_exc/dt = exp((tau_exc - dt/2.0)/tau_exc) * g_exc - alpha_exc : exponential \u0026quot;\u0026quot;\u0026quot;, spike=\u0026quot; v \u0026gt;= v_T \u0026quot;, reset=\u0026quot; v = v_r \u0026quot;, refractory = 2.0 )   Conductances / currents  Example 2: AdEx - Adaptive exponential neuron Link to the Jupyter notebook on github: AdEx.ipynb\n$$ \\tau , \\frac{dv}{dt} = (E_L - v) + \\delta_T , \\exp \\frac{v-v_T}{\\delta_T} + I - w $$ $$ \\tau_w , \\frac{dw}{dt} = a , (v - E_L) - w $$\nAdEx = Neuron( parameters=\u0026quot;\u0026quot;\u0026quot; tau = 20. E_L = -70. v_T = -50. ; v_r = -58. delta_T = 2.0 a = 0.2 ; b = 0. tau_w = 30. I = 50.0 \u0026quot;\u0026quot;\u0026quot;, equations=\u0026quot;\u0026quot;\u0026quot; tau * dv/dt = (E_L - v) + delta_T * exp((v-v_T)/delta_T) + I - w : init=E_L tau_w * dw/dt = a * (v - E_L) - w : init=0.0 \u0026quot;\u0026quot;\u0026quot;, spike=\u0026quot; v \u0026gt;= 0.0 \u0026quot;, reset=\u0026quot; v = v_r ; w += b \u0026quot;, refractory = 2.0 )   3 - Synaptic plasticity  Rate-coded synapses : Intrator \u0026amp; Cooper BCM learning rule Synapses can also implement equations that will be evaluated after each neural update.\nIBCM = Synapse( parameters = \u0026quot;\u0026quot;\u0026quot; eta = 0.01 : projection tau = 2000.0 : projection \u0026quot;\u0026quot;\u0026quot;, equations = \u0026quot;\u0026quot;\u0026quot; tau * dtheta/dt + theta = post.r^2 : postsynaptic, exponential dw/dt = eta * post.r * (post.r - theta) * pre.r : min=0.0, explicit \u0026quot;\u0026quot;\u0026quot;, psp = \u0026quot; w * pre.r\u0026quot; )  The synaptic efficiency (weight) must be w.\nEach synapse can access pre- and post-synaptic variables with pre. and post..\nThe postsynaptic flag allows to do computations only once per post-synaptic neurons.\npsp optionally defines what will be summed by the post-synaptic neuron (e.g. psp = \u0026quot;w * log(pre.r)\u0026quot;).\n Plastic projections The synapse type just has to be passed to the Projection:\nproj = Projection(inp, pop, 'exc', IBCM)  Synaptic variables can be accessed as lists of lists for the whole projection:\nproj.w proj.theta  or for a single post-synaptic neuron (Dendrite):\nproj[10].w   Example 3: Miconi\u0026rsquo;s reward modulated RC network Link to the Jupyter notebook on github: Miconi.ipynb\n Spiking synapses : Example of Short-term plasticity (STP) Spiking synapses can define a pre_spike field, defining what happens when a pre-synaptic spike arrives at the synapse.\ng_target is an alias for the corresponding post-synaptic conductance: it will be replaced by g_exc or g_inh depending on how the synapse is used.\nBy default, a pre-synaptic spike increments the post-synaptic conductance from w: g_target += w\nSTP = Synapse( parameters = \u0026quot;\u0026quot;\u0026quot; tau_rec = 100.0 : projection tau_facil = 0.01 : projection U = 0.5 \u0026quot;\u0026quot;\u0026quot;, equations = \u0026quot;\u0026quot;\u0026quot; dx/dt = (1 - x)/tau_rec : init = 1.0, event-driven du/dt = (U - u)/tau_facil : init = 0.5, event-driven \u0026quot;\u0026quot;\u0026quot;, pre_spike=\u0026quot;\u0026quot;\u0026quot; g_target += w * u * x x *= (1 - u) u += U * (1 - u) \u0026quot;\u0026quot;\u0026quot; )   Spiking synapses : Example of Spike-Timing Dependent plasticity (STDP) post_spike similarly defines what happens when a post-synaptic spike is emitted.\nSTDP = Synapse( parameters = \u0026quot;\u0026quot;\u0026quot; tau_plus = 20.0 : projection ; tau_minus = 20.0 : projection A_plus = 0.01 : projection ; A_minus = 0.01 : projection w_min = 0.0 : projection ; w_max = 1.0 : projection \u0026quot;\u0026quot;\u0026quot;, equations = \u0026quot;\u0026quot;\u0026quot; tau_plus * dx/dt = -x : event-driven # pre-synaptic trace tau_minus * dy/dt = -y : event-driven # post-synaptic trace \u0026quot;\u0026quot;\u0026quot;, pre_spike=\u0026quot;\u0026quot;\u0026quot; g_target += w x += A_plus * w_max w = clip(w + y, w_min , w_max) \u0026quot;\u0026quot;\u0026quot;, post_spike=\u0026quot;\u0026quot;\u0026quot; y -= A_minus * w_max w = clip(w + x, w_min , w_max) \u0026quot;\u0026quot;\u0026quot;)   Spiking synapses : Example of Spike-Timing Dependent plasticity (STDP)  And much more\u0026hellip;   Standard populations (SpikeSourceArray, TimedArray, PoissonPopulation, HomogeneousCorrelatedSpikeTrains), OpenCV bindings.\n  Standard neurons:\n LeakyIntegrator, Izhikevich, IF_curr_exp, IF_cond_exp, IF_curr_alpha, IF_cond_alpha, HH_cond_exp, EIF_cond_exp_isfa_ista, EIF_cond_alpha_isfa_ista    Standard synapses:\n Hebb, Oja, IBCM, STP, STDP    Parallel simulations with parallel_run.\n  Convolutional and pooling layers.\n  Hybrid rate-coded / spiking networks.\n  Structural plasticity.\n  RTFD: https://annarchy.readthedocs.io\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"ae5f560c4e7277f9aad09de95c05ce85","permalink":"https://julien-vitay.net/slides/annarchy/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/slides/annarchy/","section":"slides","summary":"ANNarchy (Artificial Neural Networks architect) Julien Vitay  ANNarchy (Artificial Neural Networks architect) Source code:\nhttps://bitbucket.org/annarchy/annarchy\nDocumentation:\nhttps://annarchy.readthedocs.io/en/stable/\nForum:\nhttps://groups.google.com/forum/#!forum/annarchy\nNotebooks used in this tutorial:\nhttps://github.com/vitay/ANNarchy-notebooks\n Installation Installation guide: https://annarchy.","tags":null,"title":"Introduction to ANNarchy","type":"slides"}]