@book{Sutton2017,
  title = {Reinforcement {{Learning}}: {{An Introduction}}},
  author = {Sutton, R. S. and Barto, A. G.},
  year = {2017},
  edition = {Second},
  publisher = {{MIT Press}},
  address = {{Cambridge, MA}},
}

@article{Vitay2014,
  title = {Timing and Expectation of Reward: {{A}} Neuro-Computational Model of the Afferents to the Ventral Tegmental Area},
  author = {Vitay, Julien and Hamker, Fred H.},
  year = {2014},
  journal = {Frontiers in Neurorobotics},
  volume = {8},
  number = {4},
  issn = {1662-5218 (Electronic)\textbackslash r1662-5218 (Linking)},
  doi = {10.3389/fnbot.2014.00004},
  abstract = {Neural activity in dopaminergic areas such as the ventral tegmental area is influenced by timing processes, in particular by the temporal expectation of rewards during Pavlovian conditioning. Receipt of a reward at the expected time allows to compute reward-prediction errors which can drive learning in motor or cognitive structures. Reciprocally, dopamine plays an important role in the timing of external events. Several models of the dopaminergic system exist, but the substrate of temporal learning is rather unclear. In this article, we propose a neuro-computational model of the afferent network to the ventral tegmental area, including the lateral hypothalamus, the pedunculopontine nucleus, the amygdala, the ventromedial prefrontal cortex, the ventral basal ganglia (including the nucleus accumbens and the ventral pallidum), as well as the lateral habenula and the rostromedial tegmental nucleus. Based on a plausible connectivity and realistic learning rules, this neuro-computational model reproduces several experimental observations, such as the progressive cancelation of dopaminergic bursts at reward delivery, the appearance of bursts at the onset of reward-predicting cues or the influence of reward magnitude on activity in the amygdala and ventral tegmental area. While associative learning occurs primarily in the amygdala, learning of the temporal relationship between the cue and the associated reward is implemented as a dopamine-modulated coincidence detection mechanism in the nucleus accumbens.},
  copyright = {All rights reserved},
  keywords = {Amygdala,Basal ganglia,Classical conditioning,Dopamine,Timing,VTA},
}

@article{Barreto2016,
  title = {Successor {{Features}} for {{Transfer}} in {{Reinforcement Learning}}},
  author = {Barreto, Andr{\'e} and Dabney, Will and Munos, R{\'e}mi and Hunt, Jonathan J. and Schaul, Tom and {van Hasselt}, Hado and Silver, David},
  year = {2016},
  month = jun,
  journal = {arXiv:1606.05312 [cs]},
  eprint = {1606.05312},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Transfer in reinforcement learning refers to the notion that generalization should occur not only within a task but also across tasks. We propose a transfer framework for the scenario where the reward function changes between tasks but the environment's dynamics remain the same. Our approach rests on two key ideas: "successor features", a value function representation that decouples the dynamics of the environment from the rewards, and "generalized policy improvement", a generalization of dynamic programming's policy improvement operation that considers a set of policies rather than a single one. Put together, the two ideas lead to an approach that integrates seamlessly within the reinforcement learning framework and allows the free exchange of information across tasks. The proposed method also provides performance guarantees for the transferred policy even before any learning has taken place. We derive two theorems that set our approach in firm theoretical ground and present experiments that show that it successfully promotes transfer in practice, significantly outperforming alternative methods in a sequence of navigation tasks and in the control of a simulated robotic arm.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence},
}

@article{Kulkarni2016,
  title = {Deep {{Successor Reinforcement Learning}}},
  author = {Kulkarni, Tejas D. and Saeedi, Ardavan and Gautam, Simanta and Gershman, Samuel J.},
  year = {2016},
  month = jun,
  journal = {arXiv:1606.02396 [cs, stat]},
  eprint = {1606.02396},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Learning robust value functions given raw observations and rewards is now possible with model-free and model-based deep reinforcement learning algorithms. There is a third alternative, called Successor Representations (SR), which decomposes the value function into two components -- a reward predictor and a successor map. The successor map represents the expected future state occupancy from any given state and the reward predictor maps states to scalar rewards. The value function of a state can be computed as the inner product between the successor map and the reward weights. In this paper, we present DSR, which generalizes SR within an end-to-end deep reinforcement learning framework. DSR has several appealing properties including: increased sensitivity to distal reward changes due to factorization of reward and world dynamics, and the ability to extract bottleneck states (subgoals) given successor maps trained under a random policy. We show the efficacy of our approach on two diverse environments given raw pixel observations -- simple grid-world domains (MazeBase) and the Doom game engine.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
}

@article{Corbit2011,
  title = {The General and Outcome-Specific Forms of {{Pavlovian-instrumental}} Transfer Are Differentially Mediated by the Nucleus Accumbens Core and Shell.},
  author = {Corbit, Laura H and Balleine, Bernard W},
  year = {2011},
  month = aug,
  journal = {The Journal of neuroscience : the official journal of the Society for Neuroscience},
  volume = {31},
  number = {33},
  pages = {11786--94},
  doi = {10.1523/JNEUROSCI.2711-11.2011},
  abstract = {Tests of Pavlovian-instrumental transfer (PIT) demonstrate that reward-predictive stimuli can exert a powerful motivational influence on the performance of instrumental actions. Recent evidence suggests that predictive stimuli produce this effect through either the general arousal (general PIT) or the specific predictions (outcome-specific PIT) produced by their association with reward. In two experiments, we examined the effects of pretraining lesions (Experiment 1) or muscimol-induced inactivation (Experiment 2) of either the core or shell regions of the nucleus accumbens (NAC) on these forms of PIT. Rats received Pavlovian training in which three auditory stimuli each predicted the delivery of a distinct food outcome. Separately, the rats were trained to perform two instrumental actions, each of which earned one of the outcomes used in Pavlovian conditioning. Finally, the effects of the three stimuli on performance of the two actions were assessed in extinction. Here we report evidence of a double dissociation between general and outcome-specific PIT at the level of the accumbens. Shell lesions eliminated outcome-specific PIT but spared general PIT, whereas lesions of the core abolished general PIT but spared outcome-specific PIT. Importantly, the infusion of muscimol into core or shell made immediately before the PIT tests produced a similar pattern of results. These results suggest that whereas the NAC core mediates the general excitatory effects of reward-related cues, the NAC shell mediates the effect of outcome-specific reward predictions on instrumental performance, and thereby serve to clarify reported discrepancies regarding the role of the NAC core and shell in PIT.},
  keywords = {Acoustic Stimulation,Acoustic Stimulation: methods,Animals,Conditioning; Classical,Conditioning; Classical: physiology,Extinction; Psychological,Extinction; Psychological: physiology,Feeding Behavior,Feeding Behavior: physiology,Male,Motivation,Motivation: physiology,Nucleus Accumbens,Nucleus Accumbens: anatomy \& histology,Nucleus Accumbens: physiology,Rats,Rats; Long-Evans}
}

@article{Dayan1993,
  title = {Improving {{Generalization}} for {{Temporal Difference Learning}}: {{The Successor Representation}}},
  shorttitle = {Improving {{Generalization}} for {{Temporal Difference Learning}}},
  author = {Dayan, Peter},
  year = {1993},
  month = jul,
  journal = {Neural Computation},
  volume = {5},
  number = {4},
  pages = {613--624},
  issn = {0899-7667},
  doi = {10.1162/neco.1993.5.4.613},
  abstract = {Estimation of returns over time, the focus of temporal difference (TD) algorithms, imposes particular constraints on good function approximators or representations. Appropriate generalization between states is determined by how similar their successors are, and representations should follow suit. This paper shows how TD machinery can be used to learn such representations, and illustrates, using a navigation task, the appropriately distributed nature of the result.},
}

@incollection{Dickinson2002,
  title = {The Role of Learning in the Operation of Motivational Systems},
  booktitle = {Steven's Handbook of Experimental Psychology: {{Learning}}, Motivation, and Emotion, {{Vol}}. 3, 3rd Ed},
  author = {Dickinson, Anthony and Balleine, Bernard},
  year = {2002},
  pages = {497--533},
  publisher = {{John Wiley \& Sons Inc}},
  address = {{Hoboken, NJ, US}},
  abstract = {Presents an analysis motivation that vindicates N. E. Miller's (1951) original claim that acquired behavior is motivated by desires rather than drives and, moreover, that desires are learned. Desires, however, come in two forms. The first is mediated by what the authors have called Pavlovian incentive learning, a process that they have analyzed in terms of a Konorskian model in which the CS activates an appetitive motivational system via a representation of the US or reinforcer with the Pavlovian desire arising from a feedback activation of this representation. These Pavlovian desires are modulated by two further mechanisms: the first being engaged by primary motivational states, and the second by an aversive system that serves to inhibit the appetitive system. Although the Pavlovian and instrumental incentive learning are dissociable and are treated separately in this analysis, they are said to function in parallel and in a dynamic interaction throughout the behavioral stream with the nature of the interaction varying with the status of the current action. Instrumental incentive learning can only motivate cognitively mediated, goal-directed actions but not S-R habits, which are primarily under the motivational influence of the Pavlovian form of incentive learning. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  isbn = {978-0-471-38047-4},
  keywords = {Classical Conditioning,Instrumentality,Learning,Motivation,Operant Conditioning},
}

@article{Doll2012,
  title = {The Ubiquity of Model-Based Reinforcement Learning},
  author = {Doll, Bradley B and Simon, Dylan A and Daw, Nathaniel D},
  year = {2012},
  month = dec,
  journal = {Current Opinion in Neurobiology},
  series = {Decision Making},
  volume = {22},
  number = {6},
  pages = {1075--1081},
  issn = {0959-4388},
  doi = {10.1016/j.conb.2012.08.003},
  abstract = {The reward prediction error (RPE) theory of dopamine (DA) function has enjoyed great success in the neuroscience of learning and decision-making. This theory is derived from model-free reinforcement learning (RL), in which choices are made simply on the basis of previously realized rewards. Recently, attention has turned to correlates of more flexible, albeit computationally complex, model-based methods in the brain. These methods are distinguished from model-free learning by their evaluation of candidate actions using expected future outcomes according to a world model. Puzzlingly, signatures from these computations seem to be pervasive in the very same regions previously thought to support model-free learning. Here, we review recent behavioral and neural evidence about these two systems, in attempt to reconcile their enigmatic cohabitation in the brain.},
}

@incollection{Miller2018,
  title = {Re-Aligning Models of Habitual and Goal-Directed Decision-Making},
  booktitle = {Goal-{{Directed Decision Making}} : {{Computations}} and {{Neural Circuits}}},
  author = {Miller, Kevin and Ludvig, Elliot Andrew and Pezzulo, Giovanni and Shenhav, Amitai},
  editor = {Bornstein, Aaron and Morris, Richard W. and Shenhav, Amitai},
  year = {2018},
  month = sep,
  publisher = {{Academic Press}},
  abstract = {The classic dichotomy between habitual and goal-directed behavior is often mapped onto a dichotomy between model-free and model-based reinforcement learning (RL) algorithms, putatively implemented in segregated neuronal circuits. Despite significant heuristic value in motivating  experimental investigations,  several lines of evidence suggest that this mapping is in need of modification and/or realignment. First, whereas habitual and goal-directed behaviors have been shown to depend on cleanly separable neural circuitry, recent data suggest that model-based and model-free representations in the brain are largely overlapping. Second, habitual behaviors need not involve representations of expected reinforcement (i.e., need not involve RL, model-free or otherwise), but may be based instead on simple stimulus-response associations. Finally, goal-directed decisions may not reflect a single model-based algorithm but rather a continuum of ``model-basedness''. These lines of evidence thus suggest a possible reconceptualization of the distinction between model-free vs. model-based RL--one in which both contribute to a single goal-directed system that is value-based, as opposed to distinct, habitual mechanisms that are value-free. In this chapter, we discuss new models that have extended the RL approach to modeling habitual and goal-directed behavior and assess how these have clarified our understanding of the underlying neural circuitry.},
  isbn = {978-0-12-812098-9},
}

@inproceedings{Ducarouge2017,
  title = {The {{Successor Representation}} as a Model of Behavioural Flexibility},
  booktitle = {Journ\'ees {{Francophones}} Sur La {{Planification}}, La {{D\'ecision}} et l'{{Apprentissage}} Pour La Conduite de Syst\`emes ({{JFPDA}} 2017)},
  author = {Ducarouge, Alexis and Sigaud, Olivier},
  year = {2017},
  month = jul,
  series = {Actes Des {{Journ\'ees Francophones}} Sur La {{Planification}}, La {{D\'ecision}} et l'{{Apprentissage}} Pour La Conduite de Syst\`emes ({{JFPDA}} 2017)},
  address = {{Caen, France}},
  abstract = {Accounting for behavioural capabilities and flexibilities experimentally observed in animals is a major issue in computational neurosciences. In order to design a comprehensive algorithmic framework for this purpose, the model-free and model-based reinforcement learning (RL) components are generally taken as reference either in isolation or in combination. In this article, we consider the RL Successor Representation (SR) approach as an alternative. We compare it to the standard model-free and modelbased models on three relevant experimental data-sets. These modelling experiments demonstrate that SR is able to account better for several behavioural flexibilities while being algorithmically simpler.},
  keywords = {behavioural flexibility,internal motivation,latent learning,policy revaluation.,Reinforcement learning,successor representation},
}

@article{Gardner2018,
  title = {Rethinking Dopamine as Generalized Prediction Error},
  author = {Gardner, Matthew P. H. and Schoenbaum, Geoffrey and Gershman, Samuel J.},
  year = {2018},
  month = nov,
  journal = {Proceedings of the Royal Society B: Biological Sciences},
  volume = {285},
  number = {1891},
  pages = {20181645},
  doi = {10.1098/rspb.2018.1645},
  abstract = {Midbrain dopamine neurons are commonly thought to report a reward prediction error (RPE), as hypothesized by reinforcement learning (RL) theory. While this theory has been highly successful, several lines of evidence suggest that dopamine activity also encodes sensory prediction errors unrelated to reward. Here, we develop a new theory of dopamine function that embraces a broader conceptualization of prediction errors. By signalling errors in both sensory and reward predictions, dopamine supports a form of RL that lies between model-based and model-free algorithms. This account remains consistent with current canon regarding the correspondence between dopamine transients and RPEs, while also accounting for new data suggesting a role for these signals in phenomena such as sensory preconditioning and identity unblocking, which ostensibly draw upon knowledge beyond reward predictions.},
}

@article{Gershman2012,
  title = {The {{Successor Representation}} and {{Temporal Context}}},
  author = {Gershman, Samuel J. and Moore, Christopher D. and Todd, Michael T. and Norman, Kenneth A. and Sederberg, Per B.},
  year = {2012},
  month = feb,
  journal = {Neural Computation},
  volume = {24},
  number = {6},
  pages = {1553--1568},
  issn = {0899-7667},
  doi = {10.1162/NECO_a_00282},
  abstract = {The successor representation was introduced into reinforcement learning by Dayan (1993) as a means of facilitating generalization between states with similar successors. Although reinforcement learning in general has been used extensively as a model of psychological and neural processes, the psychological validity of the successor representation has yet to be explored. An interesting possibility is that the successor representation can be used not only for reinforcement learning but for episodic learning as well. Our main contribution is to show that a variant of the temporal context model (TCM; Howard \& Kahana, 2002), an influential model of episodic memory, can be understood as directly estimating the successor representation using the temporal difference learning algorithm (Sutton \& Barto, 1998). This insight leads to a generalization of TCM and new experimental predictions. In addition to casting a new normative light on TCM, this equivalence suggests a previously unexplored point of contact between different learning systems.},
}

@article{Gershman2018,
  title = {The {{Successor Representation}}: {{Its Computational Logic}} and {{Neural Substrates}}.},
  author = {Gershman, Samuel J.},
  year = {2018},
  month = aug,
  journal = {The Journal of neuroscience : the official journal of the Society for Neuroscience},
  volume = {38},
  number = {33},
  pages = {7193--7200},
  doi = {10.1523/JNEUROSCI.0151-18.2018},
  abstract = {Reinforcement learning is the process by which an agent learns to predict long-term future reward. We now understand a great deal about the brain's reinforcement learning algorithms, but we know considerably less about the representations of states and actions over which these algorithms operate. A useful starting point is asking what kinds of representations we would want the brain to have, given the constraints on its computational architecture. Following this logic leads to the idea of the successor representation, which encodes states of the environment in terms of their predictive relationships with other states. Recent behavioral and neural studies have provided evidence for the successor representation, and computational studies have explored ways to extend the original idea. This paper reviews progress on these fronts, organizing them within a broader framework for understanding how the brain negotiates tradeoffs between efficiency and flexibility for reinforcement learning.},
  keywords = {cognitive map,dopamine,hippocampus,reinforcement learning,reward},
}

@article{Momennejad2017,
  title = {The Successor Representation in Human Reinforcement Learning},
  author = {Momennejad, I. and Russek, E. M. and Cheong, J. H. and Botvinick, M. M. and Daw, N. D. and Gershman, S. J.},
  year = {2017},
  month = sep,
  journal = {Nature Human Behaviour},
  volume = {1},
  number = {9},
  pages = {680--692},
  issn = {2397-3374},
  doi = {10.1038/s41562-017-0180-8},
  langid = {english},
  keywords = {Extinction,Human behaviour},
}

@article{Stachenfeld2017,
  title = {The Hippocampus as a Predictive Map},
  author = {Stachenfeld, Kimberly L and Botvinick, Matthew M and Gershman, Samuel J},
  year = {2017},
  month = oct,
  journal = {Nature Neuroscience},
  volume = {20},
  number = {11},
  pages = {1643--1653},
  doi = {10.1038/nn.4650},
  abstract = {The authors show how predictive representations are useful for maximizing future reward, particularly in spatial domains. They develop a predictive-map model of hippocampal place cells and entorhinal grid cells that captures a wide variety of effects from human and rodent literature.},
  keywords = {Hippocampus,Learning and memory,Reward},
}

@article{Hafting2005,
  title = {Microstructure of a Spatial Map in the Entorhinal Cortex},
  author = {Hafting, Torkel and Fyhn, Marianne and Molden, Sturla and Moser, May-Britt and Moser, Edvard I.},
  year = {2005},
  month = aug,
  journal = {Nature},
  volume = {436},
  number = {7052},
  pages = {801},
  issn = {1476-4687},
  doi = {10.1038/nature03721},
  abstract = {We can find our way about, so somewhere in our brain there must be a neural equivalent of a three-dimensional map. Work on navigation in mammals points to the hippocampus as part of this `spatial learning' system. Now an important advance shows that the entorhinal cortex, which inputs to the hippocampus, is the site where information about place, distance and direction is integrated into a neural map of the surroundings. Here a series of grid cells represents the space around the animal. Each grid cell is activated when an animal's position coincides with a vertex on a grid of equilateral triangles representing the environment. In answering so many questions about the perception of space, this raises the next question: how are these triangular-grid place fields constructed?},
  copyright = {2005 Nature Publishing Group},
  langid = {english},
}

@article{Lee2014,
  title = {Neural {{Computations Underlying Arbitration}} between {{Model-Based}} and {{Model-free Learning}}},
  author = {Lee, Sang~Wan and Shimojo, Shinsuke and O'Doherty, John~P.},
  year = {2014},
  month = feb,
  journal = {Neuron},
  volume = {81},
  number = {3},
  pages = {687--699},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2013.11.028},
  abstract = {Summary There is accumulating neural evidence to support the existence of two distinct systems for guiding action selection, a deliberative ``model-based'' and a reflexive ``model-free'' system. However, little is known about how the brain determines which of these systems controls behavior at one moment in time. We provide evidence for an arbitration mechanism that allocates the degree of control over behavior by model-based and model-free systems as a function of the reliability of their respective predictions. We show that the inferior lateral prefrontal and frontopolar cortex encode both reliability signals and the output of a comparison between those signals, implicating these regions in the arbitration process. Moreover, connectivity between these regions and model-free valuation areas is negatively modulated by the degree of model-based control in the arbitrator, suggesting that arbitration may work through modulation of the model-free valuation system when the arbitrator deems that the model-based system should drive behavior.},
}

@article{Ma2018,
  title = {Universal {{Successor Representations}} for {{Transfer Reinforcement Learning}}},
  author = {Ma, Chen and Wen, Junfeng and Bengio, Yoshua},
  year = {2018},
  month = apr,
  journal = {arXiv:1804.03758 [cs, stat]},
  eprint = {1804.03758},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {The objective of transfer reinforcement learning is to generalize from a set of previous tasks to unseen new tasks. In this work, we focus on the transfer scenario where the dynamics among tasks are the same, but their goals differ. Although general value function (Sutton et al., 2011) has been shown to be useful for knowledge transfer, learning a universal value function can be challenging in practice. To attack this, we propose (1) to use universal successor representations (USR) to represent the transferable knowledge and (2) a USR approximator (USRA) that can be trained by interacting with the environment. Our experiments show that USR can be effectively applied to new tasks, and the agent initialized by the trained USRA can achieve the goal considerably faster than random initialization.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
}

@article{Machado2018,
  title = {Count-{{Based Exploration}} with the {{Successor Representation}}},
  author = {Machado, Marlos C. and Bellemare, Marc G. and Bowling, Michael},
  year = {2018},
  month = jul,
  journal = {arXiv:1807.11622 [cs, stat]},
  eprint = {1807.11622},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {In this paper we introduce a simple approach for exploration in reinforcement learning (RL) that allows us to develop theoretically justified algorithms in the tabular case but that is also extendable to settings where function approximation is required. Our approach is based on the successor representation (SR), which was originally introduced as a representation defining state generalization by the similarity of successor states. Here we show that the norm of the SR, while it is being learned, can be used as a reward bonus to incentivize exploration. In order to better understand this transient behavior of the norm of the SR we introduce the substochastic successor representation (SSR) and we show that it implicitly counts the number of times each state (or feature) has been observed. We use this result to introduce an algorithm that performs as well as some theoretically sample-efficient approaches. Finally, we extend these ideas to a deep RL algorithm and show that it achieves state-of-the-art performance in Atari 2600 games.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
}

@article{Zhang2016,
  title = {Deep {{Reinforcement Learning}} with {{Successor Features}} for {{Navigation}} across {{Similar Environments}}},
  author = {Zhang, Jingwei and Springenberg, Jost Tobias and Boedecker, Joschka and Burgard, Wolfram},
  year = {2016},
  month = dec,
  journal = {arXiv:1612.05533 [cs]},
  eprint = {1612.05533},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {In this paper we consider the problem of robot navigation in simple maze-like environments where the robot has to rely on its onboard sensors to perform the navigation task. In particular, we are interested in solutions to this problem that do not require localization, mapping or planning. Additionally, we require that our solution can quickly adapt to new situations (e.g., changing navigation goals and environments). To meet these criteria we frame this problem as a sequence of related reinforcement learning tasks. We propose a successor feature based deep reinforcement learning algorithm that can learn to transfer knowledge from previously mastered navigation tasks to new problem instances. Our algorithm substantially decreases the required learning time after the first task instance has been solved, which makes it easily adaptable to changing environments. We validate our method in both simulated and real robot experiments with a Robotino and compare it to a set of baseline methods including classical planning-based navigation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
}

@article{Oemisch2019,
  title = {Feature-Specific Prediction Errors and Surprise across Macaque Fronto-Striatal Circuits},
  author = {Oemisch, Mariann and Westendorff, Stephanie and Azimi, Marzyeh and Hassani, Seyed Alireza and Ardid, Salva and Tiesinga, Paul and Womelsdorf, Thilo},
  year = {2019},
  month = jan,
  journal = {Nature Communications},
  volume = {10},
  number = {1},
  pages = {176},
  issn = {2041-1723},
  doi = {10.1038/s41467-018-08184-9},
  abstract = {In order to adjust expectations efficiently, prediction errors need to be associated with the features that gave rise to the unexpected outcome. Here, the authors show that neurons in anterior fronto-striatal networks encode prediction errors that are specific to feature values of different stimulus dimensions.},
  copyright = {2019 The Author(s)},
  langid = {english},
}

@book{OKeefe1978,
  title = {The Hippocampus as a Cognitive Map},
  author = {O'Keefe, John and Nadel, Lynn},
  year = {1978},
  publisher = {{Clarendon Press ; Oxford University Press}},
  address = {{Oxford : New York}},
  isbn = {978-0-19-857206-0},
  langid = {english},
  lccn = {QP383.2 .O38},
  keywords = {Cognition,Hippocampus (Brain),Memory},
}

@article{Russek2017,
  title = {Predictive Representations Can Link Model-Based Reinforcement Learning to Model-Free Mechanisms},
  author = {Russek, Evan M. and Momennejad, Ida and Botvinick, Matthew M. and Gershman, Samuel J. and Daw, Nathaniel D.},
  editor = {Daunizeau, Jean},
  year = {2017},
  month = sep,
  journal = {PLOS Computational Biology},
  volume = {13},
  number = {9},
  pages = {e1005768},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1005768},
  langid = {english},
}

@article{Schultz1998,
  title = {Predictive Reward Signal of Dopamine Neurons},
  author = {Schultz, Wolfram},
  year = {1998},
  journal = {J Neurophysiol},
  volume = {80},
  pages = {1--27},
}

@article{Watabe-Uchida2019,
  title = {Multiple {{Dopamine Systems}}: {{Weal}} and {{Woe}} of {{Dopamine}}},
  shorttitle = {Multiple {{Dopamine Systems}}},
  author = {{Watabe-Uchida}, Mitsuko and Uchida, Naoshige},
  year = {2019},
  month = feb,
  journal = {Cold Spring Harbor Symposia on Quantitative Biology},
  pages = {037648},
  issn = {0091-7451, 1943-4456},
  doi = {10.1101/sqb.2018.83.037648},
  abstract = {The ability to predict future outcomes increases the fitness of the animal. Decades of research have shown that dopamine neurons broadcast reward prediction error (RPE) signals\textemdash the discrepancy between actual and predicted reward\textemdash to drive learning to predict future outcomes. Recent studies have begun to show, however, that dopamine neurons are more diverse than previously thought. In this review, we will summarize a series of our studies that have shown unique properties of dopamine neurons projecting to the posterior ``tail'' of the striatum (TS) in terms of anatomy, activity, and function. Specifically, TS-projecting dopamine neurons are activated by a subset of negative events including threats from a novel object, send prediction errors for external threats, and reinforce avoidance behaviors. These results indicate that there are at least two axes of dopamine-mediated reinforcement learning in the brain\textemdash one learning from canonical RPEs and another learning from threat prediction errors. We argue that the existence of multiple learning systems is an adaptive strategy that makes possible each system optimized for its own needs. The compartmental organization in the mammalian striatum resembles that of a dopamine-recipient area in insects (mushroom body), pointing to a principle of dopamine function conserved across phyla.},
  langid = {english},
  pmid = {30787046},
}

